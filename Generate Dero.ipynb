{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook generates the individual files for the dero package. When making changes, be sure to update the version in the top cell. Then restart kernel and run all cells, then navigate to the root folder (outside dero), and run upload.bat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-make tests for regressions\n",
    "-make tests for summarize\n",
    "-work on reg for each combo - freezing even with parallel/timeout\n",
    "-fixed effects regressions\n",
    "-include rows in summary column for regressions\n",
    "    -cluster\n",
    "    -robust\n",
    "    -fixed effects\n",
    "-make tests for long_short_portfolio\n",
    "-make tests for transition in math using the docstrings\n",
    "-make tests for pandas.averages with count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "version = '0.8.1'\n",
    "upload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "version_info = tuple([int(i) for i in version.split('.')]) #tuple of ints, e.g. (0,2,2)\n",
    "\n",
    "with open('version.py', 'w') as f:\n",
    "    f.write('__version__ = \"{}\" \\n'.format(version))\n",
    "    f.write('__version_info__ = {}'.format(version_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "pandasql\n",
    "wrds\n",
    "numpy\n",
    "pdfrw\n",
    "selenium\n",
    "unidecode\n",
    "IPython\n",
    "sas7bdat\n",
    "statsmodels\n",
    "matplotlib\n",
    "sympy\n",
    "pandastable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from version import __version__\n",
    "from setuptools import setup\n",
    "\n",
    "\n",
    "setup(name='Dero',\n",
    "      version=__version__,\n",
    "      description=\"Nick DeRobertis Personal Library\",\n",
    "      long_description='''\n",
    "      Nick DeRobertis' personal library of functions. This is hosted on PyPi mostly for my own \n",
    "      convenience, though others may use it so long as I'm given credit. \n",
    "      ''',\n",
    "      author='Nick DeRobertis',\n",
    "      author_email='whoopnip@gmail.com',\n",
    "      license='MIT',\n",
    "      packages=['dero'],\n",
    "      classifiers=[\n",
    "        # How mature is this project? Common values are\n",
    "        #   3 - Alpha\n",
    "        #   4 - Beta\n",
    "        #   5 - Production/Stable\n",
    "        'Development Status :: 3 - Alpha',\n",
    "\n",
    "        # Indicate who your project is intended for\n",
    "        'Intended Audience :: Developers',\n",
    "\n",
    "        # Specify the Python versions you support here. In particular, ensure\n",
    "        # that you indicate whether you support Python 2, Python 3 or both.\n",
    "        'Programming Language :: Python :: 3',\n",
    "        'Programming Language :: Python :: 3.2',\n",
    "        'Programming Language :: Python :: 3.3',\n",
    "        'Programming Language :: Python :: 3.4',\n",
    "        'Programming Language :: Python :: 3.5'\n",
    "        ],\n",
    "       install_requires=[\n",
    "        'pandas',\n",
    "        'pandasql',\n",
    "        'numpy',\n",
    "        'wrds',\n",
    "        'pdfrw',\n",
    "        'selenium',\n",
    "        'unidecode',\n",
    "        'IPython',\n",
    "        'sas7bdat',\n",
    "        'statsmodels',\n",
    "        'matplotlib',\n",
    "        'sympy',\n",
    "        'pandastable'],\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "orig_path = os.getcwd()\n",
    "\n",
    "os.chdir('dero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "init_str = '''\n",
    "__version__ = \"{}\"\n",
    "__version_info__ = {}\n",
    "\n",
    "#Provided for backwards compatibility with old code. In the old code, add to the top:\n",
    "#import builtins\n",
    "#builtins.__dero_version__ = '0.1.0'\n",
    "try: \n",
    "    __dero_version__\n",
    "    if __dero_version__ == '0.1.0':\n",
    "        from dero.core import *\n",
    "        from dero.data import *\n",
    "        from dero.decorators import *\n",
    "        from dero.latex import *\n",
    "        from dero.pdf import *\n",
    "        \n",
    "        from dero.ext_logging import *\n",
    "        from dero.ext_matplotlib import *\n",
    "        from dero.ext_multiprocessing import *\n",
    "        from dero.ext_pandas import *\n",
    "        from dero.ext_selenium import *\n",
    "        from dero.ext_sympy import *\n",
    "        from dero.ext_time import *\n",
    "        pandas_to_csv = to_csv\n",
    "    else: #stuctured this way so that we can support future structuring changes, can add additional if statements\n",
    "        __dero_version__ = 'current'\n",
    "except NameError:\n",
    "    __dero_version__ = 'current'\n",
    "\n",
    "if __dero_version__ == 'current':\n",
    "    import dero.core\n",
    "    import dero.data\n",
    "    import dero.decorators\n",
    "    import dero.latex\n",
    "    import dero.pdf\n",
    "    import dero.wrds\n",
    "    import dero.logtimer\n",
    "    import dero.reg\n",
    "    import dero.summ\n",
    "    \n",
    "    import dero.ext_logging as logging\n",
    "    import dero.ext_matplotlib as matplotlib\n",
    "    import dero.ext_multiprocessing as multiprocessing\n",
    "    import dero.ext_pandas as pandas\n",
    "    import dero.ext_selenium as selenium\n",
    "    import dero.ext_sympy as sympy\n",
    "    import dero.ext_time as time\n",
    "    import dero.ext_math as math\n",
    "    \n",
    "    del dero.ext_logging\n",
    "    del dero.ext_matplotlib\n",
    "    del dero.ext_multiprocessing\n",
    "    del dero.ext_pandas\n",
    "    del dero.ext_selenium\n",
    "    del dero.ext_sympy\n",
    "    del dero.ext_time\n",
    "    del dero.ext_math\n",
    "    \n",
    "#     sys.modules['dero.logging'] = dero.ext_logging\n",
    "#     sys.modules['dero.matplotlib'] = dero.ext_matplotlib\n",
    "#     sys.modules['dero.multiprocessing'] = dero.ext_multiprocessing\n",
    "#     sys.modules['dero.pandas'] = dero.ext_pandas\n",
    "#     sys.modules['dero.selenium'] = dero.ext_selenium\n",
    "#     sys.modules['dero.sympy'] = dero.ext_sympy\n",
    "#     sys.modules['dero.time'] = dero.ext_time\n",
    "    \n",
    "#     import dero.logging\n",
    "#     import dero.matplotlib\n",
    "#     import dero.multiprocessing\n",
    "#     import dero.pandas\n",
    "#     import dero.selenium\n",
    "#     import dero.sympy\n",
    "#     import dero.time\n",
    "    \n",
    "     \n",
    "\n",
    "'''.format(version, version_info)\n",
    "\n",
    "with open('__init__.py', 'w') as f:\n",
    "    f.write(init_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_pandas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_pandas.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime, time\n",
    "from numpy import nan\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings, timeit\n",
    "from sas7bdat import SAS7BDAT\n",
    "import statsmodels.api as sm\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from multiprocessing import Pool\n",
    "from tkinter import Tk, Frame, BOTH, YES\n",
    "from pandastable import Table\n",
    "import functools, itertools\n",
    "from pandasql import PandaSQL\n",
    "\n",
    "from .pdutils import window_mapping, year_month_from_single_date, _check_portfolio_inputs, _assert_byvars_list, \\\n",
    "                     _create_cutoffs_and_sort_into_ports, _split, _sort_arr_list_into_ports_and_return_series, \\\n",
    "                     _to_list_if_str, _expand, _to_series_if_str, _to_name_if_series, \\\n",
    "                     _extract_table_names_from_sql, _get_datetime_cols, \\\n",
    "                     _select_long_short_ports, _portfolio_difference\n",
    "\n",
    "from .ext_time import estimate_time\n",
    "\n",
    "def to_csv(dataframe, path, filename, output=True, action='w', index=True):\n",
    "    '''\n",
    "    action='w' for overwrite, 'a' for append\n",
    "    set index to False to not include index in output\n",
    "    '''   \n",
    "    if action == 'a':\n",
    "        headers = False\n",
    "    else:\n",
    "        headers = True\n",
    "    \n",
    "    if dataframe is not None: #if dataframe exists\n",
    "        filepath = os.path.join(path,filename + '.csv')\n",
    "        f = open(filepath, action, encoding='utf-8')\n",
    "        if output is True: print(\"Now saving %s\" % filepath)\n",
    "        try: f.write(dataframe.to_csv(encoding='utf-8', index=index, header=headers)) #could use easier dataframe.to_csv(filepath) syntax, but won't overwrite\n",
    "        except: f.write(dataframe.to_csv(encoding='utf-8', index=index, header=headers).replace('\\ufffd',''))\n",
    "        f.close()\n",
    "    else:\n",
    "        print(\"{} does not exist.\".format(dataframe)) #does nothing if dataframe doesn't exist\n",
    "    \n",
    "def convert_sas_date_to_pandas_date(sasdates):\n",
    "    epoch = datetime.datetime(1960, 1, 1)\n",
    "    \n",
    "    def to_pandas(date):\n",
    "        return epoch + datetime.timedelta(days=date)\n",
    "    \n",
    "    if isinstance(sasdates, pd.Series):\n",
    "        #Below code is to reduce down to unique dates and create a mapping\n",
    "        \n",
    "#         unique = pd.Series(sasdates.dropna().unique()).astype(int)\n",
    "#         shift = unique.apply(datetime.timedelta)\n",
    "#         pd_dates = epoch + shift\n",
    "        \n",
    "#         for_merge = pd.concat([unique, pd_dates], axis=1)\n",
    "#         for_merge.columns = [sasdates.name, 0]\n",
    "        \n",
    "#         orig_df = pd.DataFrame(sasdates)\n",
    "#         orig_df.reset_index(inplace=True)\n",
    "        \n",
    "#         return for_merge.merge(orig_df, how='right', on=[sasdates.name]).sort_values('index').reset_index()[0]\n",
    "\n",
    "        return apply_func_to_unique_and_merge(sasdates, to_pandas)\n",
    "    \n",
    "    \n",
    "#         return pd.Series([epoch + datetime.timedelta(days=int(float(date))) if not pd.isnull(date) else nan for date in sasdates])\n",
    "    else:\n",
    "        return epoch + datetime.timedelta(days=sasdates)\n",
    "    \n",
    "def year_month_from_date(df, date='Date', yearname='Year', monthname='Month'):\n",
    "    '''\n",
    "    Takes a dataframe with a datetime object and creates year and month variables\n",
    "    '''\n",
    "    df = df.copy()\n",
    "#     df[yearname] =  [date.year  for date in df[date]]\n",
    "#     df[monthname] = [date.month for date in df[date]]\n",
    "    df[[yearname, monthname]] = apply_func_to_unique_and_merge(df[date], year_month_from_single_date)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def expand_time(df, intermediate_periods=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates new observations in the dataset advancing the time by the int or list given. Creates a new date variable.\n",
    "    See _expand_time for keyword arguments.\n",
    "    \n",
    "    Specify intermediate_periods=True to get periods in between given time periods, e.g.\n",
    "    passing time=[12,24,36] will get periods 12, 13, 14, ..., 35, 36. \n",
    "    \"\"\"\n",
    "    \n",
    "    if intermediate_periods:\n",
    "        assert 'time' in kwargs\n",
    "        time = kwargs['time']\n",
    "        time = [t for t in range(min(time),max(time) + 1)]\n",
    "        kwargs['time'] = time\n",
    "    return _expand_time(df, **kwargs)\n",
    "\n",
    "def _expand_time(df, datevar='Date', freq='m', time=[12, 24, 36, 48, 60], newdate='Shift Date', shiftvar='Shift'):\n",
    "    '''\n",
    "    Creates new observations in the dataset advancing the time by the int or list given. Creates a new date variable.\n",
    "    '''\n",
    "    def log(message):\n",
    "        if message != '\\n':\n",
    "            time = datetime.datetime.now().replace(microsecond=0)\n",
    "            message = str(time) + ': ' + message\n",
    "        sys.stdout.write(message + '\\n')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    log('Initializing expand_time for periods {}.'.format(time))\n",
    "    \n",
    "    if freq == 'd':\n",
    "        log('Daily frequency, getting trading day calendar.')\n",
    "        td = tradedays() #gets trading day calendar\n",
    "    else:\n",
    "        td = None\n",
    "    \n",
    "    def time_shift(shift, freq=freq, td=td):\n",
    "        if freq == 'm':\n",
    "            return relativedelta(months=shift)\n",
    "        if freq == 'd':\n",
    "            return shift * td\n",
    "        if freq == 'a':\n",
    "            return relativedelta(years=shift)\n",
    "    \n",
    "    if isinstance(time, int):\n",
    "        time = [time]\n",
    "    else: assert isinstance(time, list)\n",
    "    \n",
    "    \n",
    "    log('Calculating number of rows.')\n",
    "    num_rows = len(df.index)\n",
    "    log('Calculating number of duplicates.')\n",
    "    duplicates = len(time)\n",
    "    \n",
    "    #Expand number of rows\n",
    "    if duplicates > 1:\n",
    "        log('Duplicating observations {} times.'.format(duplicates - 1))\n",
    "        df = df.append([df] * (duplicates - 1)).sort_index().reset_index(drop=True)\n",
    "        log('Duplicated.')\n",
    "    \n",
    "    log('Creating shift variable.')\n",
    "    df[shiftvar] = time * num_rows #Create a variable containing amount of time to shift\n",
    "    #Now create shifted date\n",
    "    log('Creating shifted date.')\n",
    "    df[newdate] = [date + time_shift(int(shift)) for date, shift in zip(df[datevar],df[shiftvar])]\n",
    "    log('expand_time completed.')\n",
    "    \n",
    "    #Cleanup and exit\n",
    "    return df #.drop('Shift', axis=1)\n",
    "\n",
    "def expand_months(df, datevar='Date', newdatevar='Daily Date', trade_days=True):\n",
    "    \"\"\"\n",
    "    Takes a monthly dataframe and returns a daily (trade day or calendar day) dataframe. \n",
    "    For each row in the input data, duplicates that row over each trading/calendar day in the month of \n",
    "    the date in that row. Creates a new date column containing the daily date.\n",
    "    \n",
    "    NOTE: If the input dataset has multiple observations per month, all of these will be expanded. Therefore\n",
    "    you will have one row for each trade day for each original observation. \n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing a date variable\n",
    "    \n",
    "    Optional inputs:\n",
    "    datevar: str, name of column containing dates in the input df\n",
    "    newdatevar: str, name of new column to be created containing daily dates\n",
    "    tradedays: bool, True to use trading days and False to use calendar days\n",
    "    \"\"\"\n",
    "    if trade_days:\n",
    "        td = tradedays()\n",
    "    else:\n",
    "        td = 'D'\n",
    "    \n",
    "    expand = functools.partial(_expand, datevar=datevar, td=td, newdatevar=newdatevar)\n",
    "    \n",
    "    \n",
    "    expand_all = np.vectorize(expand, otypes=[np.ndarray])\n",
    "        \n",
    "    days =  pd.DataFrame(np.concatenate(expand_all(df[datevar].unique()), axis=0),\n",
    "                         columns=[datevar, newdatevar], dtype='datetime64')\n",
    "\n",
    "    return df.merge(days, on=datevar, how='left')\n",
    "\n",
    "def cumulate(df, cumvars, method, periodvar='Date',  byvars=None, time=None, grossify=False,\n",
    "             multiprocess=True):\n",
    "    \"\"\"\n",
    "    Cumulates a variable over time. Typically used to get cumulative returns. \n",
    "    \n",
    "    NOTE: Method zero not yet working\n",
    "    \n",
    "    method = 'between', 'zero', or 'first'. \n",
    "             If 'zero', will give returns since the original date. Note: for periods before the original date, \n",
    "             this will turn positive returns negative as we are going backwards in time.\n",
    "             If 'between', will give returns since the prior requested time period. Note that\n",
    "             the first period is period 0.\n",
    "             If 'first', will give returns since the first requested time period.\n",
    "             For example, if our input data was for date 1/5/2006, but we had shifted dates:\n",
    "                 permno  date      RET  shift_date\n",
    "                 10516   1/5/2006  110%  1/5/2006\n",
    "                 10516   1/5/2006  120%  1/6/2006\n",
    "                 10516   1/5/2006  105%  1/7/2006\n",
    "                 10516   1/5/2006  130%  1/8/2006\n",
    "             Then cumulate(df, 'RET', cumret='between', time=[1,3], get='RET', periodvar='shift_date') would return:\n",
    "                 permno  date      RET  shift_date  cumret\n",
    "                 10516   1/5/2006  110%  1/5/2006    110%\n",
    "                 10516   1/5/2006  120%  1/6/2006    120%\n",
    "                 10516   1/5/2006  105%  1/7/2006    126%\n",
    "                 10516   1/5/2006  130%  1/8/2006    130%\n",
    "             Then cumulate(df, 'RET', cumret='first', periodvar='shift_date') would return:\n",
    "                 permno  date      RET  shift_date  cumret\n",
    "                 10516   1/5/2006  110%  1/5/2006    110%\n",
    "                 10516   1/5/2006  120%  1/6/2006    120%\n",
    "                 10516   1/5/2006  105%  1/7/2006    126%\n",
    "                 10516   1/5/2006  130%  1/8/2006    163.8%\n",
    "    byvars: string or list of column names to use to seperate by groups\n",
    "    time: list of ints, for use with method='between'. Defines which periods to calculate between.\n",
    "    grossify: bool, set to True to add one to all variables then subtract one at the end\n",
    "    multiprocess: bool or int, set to True to use all available processors, \n",
    "                  set to False to use only one, pass an int less or equal to than number of \n",
    "                  processors to use that amount of processors \n",
    "    \"\"\"\n",
    "    import time as time2 #accidentally used time an an input parameter and don't want to break prior code\n",
    "    \n",
    "    def log(message):\n",
    "        if message != '\\n':\n",
    "            time = datetime.datetime.now().replace(microsecond=0)\n",
    "            message = str(time) + ': ' + message\n",
    "        sys.stdout.write(message + '\\n')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    log('Initializing cumulate.')\n",
    "    \n",
    "    df = df.copy() #don't want to modify original dataframe\n",
    "    \n",
    "    if time:\n",
    "        sort_time = sorted(time)\n",
    "    else: sort_time = None\n",
    "        \n",
    "    if isinstance(cumvars, (str, int)):\n",
    "        cumvars = [cumvars]\n",
    "    assert isinstance(cumvars, list)\n",
    "\n",
    "    assert isinstance(grossify, bool)\n",
    "    \n",
    "    if grossify:\n",
    "        for col in cumvars:\n",
    "            df[col] = df[col] + 1\n",
    "    \n",
    "    def unflip(df, cumvars):\n",
    "        flipcols = ['cum_' + str(c) for c in cumvars] #select cumulated columns\n",
    "        for col in flipcols:\n",
    "            tempdf[col] = tempdf[col].shift(1) #shift all values down one row for cumvars\n",
    "            tempdf[col] = -tempdf[col] + 2 #converts a positive return into a negative return\n",
    "        tempdf = tempdf[1:].copy() #drop out period 0\n",
    "        tempdf = tempdf.sort_values(periodvar) #resort to original order\n",
    "        \n",
    "    def flip(df, flip):\n",
    "        flip_df = df[df['window'].isin(flip)]\n",
    "        rest = df[~df['window'].isin(flip)]\n",
    "        flip_df = flip_df.sort_values(byvars + [periodvar], ascending=False)\n",
    "        return pd.concat([flip_df, rest], axis=0)\n",
    "    \n",
    "    def _cumulate(array_list, mp=multiprocess):\n",
    "        if multiprocess:\n",
    "            if isinstance(multiprocess, int):\n",
    "                return _cumulate_mp(array_list, mp=mp) #use mp # processors\n",
    "            else:\n",
    "                return _cumulate_mp(array_list) #use all processors\n",
    "        else:\n",
    "            return _cumulate_sp(array_list)\n",
    "    \n",
    "    def _cumulate_sp(array_list):\n",
    "        out_list = []\n",
    "        for array in array_list:\n",
    "            out_list.append(np.cumprod(array, axis=0))\n",
    "        return np.concatenate(out_list, axis=0)\n",
    "    \n",
    "    def _cumulate_mp(array_list, mp=None):\n",
    "        if mp:\n",
    "            with Pool(mp) as pool: #use mp # processors\n",
    "                return _cumulate_mp_main(array_list, pool)\n",
    "        else:\n",
    "            with Pool() as pool: #use all processors\n",
    "                return _cumulate_mp_main(array_list, pool)\n",
    "        \n",
    "    def _cumulate_mp_main(array_list, pool):\n",
    "        \n",
    "        #For time estimation\n",
    "        counter = []\n",
    "        num_loops = len(array_list)\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        #Mp setup\n",
    "        cum = functools.partial(np.cumprod, axis=0)\n",
    "        results = [pool.apply_async(cum, (arr,), callback=counter.append) for arr in array_list]\n",
    "        \n",
    "        #Time estimation\n",
    "        while len(counter) < num_loops:\n",
    "            estimate_time(num_loops, len(counter), start_time)\n",
    "            time2.sleep(0.5)\n",
    "            \n",
    "        #Collect and output results. A timeout of 1 should be fine because\n",
    "        #it should wait until completion anyway\n",
    "        return np.concatenate([r.get(timeout=1) for r in results], axis=0)\n",
    "        \n",
    "\n",
    "    def split(df, cumvars, periodvar):\n",
    "        \"\"\"\n",
    "        Splits a dataframe into a list of arrays based on a key variable\n",
    "        \"\"\"\n",
    "#         df = df.sort_values(['__key_var__', periodvar])\n",
    "        small_df = df[['__key_var__'] + cumvars]\n",
    "        arr = small_df.values\n",
    "        splits = []\n",
    "        for i in range(arr.shape[0]):\n",
    "            if i == 0: continue\n",
    "            if arr[i,0] != arr[i-1,0]: #different key\n",
    "                splits.append(i)\n",
    "        return np.split(arr[:,1:], splits)\n",
    "    \n",
    "    #####TEMPORARY CODE######\n",
    "    assert method.lower() != 'zero'\n",
    "    #########################\n",
    "    \n",
    "    if isinstance(byvars, str):\n",
    "        byvars = [byvars]\n",
    "    \n",
    "    assert method.lower() in ('zero','between','first')\n",
    "    assert not ((method.lower() == 'between') and (time == None)) #need time for between method\n",
    "    if time != None and method.lower() != 'between':\n",
    "        warnings.warn('Time provided but method was not between. Time will be ignored.')\n",
    "\n",
    "    #Creates a variable containing index of window in which the observation belongs\n",
    "    if method.lower() == 'between':\n",
    "        df = _map_windows(df, sort_time, method=method, periodvar=periodvar, byvars=byvars)\n",
    "    else:\n",
    "        df['__map_window__'] = 1\n",
    "        df.loc[df[periodvar] == min(df[periodvar]), '__map_window__'] = 0\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    ####################TEMP\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    #######################\n",
    "    \n",
    "    \n",
    "    if not byvars:  byvars = ['__map_window__']\n",
    "    else: byvars.append('__map_window__')\n",
    "    assert isinstance(byvars, list)\n",
    "    \n",
    "    #need to determine when to cumulate backwards\n",
    "    #check if method is zero, there only negatives and zero, and there is at least one negative in each window\n",
    "    if method.lower() == 'zero': \n",
    "        #flip is a list of indices of windows for which the window should be flipped\n",
    "        flip = [j for j, window in enumerate(windows) \\\n",
    "               if all([i <= 0 for i in window]) and any([i < 0 for i in window])]\n",
    "        df = flip(df, flip)\n",
    "        \n",
    "\n",
    "    log('Creating by groups.')\n",
    "\n",
    "    #Create by groups\n",
    "    df['__key_var__'] = '__key_var__' #container for key\n",
    "    for col in [df[c].astype(str) for c in byvars]:\n",
    "        df['__key_var__'] += col\n",
    "\n",
    "    array_list = split(df, cumvars, periodvar)\n",
    "    \n",
    "#     container_array = df[cumvars].values\n",
    "    full_array = _cumulate(array_list)\n",
    "    \n",
    "    new_cumvars = ['cum_' + str(c) for c in cumvars]\n",
    "\n",
    "    cumdf = pd.DataFrame(full_array, columns=new_cumvars, dtype=np.float64)\n",
    "    outdf = pd.concat([df.reset_index(drop=True), cumdf], axis=1)\n",
    "    \n",
    "    if method.lower == 'zero' and flip != []: #if we flipped some of the dataframe\n",
    "        pass #TEMPORARY\n",
    "    \n",
    "    \n",
    "    \n",
    "    if grossify:\n",
    "        all_cumvars = cumvars + new_cumvars\n",
    "        for col in all_cumvars:\n",
    "            outdf[col] = outdf[col] - 1\n",
    "    \n",
    "    drop_cols = [col for col in outdf.columns if col.startswith('__')]\n",
    "    \n",
    "    return outdf.drop(drop_cols, axis=1)\n",
    "\n",
    "def long_to_wide(df, groupvars, values, colindex=None):\n",
    "    '''\n",
    "    \n",
    "    groupvars = string or list of variables which signify unique observations in the output dataset\n",
    "    values = string or list of variables which contain the values which need to be transposed\n",
    "    colindex = variable containing extension for column name in the output dataset. If not specified, just uses the\n",
    "               count of the row within the group.\n",
    "    \n",
    "    NOTE: Don't have any variables named key or idx\n",
    "    \n",
    "    For example, if we had a long dataset of returns, with returns 12, 24, 36, 48, and 60 months after the date:\n",
    "            ticker    ret    months\n",
    "            AA        .01    12\n",
    "            AA        .15    24\n",
    "            AA        .21    36\n",
    "            AA       -.10    48\n",
    "            AA        .22    60\n",
    "    and we want to get this to one observation per ticker:\n",
    "            ticker    ret12    ret24    ret36    ret48    ret60    \n",
    "            AA        .01      .15      .21     -.10      .22\n",
    "    We would use:\n",
    "    long_to_wide(df, groupvars='ticker', values='ret', colindex='months')\n",
    "    '''\n",
    "    \n",
    "    df = df.copy() #don't overwrite original\n",
    "    \n",
    "    #Check for duplicates\n",
    "    if df.duplicated().any():\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        warnings.warn('Found duplicate rows and deleted.')\n",
    "    \n",
    "    #Ensure type of groupvars is correct\n",
    "    if isinstance(groupvars,str):\n",
    "        groupvars = [groupvars]\n",
    "    assert isinstance(groupvars, list)\n",
    "    \n",
    "    #Ensure type of values is correct\n",
    "    if isinstance(values,str):\n",
    "        values = [values]\n",
    "    assert isinstance(values, list)\n",
    "    #Use count of the row within the group for column index if not specified\n",
    "    if colindex == None:\n",
    "        df['__idx__'] = df.groupby(groupvars).cumcount()\n",
    "        colindex = '__idx__'\n",
    "    \n",
    "    df['__key__'] = df[groupvars[0]].astype(str) #create key variable\n",
    "    if len(groupvars) > 1: #if there are multiple groupvars, combine into one key\n",
    "        for var in groupvars[1:]:\n",
    "            df['__key__'] = df['__key__'] + '_' + df[var].astype(str)\n",
    "    \n",
    "    #Create seperate wide datasets for each value variable then merge them together\n",
    "    for i, value in enumerate(values):\n",
    "        if i == 0:\n",
    "            combined = df.copy()\n",
    "        #Create wide dataset\n",
    "        raw_wide = df.pivot(index='__key__', columns=colindex, values=value)\n",
    "        raw_wide.columns = [value + str(col) for col in raw_wide.columns]\n",
    "        wide = raw_wide.reset_index()\n",
    "\n",
    "        #Merge back to original dataset\n",
    "        combined = combined.merge(wide, how='left', on='__key__')\n",
    "    \n",
    "    return combined.drop([colindex,'__key__'] + values, axis=1).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def load_sas(filepath, csv=True, **read_csv_kwargs):  \n",
    "    sas_name = os.path.basename(filepath) #e.g. dsename.sas7bdat\n",
    "    folder = os.path.dirname(filepath) #location of sas file\n",
    "    filename, extension = os.path.splitext(sas_name) #returns ('dsenames','.sas7bdat')\n",
    "    csv_name = filename + '.csv'\n",
    "    csv_path = os.path.join(folder, csv_name)\n",
    "    \n",
    "    if os.path.exists(csv_path) and csv:\n",
    "        if os.path.getmtime(csv_path) > os.path.getmtime(filepath): #if csv was modified more recently\n",
    "            #Read from csv (don't touch sas7bdat because slower loading)\n",
    "            try: return pd.read_csv(csv_path, encoding='utf-8', **read_csv_kwargs)\n",
    "            except UnicodeDecodeError: return pd.read_csv(csv_path, encoding='cp1252', **read_csv_kwargs)\n",
    "    \n",
    "    #In the case that there is no csv already, or that the sas7bdat has been modified more recently\n",
    "    #Pull from SAS file\n",
    "    df = SAS7BDAT(filepath).to_data_frame()\n",
    "    #Write to csv file\n",
    "    if csv:\n",
    "        to_csv(df, folder, filename, output=False, index=False)\n",
    "    return df\n",
    "\n",
    "def averages(df, avgvars, byvars, wtvar=None, count=False, flatten=True):\n",
    "    '''\n",
    "    Returns equal- and value-weighted averages of variables within groups\n",
    "    \n",
    "    avgvars: List of strings or string of variable names to take averages of\n",
    "    byvars: List of strings or string of variable names for by groups\n",
    "    wtvar: String of variable to use for calculating weights in weighted average\n",
    "    count: False or string of variable name, pass variable name to get count of non-missing\n",
    "           of that variable within groups.\n",
    "    flatten: Boolean, False to return df with multi-level index\n",
    "    '''\n",
    "    #Check types\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    if isinstance(avgvars, str): avgvars = [avgvars]\n",
    "    else:\n",
    "        assert isinstance(avgvars, list)\n",
    "    assert isinstance(byvars, (str, list))\n",
    "    if wtvar != None:\n",
    "        assert isinstance(wtvar, str)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if count:\n",
    "        df = groupby_merge(df, byvars, 'count', subset=count)\n",
    "        avgvars += [count + '_count']\n",
    "    \n",
    "    g = df.groupby(byvars)\n",
    "    avg_df  = g.mean()[avgvars]\n",
    "    \n",
    "    if wtvar == None:\n",
    "        if flatten:\n",
    "            return avg_df.reset_index()\n",
    "        else:\n",
    "            return avg_df\n",
    "    \n",
    "    for var in avgvars:\n",
    "        colname = var + '_wavg'\n",
    "        df[colname] = df[wtvar] / g[wtvar].transform('sum') * df[var]\n",
    "    \n",
    "    wavg_cols = [col for col in df.columns if col[-4:] == 'wavg']\n",
    "    \n",
    "    g = df.groupby(byvars) #recreate because we not have _wavg cols in df\n",
    "    wavg_df = g.sum()[wavg_cols]\n",
    "    \n",
    "    outdf = pd.concat([avg_df,wavg_df], axis=1)\n",
    "    \n",
    "    if flatten:\n",
    "        return outdf.reset_index()\n",
    "    else:\n",
    "        return outdf\n",
    "    \n",
    "def portfolio(df, groupvar, ngroups=10, byvars=None, cutdf=None, portvar='portfolio',\n",
    "              multiprocess=False):\n",
    "    '''\n",
    "    Constructs portfolios based on percentile values of groupvar. If ngroups=10, then will form 10 portfolios,\n",
    "    with portfolio 1 having the bottom 10 percentile of groupvar, and portfolio 10 having the top 10 percentile\n",
    "    of groupvar.\n",
    "    \n",
    "    df: pandas dataframe, input data\n",
    "    groupvar: string, name of variable in df to form portfolios on\n",
    "    ngroups: integer, number of portfolios to form\n",
    "    byvars: string, list, or None, name of variable(s) in df, finds portfolios within byvars. For example if byvars='Month',\n",
    "            would take each month and form portfolios based on the percentiles of the groupvar during only that month\n",
    "    cutdf: pandas dataframe or None, optionally determine percentiles using another dataset. See second note.\n",
    "    portvar: string, name of portfolio variable in the output dataset\n",
    "    multiprocess: bool or int, set to True to use all available processors, \n",
    "                  set to False to use only one, pass an int less or equal to than number of \n",
    "                  processors to use that amount of processors \n",
    "    \n",
    "    NOTE: Resets index and drops in output data, so don't use if index is important (input data not affected)\n",
    "    NOTE: If using a cutdf, MUST have the same bygroups as df. The number of observations within each bygroup\n",
    "          can be different, but there MUST be a one-to-one match of bygroups, or this will NOT work correctly.\n",
    "          This may require some cleaning of the cutdf first.\n",
    "    NOTE: For some reason, multiprocessing seems to be slower in testing, so it is disabled by default\n",
    "    '''\n",
    "    #Check types\n",
    "    _check_portfolio_inputs(df, groupvar, ngroups=ngroups, byvars=byvars, cutdf=cutdf, portvar=portvar)\n",
    "    byvars = _assert_byvars_list(byvars)\n",
    "    if cutdf != None:\n",
    "        assert isinstance(cutdf, pd.DataFrame)\n",
    "    else: #this is where cutdf == None, the default case\n",
    "        cutdf = df\n",
    "        tempcutdf = cutdf.copy()\n",
    "    \n",
    "    pct_per_group = 100/ngroups\n",
    "    percentiles = [i*pct_per_group for i in range(ngroups)] #percentile values, e.g. 0, 10, 20, 30... 100\n",
    "    percentiles += [100]\n",
    "    \n",
    "#     pct_per_group = int(100/ngroups)\n",
    "#     percentiles = [i for i in range(0, 100 + pct_per_group, pct_per_group)] #percentile values, e.g. 0, 10, 20, 30... 100\n",
    "    \n",
    "    #Create new functions with common arguments added\n",
    "    create_cutoffs_and_sort_into_ports = functools.partial(_create_cutoffs_and_sort_into_ports, \n",
    "                                       groupvar=groupvar, portvar=portvar, percentiles=percentiles)\n",
    "    split = functools.partial(_split, keepvars=[groupvar], force_numeric=True)\n",
    "    sort_arr_list_into_ports_and_return_series = functools.partial(_sort_arr_list_into_ports_and_return_series,\n",
    "                                                         percentiles=percentiles,\n",
    "                                                         multiprocess=multiprocess)\n",
    "    \n",
    "    tempdf = df.copy()\n",
    "    \n",
    "    #If there are no byvars, just complete portfolio sort\n",
    "    if byvars == None: return create_cutoffs_and_sort_into_ports(tempdf, cutdf)\n",
    "    \n",
    "    #The below rename is incase there is already a variable named index in the data\n",
    "    #The rename will just not do anything if there's not\n",
    "    tempdf = tempdf.reset_index(drop=True).rename(\n",
    "        columns={'index':'__temp_index__'}).reset_index() #get a variable 'index' containing obs count\n",
    "    \n",
    "    #Also replace index in byvars if there\n",
    "    temp_byvars = [b if b != 'index' else '__temp_index__' for b in byvars]\n",
    "    all_byvars = [temp_byvars, byvars] #list of lists\n",
    "    \n",
    "    #else, deal with byvars\n",
    "    #First create a key variable based on all the byvars\n",
    "    for i, this_df in enumerate([tempdf, tempcutdf]):\n",
    "        this_df['__key_var__'] = 'key' #container for key\n",
    "        for col in [this_df[c].astype(str) for c in all_byvars[i]]:\n",
    "            this_df['__key_var__'] += col\n",
    "        this_df.sort_values('__key_var__', inplace=True)\n",
    "    \n",
    "    #Now split into list of arrays and process\n",
    "    array_list = split(tempdf)\n",
    "    cut_array_list = split(tempcutdf)\n",
    "\n",
    "    tempdf = tempdf.reset_index(drop=True) #need to reset index again for adding new column\n",
    "    tempdf[portvar] = sort_arr_list_into_ports_and_return_series(array_list, cut_array_list)\n",
    "    return tempdf.sort_values('index').drop(['__key_var__','index'], axis=1).rename(\n",
    "                columns={'__temp_index__':'index'}).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "def portfolio_averages(df, groupvar, avgvars, ngroups=10, byvars=None, cutdf=None, wtvar=None,\n",
    "                       count=False, portvar='portfolio', avgonly=False):\n",
    "    '''\n",
    "    Creates portfolios and calculates equal- and value-weighted averages of variables within portfolios. If ngroups=10,\n",
    "    then will form 10 portfolios, with portfolio 1 having the bottom 10 percentile of groupvar, and portfolio 10 having \n",
    "    the top 10 percentile of groupvar.\n",
    "    \n",
    "    df: pandas dataframe, input data\n",
    "    groupvar: string, name of variable in df to form portfolios on\n",
    "    avgvars: string or list, variables to be averaged\n",
    "    ngroups: integer, number of portfolios to form\n",
    "    byvars: string, list, or None, name of variable(s) in df, finds portfolios within byvars. For example if byvars='Month',\n",
    "            would take each month and form portfolios based on the percentiles of the groupvar during only that month\n",
    "    cutdf: pandas dataframe or None, optionally determine percentiles using another dataset\n",
    "    wtvar: string, name of variable in df to use for weighting in weighted average\n",
    "    count: False or string of variable name, pass variable name to get count of non-missing\n",
    "           of that variable within groups.\n",
    "    portvar: string, name of portfolio variable in the output dataset\n",
    "    avgonly: boolean, True to return only averages, False to return (averages, individual observations with portfolios)\n",
    "    \n",
    "    NOTE: Resets index and drops in output data, so don't use if index is important (input data not affected)\n",
    "    '''\n",
    "    ports = portfolio(df, groupvar, ngroups=ngroups, byvars=byvars, cutdf=cutdf, portvar=portvar)\n",
    "    if byvars:\n",
    "        assert isinstance(byvars, (str, list))\n",
    "        if isinstance(byvars, str): byvars = [byvars]\n",
    "        by = [portvar] + byvars\n",
    "        avgs = averages(ports, avgvars, byvars=by, wtvar=wtvar, count=count)\n",
    "    else:\n",
    "        avgs = averages(ports, avgvars, byvars=portvar, wtvar=wtvar, count=count)\n",
    "    \n",
    "    if avgonly:\n",
    "        return avgs\n",
    "    else:\n",
    "        return avgs, ports\n",
    "    \n",
    "def reg_by(df, yvar, xvars, groupvar, merge=False, cons=True):\n",
    "    \"\"\"\n",
    "    Runs a regression of df[yvar] on df[xvars] by values of groupvar. Outputs a dataframe with values of \n",
    "    groupvar and corresponding coefficients, unless merge=True, then outputs the original dataframe with the\n",
    "    appropriate coefficients merged in.\n",
    "    \n",
    "    Required inputs:\n",
    "    groupvar: str or list of strs, column names of columns identifying by groups\n",
    "    \n",
    "    Optional Options:\n",
    "    cons: True to include a constant, False to not\n",
    "    \"\"\"   \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    if isinstance(xvars, str):\n",
    "        xvars = [xvars]\n",
    "    assert isinstance(xvars, list)\n",
    "    \n",
    "    drop_group = False\n",
    "    if isinstance(groupvar, list):\n",
    "        df['__key_regby__'] = ''\n",
    "        for var in groupvar:\n",
    "            df['__key_regby__'] = df['__key_regby__'] + df[var].astype(str)\n",
    "        groupvar = '__key_regby__'\n",
    "        drop_group = True\n",
    "    \n",
    "    for group in df[groupvar].unique():\n",
    "        tempdf = df[df[groupvar] == group][xvars + [yvar]].dropna() #will fail with nans\n",
    "        X = tempdf[xvars]\n",
    "        \n",
    "        if cons:\n",
    "            X = sm.add_constant(X)\n",
    "        \n",
    "        y = tempdf[yvar]\n",
    "\n",
    "        if len(tempdf.index) > len(xvars) + 1: #if enough observations, run regression\n",
    "            model = sm.OLS(y, X)\n",
    "            result = model.fit()\n",
    "            this_result = pd.DataFrame(result.params).T\n",
    "        else: #not enough obs, return nans \n",
    "            if cons:\n",
    "                rhs = ['const'] + xvars \n",
    "            else:\n",
    "                rhs = xvars\n",
    "            this_result = pd.DataFrame([tuple([nan for col in rhs])], columns=rhs)\n",
    "        \n",
    "        this_result[groupvar] = group\n",
    "        result_df = result_df.append(this_result) #  Or whatever summary info you want\n",
    "    \n",
    "    result_df.columns = ['coef_' + col if col not in (groupvar, 'const') else col for col in result_df.columns]\n",
    "    \n",
    "    if merge:\n",
    "        out = df.merge(result_df, how='left', on=groupvar)\n",
    "        if drop_group:\n",
    "            out.drop(groupvar, axis=1, inplace=True)\n",
    "        return out\n",
    "    \n",
    "    return result_df.reset_index(drop=True)\n",
    "\n",
    "def factor_reg_by(df, groupvar, fac=4, retvar='RET'):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with RET, mktrf, smb, hml, and umd, and produces abnormal returns by groups.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas datafram containing mktrf, smb, hml, umd, (or what's required for chosen model)\n",
    "        and a return variable\n",
    "    groupvar: str or list of strs, column names of columns on which to form by groups\n",
    "    fac: int (1, 3, 4), factor model to run\n",
    "    retvar: str, name of column containing returns\n",
    "    \"\"\"\n",
    "    assert fac in (1, 3, 4)\n",
    "    factors = ['mktrf']\n",
    "    if fac >= 3:\n",
    "        factors += ['smb','hml']\n",
    "    if fac == 4:\n",
    "        factors += ['umd']\n",
    "        \n",
    "#     factor_loadings = reg_by(df, 'RET', factors, groupvar)\n",
    "#     outdf = df.merge(factor_loadings, how='left', on=groupvar) #merge back to sample\n",
    "    outdf = reg_by(df, retvar, factors, groupvar, merge=True)\n",
    "    outdf['AB' + retvar] = outdf[retvar] - sum([outdf[fac] * outdf['coef_' + fac] for fac in factors]) #create abnormal returns\n",
    "    return outdf\n",
    "\n",
    "def state_abbrev(df, col, toabbrev=False):\n",
    "    df = df.copy()\n",
    "    states_to_abbrev = {\n",
    "    'Alabama': 'AL', \n",
    "    'Montana': 'MT',\n",
    "    'Alaska': 'AK', \n",
    "    'Nebraska': 'NE',\n",
    "    'Arizona': 'AZ', \n",
    "    'Nevada': 'NV',\n",
    "    'Arkansas': 'AR', \n",
    "    'New Hampshire': 'NH',\n",
    "    'California': 'CA', \n",
    "    'New Jersey': 'NJ',\n",
    "    'Colorado': 'CO', \n",
    "    'New Mexico': 'NM',\n",
    "    'Connecticut': 'CT', \n",
    "    'New York': 'NY',\n",
    "    'Delaware': 'DE', \n",
    "    'North Carolina': 'NC',\n",
    "    'Florida': 'FL', \n",
    "    'North Dakota': 'ND',\n",
    "    'Georgia': 'GA', \n",
    "    'Ohio': 'OH',\n",
    "    'Hawaii': 'HI', \n",
    "    'Oklahoma': 'OK',\n",
    "    'Idaho': 'ID', \n",
    "    'Oregon': 'OR',\n",
    "    'Illinois': 'IL', \n",
    "    'Pennsylvania': 'PA',\n",
    "    'Indiana': 'IN', \n",
    "    'Rhode Island': 'RI',\n",
    "    'Iowa': 'IA', \n",
    "    'South Carolina': 'SC',\n",
    "    'Kansas': 'KS', \n",
    "    'South Dakota': 'SD',\n",
    "    'Kentucky': 'KY', \n",
    "    'Tennessee': 'TN',\n",
    "    'Louisiana': 'LA', \n",
    "    'Texas': 'TX',\n",
    "    'Maine': 'ME', \n",
    "    'Utah': 'UT',\n",
    "    'Maryland': 'MD', \n",
    "    'Vermont': 'VT',\n",
    "    'Massachusetts': 'MA', \n",
    "    'Virginia': 'VA',\n",
    "    'Michigan': 'MI', \n",
    "    'Washington': 'WA',\n",
    "    'Minnesota': 'MN', \n",
    "    'West Virginia': 'WV',\n",
    "    'Mississippi': 'MS', \n",
    "    'Wisconsin': 'WI',\n",
    "    'Missouri': 'MO', \n",
    "    'Wyoming': 'WY', }\n",
    "    if toabbrev:\n",
    "        df[col] = df[col].replace(states_to_abbrev)\n",
    "    else:\n",
    "        abbrev_to_states = dict ( (v,k) for k, v in states_to_abbrev.items() )\n",
    "        df[col] = df[col].replace(abbrev_to_states)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_not_trade_days(tradedays_path= r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\Other SAS\\tradedays.sas7bdat'):\n",
    "    df = dero.load_sas(tradedays_path)\n",
    "    trading_days = pd.to_datetime(df['date']).tolist()\n",
    "    all_days = pd.date_range(start=trading_days[0],end=trading_days[-1]).tolist()\n",
    "    notrade_days = [day for day in all_days if day not in trading_days]\n",
    "    \n",
    "    outdir = os.path.dirname(tradedays_path)\n",
    "    outpath = os.path.join(outdir, 'not tradedays.csv')\n",
    "    \n",
    "    with open(outpath, 'w') as f:\n",
    "        f.write('date\\n')\n",
    "        f.write('\\n'.join([day.date().isoformat() for day in notrade_days]))\n",
    "        \n",
    "def tradedays(notradedays_path=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\Other SAS\\not tradedays.csv'):\n",
    "    notrade_days = pd.read_csv(notradedays_path)['date'].tolist()\n",
    "    return CustomBusinessDay(holidays=notrade_days)\n",
    "\n",
    "def select_rows_by_condition_on_columns(df, cols, condition='== 1', logic='or'):\n",
    "    \"\"\"\n",
    "    Selects rows of a pandas dataframe by evaluating a condition on a subset of the dataframe's columns.\n",
    "    \n",
    "    df: pandas dataframe\n",
    "    cols: list of column names, the subset of columns on which to evaluate conditions\n",
    "    condition: string, needs to contain comparison operator and right hand side of comparison. For example,\n",
    "               '== 1' checks for each row that the value of each column is equal to one.\n",
    "    logic: 'or' or 'and'. With 'or', only one of the columns in cols need to match the condition for the row to be kept.\n",
    "            With 'and', all of the columns in cols need to match the condition.\n",
    "    \"\"\"\n",
    "    #First eliminate spaces in columns, this method will not work with spaces\n",
    "    new_cols = [col.replace(' ','_').replace('.','_') for col in cols]\n",
    "    df.rename(columns={col:new_col for col, new_col in zip(cols, new_cols)}, inplace=True)\n",
    "    \n",
    "    #Now create a string to query the dataframe with\n",
    "    logic_spaces = ' ' + logic + ' '\n",
    "    query_str = logic_spaces.join([str(col) + condition for col in new_cols]) #'col1 == 1, col2 == 1', etc.\n",
    "    \n",
    "    #Query dataframe\n",
    "    outdf = df.query(query_str).copy()\n",
    "    \n",
    "    #Rename columns back to original\n",
    "    outdf.rename(columns={new_col:col for col, new_col in zip(cols, new_cols)}, inplace=True)\n",
    "    \n",
    "    return outdf\n",
    "\n",
    "def show_df(df):\n",
    "    pool = ThreadPool(1)\n",
    "    pool.apply_async(_show_df, args=[df])\n",
    "    \n",
    "def _show_df(df):\n",
    "    root = Tk()\n",
    "    frame = Frame(root)\n",
    "    frame.pack(fill=BOTH, expand=YES)\n",
    "    pt = Table(parent=frame, dataframe=df)\n",
    "    pt.show()\n",
    "    pt.queryBar()\n",
    "    root.mainloop()\n",
    "    \n",
    "def groupby_merge(df, byvars, func_str, *func_args, subset='all', replace=False):\n",
    "    \"\"\"\n",
    "    Creates a pandas groupby object, applies the aggregation function in func_str, and merges back the \n",
    "    aggregated data to the original dataframe.\n",
    "    \n",
    "    Required Inputs:\n",
    "    df: Pandas DataFrame\n",
    "    byvars: str or list, column names which uniquely identify groups\n",
    "    func_str: str, name of groupby aggregation function such as 'min', 'max', 'sum', 'count', etc.\n",
    "    \n",
    "    Optional Input:\n",
    "    subset: str or list, column names for which to apply aggregation functions\n",
    "    func_args: tuple, arguments to pass to func\n",
    "    replace: bool, True to replace original columns in the data with aggregated/transformed columns\n",
    "    \n",
    "    Usage:\n",
    "    df = groupby_merge(df, ['PERMNO','byvar'], 'max', subset='RET')\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert byvars to list if neceessary\n",
    "    if isinstance(byvars, str):\n",
    "        byvars = [byvars]\n",
    "    \n",
    "    #Store all variables except byvar in subset if subset is 'all'\n",
    "    if subset == 'all':\n",
    "        subset = [col for col in df.columns if col not in byvars]\n",
    "        \n",
    "    #Convert subset to list if necessary\n",
    "    if isinstance(subset, str):\n",
    "        subset = [subset]\n",
    "    \n",
    "    if func_str == 'transform':\n",
    "        #transform works very differently from other aggregation functions\n",
    "        \n",
    "        #First we need to deal with nans in the by variables. If there are any nans, transform will error out\n",
    "        #Therefore we must fill the nans in the by variables beforehand and replace afterwards\n",
    "        df[byvars] = df[byvars].fillna(value='__tempnan__')\n",
    "        \n",
    "        #Now we must deal with nans in the subset variables. If there are any nans, tranform will error out\n",
    "        #because it tries to ignore the nan. Therefore we must remove these rows from the dataframe,\n",
    "        #transform, then add those rows back.\n",
    "        any_nan_subset_mask = pd.Series([all(i) for i in \\\n",
    "                                        (zip(*[~pd.isnull(df[col]) for col in subset]))],\n",
    "                                        index=df.index)\n",
    "        no_nans = df[any_nan_subset_mask]\n",
    "        \n",
    "        grouped = no_nans.groupby(byvars)\n",
    "        func = getattr(grouped, func_str) #pull method of groupby class with same name as func_str\n",
    "        grouped = func(*func_args)[subset] #apply the class method and select subset columns\n",
    "        grouped.columns = [col + '_' + func_str for col in grouped.columns] #rename transformed columns\n",
    "        \n",
    "        df.replace('__tempnan__', nan, inplace=True) #fill nan back into dataframe\n",
    "        \n",
    "        #Put nan rows back\n",
    "        grouped = grouped.reindex(df.index)\n",
    "        \n",
    "        full = pd.concat([df, grouped], axis=1)\n",
    "        \n",
    "    else: #.min(), .max(), etc.\n",
    "        \n",
    "        \n",
    "        \n",
    "#         grouped = df.groupby(byvars, as_index=False)[byvars + subset]\n",
    "#         func = getattr(grouped, func_str) #pull method of groupby class with same name as func_str\n",
    "#         grouped = func(*func_args) #apply the class method\n",
    "\n",
    "\n",
    "        grouped = df.groupby(byvars)[subset]\n",
    "        func = getattr(grouped, func_str) #pull method of groupby class with same name as func_str\n",
    "        grouped = func(*func_args) #apply the class method\n",
    "        grouped = grouped.reset_index()\n",
    "        \n",
    "        \n",
    "        #Merge and output\n",
    "        full = df.merge(grouped, how='left', on=byvars, suffixes=['','_' + func_str])\n",
    "    \n",
    "    if replace:\n",
    "        _replace_with_transformed(full, func_str)\n",
    "    \n",
    "    return full\n",
    "    \n",
    "def _replace_with_transformed(df, func_str='transform'):\n",
    "    transform_cols = [col for col in df.columns if col.endswith('_' + func_str)]\n",
    "    orig_names = [col[:col.find('_' + func_str)] for col in transform_cols]\n",
    "    df.drop(orig_names, axis=1, inplace=True)\n",
    "    df.rename(columns={old: new for old, new in zip(transform_cols, orig_names)}, inplace=True)\n",
    "    \n",
    "def groupby_index(df, byvars, sortvars=None, ascending=True):\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is a copy of the old one with an additional column containing an index\n",
    "    by groups. Each time the bygroup changes, the index restarts at 0.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas DataFrame\n",
    "    byvars: str or list of column names containing group identifiers\n",
    "    \n",
    "    Optional inputs:\n",
    "    sortvars: str or list of column names to sort by within by groups\n",
    "    ascending: bool, direction of sort\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert sortvars to list if necessary\n",
    "    if isinstance(sortvars, str):\n",
    "        sortvars = [sortvars]\n",
    "    if sortvars == None: sortvars = []\n",
    "    \n",
    "    df = df.copy() #don't modify the original dataframe\n",
    "    df.sort_values(byvars + sortvars, inplace=True, ascending=ascending)\n",
    "    df['__temp_cons__'] = 1\n",
    "    df = groupby_merge(df, byvars, 'transform', (lambda x: [i for i in range(len(x))]), subset=['__temp_cons__'])\n",
    "    df.drop('__temp_cons__', axis=1, inplace=True)\n",
    "    return df.rename(columns={'__temp_cons___transform': 'group_index'})\n",
    "\n",
    "def to_copy_paste(df, index=False, column_names=True):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and prints all of its data in such a format that it can be copy-pasted to create\n",
    "    a new dataframe from the pandas.DataFrame() constructor.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe\n",
    "    \n",
    "    Optional inputs:\n",
    "    index: bool, True to include index\n",
    "    column_names: bool, False to exclude column names\n",
    "    \"\"\"\n",
    "    print('pd.DataFrame(data = [')\n",
    "    for tup in df.iterrows():        \n",
    "        data = tup[1].values\n",
    "        print(str(tuple(data)) + ',')\n",
    "    last_line = ']'\n",
    "    if column_names:\n",
    "        last_line += ', columns = {}'.format([i for i in df.columns]) #list comp to remove Index() around cols\n",
    "    if index:\n",
    "        last_line += ',\\n index = {}'.format([i for i in df.index]) #list comp to remove Index() around index\n",
    "    last_line += ')' #end command\n",
    "    print(last_line)\n",
    "    \n",
    "def _join_col_strings(*args):\n",
    "    strs = [str(arg) for arg in args]\n",
    "    return '_'.join(strs)\n",
    "\n",
    "def join_col_strings(df, cols):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and column name(s) and concatenates string versions of the columns with those names.\n",
    "    Useful for when a group is identified by several variables and we need one key variable to describe a group.\n",
    "    Returns a pandas Series.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe\n",
    "    cols: str or list, names of columns in df to be concatenated\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "    assert isinstance(cols, list)\n",
    "    \n",
    "    jc = np.vectorize(_join_col_strings)\n",
    "    \n",
    "    return pd.Series(jc(*[df[col] for col in cols]))\n",
    "\n",
    "def winsorize(df, pct, subset=None, byvars=None, bot=True, top=True):\n",
    "    \"\"\"\n",
    "    Finds observations above the pct percentile and replaces the with the pct percentile value.\n",
    "    Does this for all columns, or the subset given by subset\n",
    "    \n",
    "    Required inputs:\n",
    "    df: Pandas dataframe\n",
    "    pct: 0 < float < 1 or list of two values 0 < float < 1. If two values are given, the first\n",
    "         will be used for the bottom percentile and the second will be used for the top. If one value\n",
    "         is given and both bot and top are True, will use the same value for both.\n",
    "    \n",
    "    Optional inputs:\n",
    "    subset: List of strings or string of column name(s) to winsorize\n",
    "    byvars: str, list of strs, or None. Column names of columns identifying groups in the data.\n",
    "            Winsorizing will be done within those groups.\n",
    "    bot: bool, True to winsorize bottom observations\n",
    "    top: bool, True to winsorize top observations\n",
    "    \n",
    "    Example usage:\n",
    "    winsorize(df, .05, subset='RET') #replaces observations of RET below the 5% and above the 95% values\n",
    "    winsorize(df, [.05, .1], subset='RET') #replaces observations of RET below the 5% and above the 90% values\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Check inputs\n",
    "    assert any([bot, top]) #must winsorize something\n",
    "    if isinstance(pct, float):\n",
    "        bot_pct = pct\n",
    "        top_pct = 1 - pct\n",
    "    elif isinstance(pct, list):\n",
    "        bot_pct = pct[0]\n",
    "        top_pct = 1 - pct[1]\n",
    "    else:\n",
    "        raise ValueError('pct must be float or a list of two floats')\n",
    "        \n",
    "    def temp_winsor(col):\n",
    "        return _winsorize(col, top_pct, bot_pct, top=top, bot=bot)\n",
    "\n",
    "    #Save column order\n",
    "    cols = df.columns\n",
    "    \n",
    "    #Get a dataframe of data to be winsorized, and a dataframe of the other columns\n",
    "    to_winsor, rest = _select_numeric_or_subset(df, subset, extra_include=byvars)\n",
    "\n",
    "    #Now winsorize\n",
    "    if byvars: #use groupby to process groups individually\n",
    "        to_winsor = groupby_merge(to_winsor, byvars, 'transform', (temp_winsor), replace=True)\n",
    "    else: #do entire df, one column at a time\n",
    "        to_winsor.apply(temp_winsor, axis=0)\n",
    "    \n",
    "    return pd.concat([to_winsor,rest], axis=1)[cols]\n",
    "\n",
    "\n",
    "def _winsorize(col, top_pct, bot_pct, top=True, bot=True):\n",
    "    \"\"\"\n",
    "    Winsorizes a pandas Series\n",
    "    \"\"\"\n",
    "    if top:\n",
    "        top_val = col.quantile(top_pct)\n",
    "        col.loc[col > top_val] = top_val\n",
    "    if bot:\n",
    "        bot_val = col.quantile(bot_pct)\n",
    "        col.loc[col < bot_val] = bot_val\n",
    "    return col\n",
    "            \n",
    "def _select_numeric_or_subset(df, subset, extra_include=None):\n",
    "    \"\"\"\n",
    "    If subset is not None, selects all numeric columns. Else selects subset. \n",
    "    If extra_include is not None and subset is None, will select all numeric columns plus \n",
    "    those in extra_include.\n",
    "    Returns a tuple of (dataframe containing subset columns, dataframe of other columns)\n",
    "    \"\"\"\n",
    "    if subset == None:\n",
    "        to_winsor = df.select_dtypes(include=[np.number, np.int64]).copy()\n",
    "        subset    = to_winsor.columns\n",
    "        rest      = df.select_dtypes(exclude=[np.number, np.int64]).copy()\n",
    "    else:\n",
    "        if isinstance(subset, str):\n",
    "            subset = [subset]\n",
    "        assert isinstance(subset, list)\n",
    "        to_winsor = df[subset].copy()\n",
    "        other_cols = [col for col in df.columns if col not in subset]\n",
    "        rest = df[other_cols].copy()\n",
    "    if extra_include:\n",
    "        to_winsor = pd.concat([to_winsor, df[extra_include]], axis=1)\n",
    "        rest.drop(extra_include, axis=1, inplace=True)\n",
    "        \n",
    "    return (to_winsor, rest)\n",
    "\n",
    "def apply_func_to_unique_and_merge(series, func):\n",
    "    \"\"\"\n",
    "    Many Pandas functions can be slow because they're doing repeated work. This function reduces\n",
    "    the given series down to unique values, applies the function, then expands back up to the\n",
    "    original shape of the data. Returns a series.\n",
    "    \n",
    "    Required inputs:\n",
    "    seres: pd.Series\n",
    "    func: function to be applied to the series.\n",
    "    \n",
    "    Usage:\n",
    "    import functools\n",
    "    to_datetime = functools.partial(pd.to_datetime, format='%Y%m')\n",
    "    apply_func_to_unique_and_merge(df['MONTH'], to_datetime)\n",
    "    \"\"\"\n",
    "\n",
    "    unique = pd.Series(series.dropna().unique())\n",
    "    new = unique.apply(func)\n",
    "\n",
    "    for_merge = pd.concat([unique, new], axis=1)\n",
    "    num_cols = [i for i in range(len(for_merge.columns) - 1)] #names of new columns\n",
    "    for_merge.columns = [series.name] + num_cols\n",
    "\n",
    "    orig_df = pd.DataFrame(series)\n",
    "    orig_df.reset_index(inplace=True)\n",
    "\n",
    "    return for_merge.merge(orig_df, how='right', on=[series.name]).sort_values('index').reset_index().loc[:,num_cols]\n",
    "\n",
    "\n",
    "def _map_windows(df, time, method='between', periodvar='Shift Date', byvars=['PERMNO','Date']):\n",
    "    \"\"\"\n",
    "    Returns the dataframe with an additional column __map_window__ containing the index of the window \n",
    "    in which the observation resides. For example, if the windows are\n",
    "    [[1],[2,3]], and the periods are 1/1/2000, 1/2/2000, 1/3/2000 for PERMNO 10516 with byvar\n",
    "    'a', the df rows would be as follows:\n",
    "         (10516, 'a', '1/1/2000', 0),\n",
    "         (10516, 'a', '1/2/2000', 1),\n",
    "         (10516, 'a', '1/3/2000', 1),\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy() #don't overwrite original dataframe\n",
    "    \n",
    "    wm = functools.partial(window_mapping, time, method=method)\n",
    "\n",
    "    df = groupby_merge(df, byvars, 'transform', (wm), subset=periodvar)\n",
    "\n",
    "    return df.rename(columns={periodvar + '_transform': '__map_window__'})\n",
    "\n",
    "def left_merge_latest(df, df2, on, left_datevar='Date', right_datevar='Date',\n",
    "                      limit_years=False, backend='pandas'):\n",
    "    \"\"\"\n",
    "    Left merges df2 to df using on, but grabbing the most recent observation (right_datevar will be\n",
    "    the soonest earlier than left_datevar). Useful for situations where data needs to be merged with\n",
    "    mismatched dates, and just the most recent data available is needed. \n",
    "    \n",
    "    Required inputs:\n",
    "    df: Pandas dataframe containing source data (all rows will be kept), must have on variables\n",
    "        and left_datevar\n",
    "    df2: Pandas dataframe containing data to be merged (only the most recent rows before source\n",
    "        data will be kept)\n",
    "    on: str or list of strs, names of columns on which to match, excluding date\n",
    "    \n",
    "    Optional inputs:\n",
    "    left_datevar: str, name of date variable on which to merge in df\n",
    "    right_datevar: str, name of date variable on which to merge in df2\n",
    "    limit_years: False or int, only applicable for backend='sql'. \n",
    "    backend: str, 'pandas' or 'sql'. Specify the underlying machinery used to perform the merge.\n",
    "             'pandas' means native pandas, while 'sql' uses pandasql. Try 'sql' if you run\n",
    "             out of memory.\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(on, str):\n",
    "        on = [on]\n",
    "        \n",
    "    if backend.lower() in ('pandas','pd'):\n",
    "        return _left_merge_latest_pandas(df, df2, on, left_datevar=left_datevar, right_datevar=right_datevar)\n",
    "    elif backend.lower() in ('sql','pandasql'):\n",
    "        return _left_merge_latest_sql(df, df2, on, left_datevar=left_datevar, right_datevar=right_datevar)\n",
    "    else:\n",
    "        raise ValueError(\"select backend='pandas' or backend='sql'.\")\n",
    "        \n",
    "    \n",
    "def _left_merge_latest_pandas(df, df2, on, left_datevar='Date', right_datevar='Date'):\n",
    "    many = df.loc[:,on + [left_datevar]].merge(df2, on=on, how='left')\n",
    "    \n",
    "    rename = False\n",
    "    #if they are named the same, pandas will automatically add _x and _y to names\n",
    "    if left_datevar == right_datevar: \n",
    "        rename = True #will need to rename the _x datevar for the last step\n",
    "        orig_left_datevar = left_datevar\n",
    "        left_datevar += '_x'\n",
    "        right_datevar += '_y'\n",
    "    \n",
    "    lt = many.loc[many[left_datevar] >= many[right_datevar]] #left with datadates less than date\n",
    "\n",
    "    #find rows within groups which have the maximum right_datevar (soonest before left_datevar)\n",
    "    data_rows = lt.groupby(on + [left_datevar], as_index=False)[right_datevar].max() \\\n",
    "        .merge(lt, on=on + [left_datevar, right_datevar], how='left')\n",
    "        \n",
    "    if rename: #remove the _x for final merge\n",
    "        data_rows.rename(columns={left_datevar: orig_left_datevar}, inplace=True)\n",
    "        return df.merge(data_rows, on=on + [orig_left_datevar], how='left')\n",
    "    \n",
    "    #if no renaming is required, just merge and exit\n",
    "    return df.merge(data_rows, on=on + [left_datevar], how='left')\n",
    "\n",
    "def _left_merge_latest_sql(df, df2, on, left_datevar='Date', right_datevar='Date'):\n",
    "    \n",
    "    if left_datevar == right_datevar:\n",
    "        df2 = df2.copy()\n",
    "        df2.rename(columns={right_datevar: right_datevar + '_y'}, inplace=True)\n",
    "        right_datevar += '_y'\n",
    "    \n",
    "    on_str = ' and \\n    '.join(['a.{0} = b.{0}'.format(i) for i in on])\n",
    "    groupby_str = ', '.join(on)\n",
    "    a_cols = ', '.join(['a.' + col for col in on + [left_datevar]])\n",
    "    b_cols = ', '.join(['b.' + col for col in df2.columns if col not in on])\n",
    "    query = \"\"\"\n",
    "    select {5}, {4}\n",
    "    from\n",
    "        df a\n",
    "    left join\n",
    "        df2 b\n",
    "    on\n",
    "        {0} and\n",
    "        a.{1} >= b.{2}\n",
    "    group by a.{3}, a.{1}\n",
    "    having\n",
    "        b.{2} = max(b.{2})\n",
    "    \"\"\".format(on_str, left_datevar, right_datevar, groupby_str, b_cols, a_cols)\n",
    "    \n",
    "    return df.merge(sql([df, df2], query), on=on + [left_datevar], how='left')\n",
    "    \n",
    "\n",
    "def var_change_by_groups(df, var, byvars, datevar='Date', numlags=1):\n",
    "    \"\"\"\n",
    "    Used for getting variable changes over time within bygroups. \n",
    "    \n",
    "    NOTE: Dataset is not sorted in this process. Sort the data in the order in which you wish\n",
    "          lags to be created before running this command.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing bygroups, a date variable, and variables of interest\n",
    "    var: str or list of strs, column names of variables to get changes\n",
    "    byvars: str or list of strs, column names of variables identifying by groups\n",
    "    \n",
    "    Optional inputs:\n",
    "    datevar: str ot list of strs, column names of variables identifying periods\n",
    "    numlags: int, number of periods to go back to get change\n",
    "    \"\"\"\n",
    "    var, byvars, datevar = [_to_list_if_str(v) for v in [var, byvars, datevar]] #convert to lists\n",
    "    short_df = df.loc[~pd.isnull(df[byvars]).any(axis=1), var + byvars + datevar].drop_duplicates()\n",
    "    for v in var:\n",
    "        short_df[v + '_lag'] = short_df.groupby(byvars)[v].shift(numlags)\n",
    "        short_df[v + '_change'] = short_df[v] - short_df[v + '_lag']\n",
    "    dropvars = [v for v in var] + [v  + '_lag' for v in var]\n",
    "    short_df = short_df.drop(dropvars, axis=1)\n",
    "    return df.merge(short_df, on=datevar + byvars, how='left')\n",
    "\n",
    "def fill_excluded_rows(df, byvars, fillvars=None, **fillna_kwargs):\n",
    "    \"\"\"\n",
    "    Takes a dataframe which does not contain all possible combinations of byvars as rows. Creates\n",
    "    those rows if fillna_kwargs are passed, calls fillna using fillna_kwargs for fillvars.\n",
    "    \n",
    "    For example, df:\n",
    "                 date     id  var\n",
    "        0  2003-06-09 42223C    1\n",
    "        1  2003-06-10 09255G    2\n",
    "    with fillna_for_excluded_rows(df, byvars=['date','id'], fillvars='var', value=0) becomes:\n",
    "                  date     id  var\n",
    "        0  2003-06-09 42223C    1\n",
    "        1  2003-06-10 42223C    0\n",
    "        2  2003-06-09 09255G    0\n",
    "        3  2003-06-10 09255G    2\n",
    "        \n",
    "    Required options:\n",
    "    df: pandas dataframe\n",
    "    byvars: variables on which dataset should be expanded to product. Can pass a str, list of \n",
    "            strs, or a list of pd.Series.\n",
    "    \n",
    "    Optional options:\n",
    "    fillvars: variables to apply fillna to\n",
    "    fillna_kwargs: See pandas.DataFrame.fillna for kwargs, value=0 is common\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    byvars, fillvars = [_to_list_if_str(v) for v in [byvars, fillvars]] #convert to lists\n",
    "    \n",
    "    \n",
    "#     multiindex = [df[i].dropna().unique() for i in byvars]\n",
    "    multiindex = [_to_series_if_str(df, i).dropna().unique() for i in byvars]\n",
    "    byvars = [_to_name_if_series(i) for i in byvars] #get name of any series\n",
    "\n",
    "\n",
    "    all_df = pd.DataFrame(index=pd.MultiIndex.from_product(multiindex)).reset_index()\n",
    "    all_df.columns = byvars\n",
    "    merged = all_df.merge(df, how='left', on=byvars)\n",
    "    \n",
    "    if fillna_kwargs:\n",
    "        fillna_kwargs.update({'inplace':False})\n",
    "        merged[fillvars] = merged[fillvars].fillna(**fillna_kwargs)\n",
    "    return merged\n",
    "\n",
    "def sql(df_list, query):\n",
    "    \"\"\"\n",
    "    Convenience function for running a pandasql query. Keeps track of which variables are of\n",
    "    datetime type, and converts them back after running the sql query.\n",
    "    \n",
    "    NOTE: Ensure that dfs are passed in the order that they are used in the query.\n",
    "    \"\"\"\n",
    "    #Pandasql looks up tables by names given in query. Here we are passed a list of dfs without names.\n",
    "    #Therefore we need to extract the names of the tables from the query, then assign \n",
    "    #those names to the dfs in df_list in the locals dictionary.\n",
    "    table_names = _extract_table_names_from_sql(query)\n",
    "    for i, name in enumerate(table_names):\n",
    "        locals().update({name: df_list[i]})\n",
    "    \n",
    "    #Get date variable column names\n",
    "    datevars = []\n",
    "#     othervars = []\n",
    "    for d in df_list:\n",
    "        datevars += _get_datetime_cols(d)\n",
    "#         othervars += [col for col in d.columns if col not in datevars]\n",
    "    datevars = list(set(datevars)) #remove duplicates\n",
    "#     othervars = list(set(othervars))\n",
    "    \n",
    "    merged = PandaSQL()(query)\n",
    "    \n",
    "    #Convert back to datetime\n",
    "    for date in [d for d in datevars if d in merged.columns]:\n",
    "        merged[date] = pd.to_datetime(merged[date])\n",
    "    return merged\n",
    "\n",
    "def long_short_portfolio(df, portvar, byvars=None, retvars=None, top_minus_bot=True):\n",
    "    \"\"\"\n",
    "    Takes a df with a column of numbered portfolios and creates a new\n",
    "    portfolio which is long the top portfolio and short the bottom portfolio. \n",
    "    Returns a df of long-short portfolio\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing a column with portfolio numbers\n",
    "    portvar: str, name of column containing portfolios\n",
    "    \n",
    "    Optional inputs:\n",
    "    byvars: str or list of strs of column names containing groups for portfolios.\n",
    "            Calculates long-short within these groups. These should be the same groups\n",
    "            in which portfolios were formed.\n",
    "    retvars: str or list of strs of variables to return in the long-short dataset. \n",
    "            By default, will use all numeric variables in the df.\n",
    "    top_minus_bot: boolean, True to be long the top portfolio, short the bottom portfolio.\n",
    "                   False to be long the bottom portfolio, short the top portfolio.\n",
    "    \"\"\"\n",
    "    long, short = _select_long_short_ports(df, portvar, top_minus_bot=top_minus_bot)\n",
    "    return _portfolio_difference(df, portvar, long, short, byvars=byvars, retvars=retvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pandas Utilities\n",
    "\n",
    "These are functions which support other functions but should not be available to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pdutils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pdutils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import timeit\n",
    "import time, datetime\n",
    "import re\n",
    "\n",
    "from .ext_time import estimate_time\n",
    "from .core import OrderedSet\n",
    "\n",
    "def _to_list_if_str(var):\n",
    "    if isinstance(var, str):\n",
    "        return [var]\n",
    "    else:\n",
    "        return var\n",
    "\n",
    "def _to_series_if_str(df, i):\n",
    "    if isinstance(i, pd.Series):\n",
    "        s = i\n",
    "    elif isinstance(i, str):\n",
    "        s = df[i]\n",
    "    else:\n",
    "        raise ValueError('Please provide a str, list of strs, or a list of pd.Series for byvars')\n",
    "    return s\n",
    "\n",
    "def _to_name_if_series(i):\n",
    "    if isinstance(i, pd.Series):\n",
    "        return i.name\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "\n",
    "def create_windows(periods, time, method='between'):\n",
    "\n",
    "    if method.lower() == 'first':\n",
    "        windows = [[0]]\n",
    "        windows += [[i for i in range(1, len(periods))]]\n",
    "        return windows\n",
    "    elif method.lower() == 'between':\n",
    "        time = [t - time[0] for t in time] #shifts time so that first period is period 0\n",
    "        windows = [[0]]\n",
    "        t_bot = 0\n",
    "        for i, t in enumerate(time): #pick each element of time\n",
    "            if t == 0: continue #already added zero\n",
    "            windows.append([i for i in range(t_bot + 1, t + 1)])\n",
    "            t_bot = t\n",
    "        #The last window is all the leftover periods after finishing time\n",
    "        extra_windows = [[i for i, per in enumerate(periods) if i not in chain.from_iterable(windows)]]\n",
    "        if extra_windows != [[]]: #don't want to add empty window\n",
    "            windows += extra_windows\n",
    "        return windows\n",
    "    \n",
    "def window_mapping(time, col, method='between'):\n",
    "    \"\"\"\n",
    "    Takes a pandas series of dates as inputs, calculates windows, and returns a series of which\n",
    "    windows each observation are in. To be used with groupby.transform()\n",
    "    \"\"\"\n",
    "    windows = create_windows(col, time, method=method)\n",
    "    return [n for i in range(len(col.index)) for n, window in enumerate(windows) if i in window]\n",
    "\n",
    "\n",
    "def year_month_from_single_date(date):\n",
    "    d = OrderedDict()\n",
    "    d.update({'Year': date.year})\n",
    "    d.update({'Month': date.month})\n",
    "    return pd.Series(d)\n",
    "\n",
    "\n",
    "############Portfolio Utilities###############\n",
    "def _check_portfolio_inputs(*args, **kwargs):\n",
    "    assert isinstance(args[0], pd.DataFrame)\n",
    "    assert isinstance(args[1], str)\n",
    "    assert isinstance(kwargs['ngroups'], int)\n",
    "    \n",
    "def _assert_byvars_list(byvars):\n",
    "    if byvars != None:\n",
    "        if isinstance(byvars, str): byvars = [byvars]\n",
    "        else:\n",
    "            assert isinstance(byvars, list)\n",
    "    return byvars\n",
    "\n",
    "def _sort_into_ports(df, cutoffs, portvar, groupvar):\n",
    "        df[portvar] = 0\n",
    "        for i, (low_cut, high_cut) in enumerate(zip(cutoffs[:-1],cutoffs[1:])):\n",
    "                rows = df[(df[groupvar] >= low_cut) & (df[groupvar] <= high_cut)].index\n",
    "                df.loc[rows, portvar] = i + 1\n",
    "        return df\n",
    "    \n",
    "def _create_cutoffs(cutdf, groupvar, percentiles):\n",
    "    return [np.nanpercentile(cutdf[groupvar], i) for i in percentiles]\n",
    "\n",
    "def _create_cutoffs_and_sort_into_ports(df, cutdf, groupvar, portvar, percentiles):\n",
    "    cutoffs = _create_cutoffs(cutdf, groupvar, percentiles)\n",
    "    return _sort_into_ports(df, cutoffs, portvar, groupvar)\n",
    "\n",
    "def _split(df, keepvars, force_numeric=False):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into a list of arrays based on a key variable. Pass keepvars\n",
    "    to keep variables other than the key variable\n",
    "    \"\"\"\n",
    "#     df = df.sort_values('__key_var__') #now done outside of function\n",
    "    small_df = df[['__key_var__'] + keepvars]\n",
    "    arr = small_df.values\n",
    "    splits = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        if i == 0: continue\n",
    "        if arr[i,0] != arr[i-1,0]: #different key\n",
    "            splits.append(i)\n",
    "    outarr = arr[:,1:]\n",
    "    if force_numeric:\n",
    "        outarr = outarr.astype('float64')\n",
    "    return np.split(outarr, splits)\n",
    "\n",
    "\n",
    "def _create_cutoffs_arr(arr, percentiles):\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return False\n",
    "    return [np.percentile(arr, i) for i in percentiles]\n",
    "\n",
    "\n",
    "def _sort_arr_into_ports(arr, cutoffs):\n",
    "    port_cutoffs = _gen_port_cutoffs(cutoffs)\n",
    "    portfolio_match = partial(_portfolio_match, port_cutoffs=port_cutoffs)\n",
    "    return [portfolio_match(elem) for elem in arr]\n",
    "\n",
    "\n",
    "def _portfolio_match(elem, port_cutoffs):\n",
    "    if np.isnan(elem): return 0\n",
    "    return [index + 1 for index, bot, top in port_cutoffs \\\n",
    "            if elem >= bot and elem <= top][0]\n",
    "    \n",
    "def _gen_port_cutoffs(cutoffs):\n",
    "    return [(i, low_cut, high_cut) \\\n",
    "                    for i, (low_cut, high_cut) \\\n",
    "                    in enumerate(zip(cutoffs[:-1],cutoffs[1:]))]\n",
    "\n",
    "\n",
    "def _sort_arr_list_into_ports(array_list, cut_array_list, percentiles, multiprocess):\n",
    "    if multiprocess:\n",
    "        if isinstance(multiprocess, int):\n",
    "            return _sort_arr_list_into_ports_mp(array_list, cut_array_list, percentiles, mp=multiprocess)\n",
    "        else:\n",
    "            return _sort_arr_list_into_ports_mp(array_list, cut_array_list, percentiles)\n",
    "    else:\n",
    "        return _sort_arr_list_into_ports_sp(array_list, cut_array_list, percentiles)\n",
    "\n",
    "\n",
    "def _create_cutoffs_arr_and_sort_into_ports(data_tup, percentiles):\n",
    "    arr, cutarr = data_tup\n",
    "    cutoffs = _create_cutoffs_arr(cutarr, percentiles)\n",
    "    if cutoffs:\n",
    "        return _sort_arr_into_ports(arr, cutoffs)\n",
    "    else:\n",
    "        return [0 for elem in arr]\n",
    "    \n",
    "def _sort_arr_list_into_ports_sp(array_list, cut_array_list, percentiles):\n",
    "    outlist = []\n",
    "    for i, arr in enumerate(array_list):\n",
    "        result = _create_cutoffs_arr_and_sort_into_ports((arr, cut_array_list[i]),\n",
    "                                                        percentiles)\n",
    "        outlist.append(result)\n",
    "    return outlist\n",
    "\n",
    "def _sort_arr_list_into_ports_mp(array_list, cut_array_list, percentiles, mp=None):\n",
    "    if mp:\n",
    "        with Pool(mp) as pool: #use mp # processors\n",
    "            return _sort_arr_list_into_ports_mp_main(array_list, cut_array_list, percentiles, pool)\n",
    "    else:\n",
    "        with Pool() as pool: #use all processors\n",
    "            return _sort_arr_list_into_ports_mp_main(array_list, cut_array_list, percentiles, pool)\n",
    "\n",
    "    \n",
    "def _sort_arr_list_into_ports_mp_main(array_list, cut_array_list, percentiles, pool):\n",
    "        #For time estimation\n",
    "        counter = []\n",
    "        num_loops = len(array_list)\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        #Mp setup\n",
    "        port = partial(_create_cutoffs_arr_and_sort_into_ports,\n",
    "                                 percentiles=percentiles)\n",
    "        \n",
    "        data_tups = [(arr, cut_array_list[i]) for i, arr in enumerate(array_list)]\n",
    "\n",
    "        results = [pool.apply_async(port, ((arr, cut_array_list[i]),), callback=counter.append) \\\n",
    "                        for i, arr in enumerate(array_list)]\n",
    "        \n",
    "        #Time estimation\n",
    "        while len(counter) < num_loops:\n",
    "            estimate_time(num_loops, len(counter), start_time)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        #Collect and output results. A timeout of 1 should be fine because\n",
    "        #it should wait until completion anyway\n",
    "        return [r.get(timeout=1) for r in results]\n",
    "\n",
    "\n",
    "def _arr_list_to_series(array_list):\n",
    "    return pd.Series(np.concatenate(array_list, axis=0))\n",
    "\n",
    "def _sort_arr_list_into_ports_and_return_series(array_list, cut_array_list, percentiles, multiprocess):\n",
    "    al = _sort_arr_list_into_ports(array_list, cut_array_list, percentiles, multiprocess)\n",
    "    return _arr_list_to_series(al)\n",
    "\n",
    "#############End portfolio utilities###########################################################################\n",
    "\n",
    "def _expand(monthly_date, datevar, td, newdatevar):\n",
    "   \n",
    "    t = time.gmtime(monthly_date/1000000000) #date coming in as integer, need to parse\n",
    "    t = datetime.date(t.tm_year, t.tm_mon, t.tm_mday) #better output than gmtime\n",
    "    \n",
    "    beginning = datetime.date(t.year, t.month, 1) #beginning of month of date\n",
    "    end = beginning + relativedelta(months=1, days=-1) #last day of month\n",
    "    days = pd.date_range(start=beginning, end=end, freq=td) #trade days within month\n",
    "    days.name = newdatevar\n",
    "    result =  np.array([(t, i) for i in days])\n",
    "    return result\n",
    "\n",
    "\n",
    "def _extract_table_names_from_sql(query):\n",
    "    \"\"\" Extract table names from an SQL query. \"\"\"\n",
    "    # a good old fashioned regex. turns out this worked better than actually parsing the code\n",
    "    tables_blocks = re.findall(r'(?:FROM|JOIN)\\s+(\\w+(?:\\s*,\\s*\\w+)*)', query, re.IGNORECASE)\n",
    "    tables = [tbl\n",
    "              for block in tables_blocks\n",
    "              for tbl in re.findall(r'\\w+', block)]\n",
    "    return OrderedSet(tables)\n",
    "\n",
    "def _get_datetime_cols(df):\n",
    "    \"\"\"\n",
    "    Returns a list of column names of df for which the dtype starts with datetime\n",
    "    \"\"\"\n",
    "    dtypes = df.dtypes\n",
    "    return dtypes.loc[dtypes.apply(lambda x: str(x).startswith('datetime'))].index.tolist()\n",
    "\n",
    "def _select_long_short_ports(df, portvar, top_minus_bot=True):\n",
    "    \"\"\"\n",
    "    Finds the appropriate portfolio number and returns (long number, short number)\n",
    "    \"\"\"\n",
    "    #Get numbered value of highest and lowest portfolio\n",
    "    top = max(df[portvar])\n",
    "    bot = min(df[portvar])\n",
    "    \n",
    "    if top_minus_bot:\n",
    "        return top, bot\n",
    "    else:\n",
    "        return bot, top\n",
    "    \n",
    "def _portfolio_difference(df, portvar, long, short, byvars=None, retvars=None):\n",
    "    \"\"\"\n",
    "    Calculates long portfolio minus short portfolio\n",
    "    \"\"\"\n",
    "    if byvars:\n",
    "        out = df[df[portvar] == long].set_index(byvars) - df[df[portvar] == short].set_index(byvars)\n",
    "    else:\n",
    "        out = df[df[portvar] == long] - df[df[portvar] == short]\n",
    "        \n",
    "    if retvars:\n",
    "        return out[retvars]\n",
    "    else:\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reg.py\n",
    "\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "def reg(df, yvar, xvars, robust=True, cluster=False):\n",
    "    \"\"\"\n",
    "    Returns a fitted regression. Takes df, produces a regression df with no missing among needed\n",
    "    variables, and fits a regression model. If robust is specified, uses heteroskedasticity-\n",
    "    robust standard errors. If cluster is specified, calculated clustered standard errors\n",
    "    by the given variable. \n",
    "    \n",
    "    Note: only specify at most one of robust and cluster.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing regression data\n",
    "    yvar: str, column name of outcome y variable\n",
    "    xvars: list of strs, column names of x variables for regression\n",
    "    \n",
    "    Optional inputs:\n",
    "    robust: bool, set to True to use heterskedasticity-robust standard errors\n",
    "    cluster: False or str, set to a column name to calculate standard errors within clusters\n",
    "             given by unique values of given column name\n",
    "    \"\"\"\n",
    "    drop_set = [yvar] + xvars\n",
    "    if cluster:\n",
    "        drop_set += [cluster]\n",
    "    \n",
    "    regdf = df.dropna(subset=drop_set)\n",
    "    y = regdf[yvar]\n",
    "    X = regdf.loc[:, xvars]\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    mod = sm.OLS(y, X)\n",
    "    \n",
    "    assert not (robust and cluster) #need to pick one of robust or cluster\n",
    "    \n",
    "    if robust:\n",
    "        return mod.fit(cov_type='HC1')\n",
    "    \n",
    "    if cluster:\n",
    "        groups = regdf[cluster].unique().tolist()\n",
    "        group_ints = regdf[cluster].apply(lambda x: groups.index(x))\n",
    "        return mod.fit(cov_type='cluster', cov_kwds={'groups': group_ints})\n",
    "    \n",
    "    return mod.fit()\n",
    "\n",
    "\n",
    "def reg_for_each_combo(df, yvar, xvars, **reg_kwargs):\n",
    "    \"\"\"\n",
    "    Takes each possible combination of xvars (starting from each var individually, then each pair\n",
    "    of vars, etc. all the way up to all xvars), and regresses yvar on each set of xvars. Returns\n",
    "    a list of fitted regressions.\n",
    "    \"\"\"\n",
    "    reg_list = []\n",
    "    for i in range(1, len(xvars) + 1):\n",
    "        for combo in itertools.combinations(xvars, i):\n",
    "            x = list(combo)\n",
    "            reg_list.append(reg(df, yvar, x, **reg_kwargs))\n",
    "            \n",
    "    return reg_list\n",
    "\n",
    "def reg_for_each_xvar_set(df, yvar, xvars_list, **reg_kwargs):\n",
    "    \"\"\"\n",
    "    Runs regressions on the same y variable for each set of x variables passed. xvars_list \n",
    "    should be a list of lists, where each individual list is one set of x variables for one model.\n",
    "    Returns a list of fitted regressions.\n",
    "    \"\"\"\n",
    "    return [reg(df, yvar, x, **reg_kwargs) for x in xvars_list]\n",
    "\n",
    "def select_models(reg_list, keepnum, xvars):\n",
    "    \"\"\"\n",
    "    Takes a list of fitted regression models and selects among them based on adjusted R-Squared. For each\n",
    "    number of variables involved in the regressions, keepnum with the highest R-squareds will be kept.\n",
    "    \n",
    "    For example, if reg_list contains 3 regressions with two variables and 6 regressions with three variables,\n",
    "    and keepnum is 2, will return a list of four regressions, 2 with two variables and 2 with three variables.\n",
    "    \"\"\"\n",
    "    outlist = []\n",
    "    for i in range(1, len(xvars) + 1):\n",
    "        reg_list_match = [reg for reg in reg_list if reg.df_model == i] #select models with this many variables\n",
    "        try:\n",
    "            r2_min = sorted([reg.rsquared_adj for reg in reg_list_match])[-keepnum] #gets keepnumth highest r2\n",
    "        except IndexError: #should happen once there are less models run than keepnum (i.e. with all xvars)\n",
    "            r2_min = sorted([reg.rsquared_adj for reg in reg_list_match])[0] #gets lowest r2 (keep all)\n",
    "        outlist += [reg for reg in reg_list_match if reg.rsquared_adj >= r2_min]\n",
    "    return outlist\n",
    "\n",
    "def produce_summary(reg_list, stderr=False, float_format='%0.1f'):\n",
    "\n",
    "    summ =  summary_col(reg_list, stars=True, float_format=float_format,\n",
    "            info_dict={'N':lambda x: \"{0:d}\".format(int(x.nobs)),\n",
    "                      'R2':lambda x: \"{:.2f}\".format(x.rsquared),\n",
    "                      'Adj-R2':lambda x: \"{:.2f}\".format(x.rsquared_adj)})\n",
    "    \n",
    "    if not stderr:\n",
    "        summ.tables[0].drop('', axis=0, inplace=True) #drops the rows containing standard errors\n",
    "        \n",
    "    return summ\n",
    "\n",
    "def reg_for_each_xvar_set_and_produce_summary(df, yvar, xvars_list, robust=True, \n",
    "                                              cluster=False, stderr=False, float_format='%0.1f'):\n",
    "    \"\"\"\n",
    "    Convenience function to run regressions for every set of xvars passed\n",
    "    and present them in a summary format. Returns a tuple of (reg_list, summary) where reg_list\n",
    "    is a list of fitted regression models, and summary is a single dataframe of results.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing regression data\n",
    "    yvar: str, column name of y variable\n",
    "    xvars_list: list of lists of strs, each individual list has column names of x variables for that model\n",
    "    \n",
    "    Optional inputs:\n",
    "    robust: bool, set to True to use heterskedasticity-robust standard errors\n",
    "    cluster: False or str, set to a column name to calculate standard errors within clusters\n",
    "             given by unique values of given column name\n",
    "    stderr: bool, set to True to keep rows for standard errors below coefficient estimates\n",
    "    \n",
    "    Note: only specify at most one of robust and cluster.\n",
    "    \"\"\"\n",
    "    reg_list = reg_for_each_xvar_set(df, yvar, xvars_list, robust=robust, cluster=cluster)\n",
    "    summ = produce_summary(reg_list, stderr=stderr, float_format=float_format)\n",
    "    return reg_list, summ\n",
    "\n",
    "def reg_for_each_combo_select_and_produce_summary(df, yvar, xvars, robust=True, cluster=False,\n",
    "                                                  keepnum=5, stderr=False, float_format='%0.1f'):\n",
    "    \"\"\"\n",
    "    Convenience function to run regressions for every combination of xvars, select the best models,\n",
    "    and present them in a summary format. Returns a tuple of (reg_list, summary) where reg_list\n",
    "    is a list of fitted regression models, and summary is a single dataframe of results\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing regression data\n",
    "    yvar: str, column name of y variable\n",
    "    xvars: list of strs, column names of all possible x variables\n",
    "    \n",
    "    Optional inputs:\n",
    "    robust: bool, set to True to use heterskedasticity-robust standard errors\n",
    "    cluster: False or str, set to a column name to calculate standard errors within clusters\n",
    "             given by unique values of given column name\n",
    "    keepnum: int, number to keep for each amount of x variables. The total number of outputted\n",
    "             regressions will be roughly keepnum * len(xvars)\n",
    "    stderr: bool, set to True to keep rows for standard errors below coefficient estimates\n",
    "    \n",
    "    Note: only specify at most one of robust and cluster.\n",
    "    \n",
    "    \"\"\"\n",
    "    reg_list = reg_for_each_combo(df, yvar, xvars, robust=robust, cluster=cluster)\n",
    "    outlist = select_models(reg_list, keepnum, xvars)\n",
    "    summ = produce_summary(outlist, stderr=stderr, float_format=float_format) \n",
    "    return outlist, summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summ.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summ.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def summary_stats(df, pct_vars=None, int_vars=None, float_vars=None, count=False):\n",
    "    \"\"\"\n",
    "    Generates a transposed df.describe() table where pct_vars are formatted with two\n",
    "    decimal percentages, int_vars are formatted with zero decimal places, and float_\n",
    "    vars are formatted with two decimal places.\n",
    "    \n",
    "    \"\"\"\n",
    "    stats = ['mean', 'std', 'min', '25%','50%','75%','max']\n",
    "    all_vars = _if_lists_exist_then_combine(pct_vars, int_vars, float_vars)\n",
    "    summ = df[all_vars].replace([np.inf, -np.inf], np.nan).dropna().describe().T\n",
    "    apply_formatting(summ, stats, pct_vars=pct_vars, int_vars=int_vars, float_vars=float_vars)\n",
    "    \n",
    "#     formats = _if_vars_list_exists_then_add_to_format_dict(pct_vars, int_vars, float_vars)\n",
    "    \n",
    "#     _apply_formatting_from_format_dict(summ, formats, stats)\n",
    "\n",
    "    if count:\n",
    "        #Format count column individually\n",
    "        summ['count'] = summ['count'].apply(lambda x: '{0:.0f}'.format(x))\n",
    "    else:\n",
    "        summ.drop('count', axis=1, inplace=True)\n",
    "        \n",
    "    return summ\n",
    "\n",
    "def summ_vars_single_stat_by_groups(df, groupvar, pct_vars=None, int_vars=None, float_vars=None, stat='mean'):\n",
    "    \"\"\"\n",
    "    Generates a table where groups are columns and rows are variables, where the values\n",
    "    are a single summary stat\n",
    "    \"\"\"\n",
    "    all_vars = _if_lists_exist_then_combine(pct_vars, float_vars, int_vars)\n",
    "\n",
    "    group = df.groupby(groupvar)[all_vars]\n",
    "    func = getattr(group, stat) #gets .mean(), .median(), etc.\n",
    "    summ = func().T #applies .mean(), .median(), etc.\n",
    "    apply_formatting(summ, summ.columns, pct_vars=pct_vars,\n",
    "                               float_vars=float_vars, int_vars=int_vars)\n",
    "    return summ\n",
    "\n",
    "def apply_formatting(df, cols, pct_vars=None, int_vars=None, float_vars=None):\n",
    "    \"\"\"\n",
    "    Applies percentage, integer, and float formatting to a summary table. Variables must\n",
    "    be in rows, with statistics in columns.\n",
    "    \n",
    "    Note: inplace\n",
    "    \"\"\"\n",
    "    formats = _if_vars_list_exists_then_add_to_format_dict(pct_vars, int_vars, float_vars)\n",
    "    \n",
    "    _apply_formatting_from_format_dict(df, formats, cols)\n",
    "\n",
    "def _if_lists_exist_then_combine(*args):\n",
    "    out = []\n",
    "    for a in args:\n",
    "        if a:\n",
    "            out += a\n",
    "    return out\n",
    "\n",
    "def _if_vars_list_exists_then_add_to_format_dict(pct_vars, int_vars, float_vars):\n",
    "    formats = {\n",
    "        0: '{:.2%}',\n",
    "        1: '{0:.0f}',\n",
    "        2: '{0:.2f}'\n",
    "    }\n",
    "    for i, var_list in enumerate([pct_vars, int_vars, float_vars]):\n",
    "        if var_list:\n",
    "            formats[tuple(var_list)] = formats.pop(i) #replaces int key with tuple of vars\n",
    "    return formats\n",
    "\n",
    "def _apply_formatting_from_format_dict(df, formats, cols):\n",
    "    \"\"\"\n",
    "    Note: inplace\n",
    "    \"\"\"\n",
    "    for k in formats: #apply appropriate format to stats columns but not count column\n",
    "        if not isinstance(k, int): #skip empty lists - these are marked by an integer\n",
    "            df.loc[list(k), cols] = df.loc[list(k), cols].applymap(\n",
    "                lambda x: formats[k].format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# WRDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrds.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrds.py\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import functools\n",
    "\n",
    "# class WRDS:\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         import wrds\n",
    "#         self.wrds_obj = wrds\n",
    "        \n",
    "#     def get(self, *args, **kwargs):\n",
    "#         \"\"\"\n",
    "#         See docstring of dero.wrds.get\n",
    "#         \"\"\"\n",
    "#         return get(*args, wrds_obj=self.wrds_obj, **kwargs)\n",
    "    \n",
    "#     def sql(self, *args, **kwargs):\n",
    "#         \"\"\"\n",
    "#         See docstring of dero.wrds.sql\n",
    "#         \"\"\"\n",
    "#         return sql(*args, wrds_obj=self.wrds_obj, **kwargs)\n",
    "\n",
    "class Capturing(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        sys.stdout = self._stdout\n",
    "\n",
    "\n",
    "def login_if_needed(func):\n",
    "    \"\"\"\n",
    "    This decorator is to be used after importing wrds. Will log in again\n",
    "    if connection is timed out.\n",
    "    \n",
    "    Usage:\n",
    "    \n",
    "    import wrds\n",
    "    \n",
    "    @login_if_needed\n",
    "    def get_msi():\n",
    "        return wrds.sql('select * from CRSP.MSI')\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def func_or_login_and_func(*args,**kwargs):\n",
    "        import wrds\n",
    "        kwargs.update({'wrds_obj':wrds})\n",
    "        with Capturing() as output:\n",
    "            result = func(*args, **kwargs)\n",
    "        if any(['Connection reset by peer' in s for s in output]): #connection error\n",
    "            wrds.CONN = wrds._authenticate(wrds.username, wrds.password)\n",
    "            return func(*args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    return func_or_login_and_func\n",
    "\n",
    "def get(libname, tablename, getvars=False, where=False, subset=False, distinct=False):\n",
    "    \"\"\"\n",
    "    Executes a standard SQL query to get variables from table. Passing an int to subset\n",
    "    causes only that many rows to be returned.\n",
    "    \n",
    "    Required inputs: (note: none of these are case sensitive)\n",
    "    libname: str, name of library in wrds\n",
    "    tablename: str, name of table within library in wrds\n",
    "    \n",
    "    Optional inputs:\n",
    "    getvars: False, str, or list of strs, names of columns to pull from table. If False will\n",
    "             pull all columns.\n",
    "    where: False or str, SQL where expression without the word where.\n",
    "            Examples:\n",
    "                'permno = 10516 and askhi > 1000'\n",
    "                'date = \"04jan2013\"d'\n",
    "                'date between \"07jan2013\"d and \"08jan2013\"d'\n",
    "    subset: False or int, set to an int to keep that many observations\n",
    "    distinct: bool, set to true to select distinct\n",
    "    \"\"\"\n",
    "#     if not wrds_obj:\n",
    "#         import wrds\n",
    "#         wrds_obj = wrds\n",
    "    \n",
    "    if isinstance(getvars, str):\n",
    "        getvars = [getvars]\n",
    "    \n",
    "    select = 'select ' + ('distinct ' if distinct else '')\n",
    "    \n",
    "    if getvars:\n",
    "        var_string =  ', '.join([g.lower() for g in getvars])\n",
    "    else:\n",
    "        var_string = '*'\n",
    "        \n",
    "    table_string = '.'.join([n.strip().upper() for n in [libname, tablename]])\n",
    "    query = select + var_string + ' from ' + table_string\n",
    "    \n",
    "    if subset:\n",
    "        query += ' (obs={})'.format(subset)\n",
    "        \n",
    "    if where:\n",
    "        query += ' where ' + where\n",
    "    \n",
    "    return sql(query)\n",
    "\n",
    "@login_if_needed\n",
    "def sql(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Replicates wrds.sql with auto-login if necessary. Pass query string and index. Documentation\n",
    "    from wrds.sql below:\n",
    "    \n",
    "    Run a SQL Query on the WRDS Database.\n",
    "    data = wrds.sql('select * from CRSP.MSI', 'DATE')\n",
    "\n",
    "    The second argument gives the column name of the index, if you'd like\n",
    "    your DataFrame to be indexed.\n",
    "    \"\"\"\n",
    "#     if not wrds_obj:\n",
    "#         import wrds\n",
    "#         wrds_obj = wrds\n",
    "\n",
    "    wrds_obj = kwargs.pop('wrds_obj')\n",
    "    \n",
    "    return wrds_obj.sql(*args, **kwargs)\n",
    "\n",
    "def strip_str(x):\n",
    "     return x.strip() if isinstance(x, str) else x\n",
    "\n",
    "\n",
    "def tolist_and_strip_outlist(func):\n",
    "    \"\"\"\n",
    "    This decorator works for functions that return a pandas Series. It converts the series to\n",
    "    a list and strips white space from the output.\n",
    "    \n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def tolist_and_strip(*args,**kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        return [r.strip() for r in result.tolist()]\n",
    "\n",
    "    return tolist_and_strip\n",
    "\n",
    "@tolist_and_strip_outlist\n",
    "def all_libraries():\n",
    "    \"\"\"\n",
    "    Returns every library in WRDS\n",
    "    \"\"\"\n",
    "    return sql('select distinct libname from dictionary.tables')['libname']\n",
    "\n",
    "\n",
    "@tolist_and_strip_outlist\n",
    "def all_tables_in_library(libname):\n",
    "    \"\"\"\n",
    "    Returns every table in a library in WRDS\n",
    "    \"\"\"\n",
    "    return sql('select distinct memname from dictionary.columns ' \n",
    "                    'where libname=\"{}\"'.format(libname.upper()))['memname']\n",
    "\n",
    "@tolist_and_strip_outlist\n",
    "def _all_columns_in_table(libname, tablename):\n",
    "    \"\"\"\n",
    "    Returns every column in a table in WRDS\n",
    "    \"\"\"\n",
    "    return sql('select name from dictionary.columns ' \n",
    "                    'where libname=\"{}\" and memname=\"{}\"'.format(\n",
    "                        libname.upper(), tablename.upper()))['name']\n",
    "\n",
    "\n",
    "@tolist_and_strip_outlist\n",
    "def _all_labels_in_table(libname, tablename):\n",
    "    \"\"\"\n",
    "    Returns every column in a table in WRDS\n",
    "    \"\"\"\n",
    "    return sql('select label from dictionary.columns ' \n",
    "                    'where libname=\"{}\" and memname=\"{}\"'.format(\n",
    "                        libname.upper(), tablename.upper()))['label']\n",
    "\n",
    "def all_columns_in_table(libname, tablename):\n",
    "    \"\"\"\n",
    "    Args are libname, tablename strings. Returns a dataframe of column names and labels.\n",
    "    \"\"\"\n",
    "    return sql('select name, label from dictionary.columns ' \n",
    "                    'where libname=\"{}\" and memname=\"{}\"'.format(\n",
    "                        libname.upper(), tablename.upper())).applymap(\n",
    "                            strip_str)\n",
    "    \n",
    "def all_tables():\n",
    "    \"\"\"\n",
    "    Returns every table in WRDS\n",
    "    \"\"\"\n",
    "    df = sql('select libname, memname, memlabel, nvar from dictionary.tables '\n",
    "                  'where not(missing(memlabel))').rename(\n",
    "                        columns={'nvar':'Number of Columns'})\n",
    "    return df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_selenium.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_selenium.py\n",
    "\n",
    "import selenium, zipfile, os\n",
    "\n",
    "def apply_proxy_with_authentication_to_chrome_selenium(proxy, port, username, password, directory):\n",
    "    '''\n",
    "    Applies a proxy server to Chrome driven by Selenium, with authentication included. This works by\n",
    "    creating a zip file extension for Chrome which sets the proxy setting and catches prompt\n",
    "    windows to include username and password info.\n",
    "\n",
    "    proxy: string, IP or address of your proxy, i.e. 'dubai.wonderproxy.com'\n",
    "    port: integer, Port for proxy, i.e. 12000\n",
    "    username: string\n",
    "    password: string\n",
    "    directory: string, This should be the wherever you want the zipfile to be placed, i.e. r'C:/Users/John/Desktop'\n",
    "    '''\n",
    "    chrome_options = selenium.webdriver.ChromeOptions()\n",
    "    #self.chrome_options.add_argument('--proxy-server={}'.format(proxy))\n",
    "\n",
    "    manifest_json = \"\"\"\n",
    "    {\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"manifest_version\": 2,\n",
    "        \"name\": \"Chrome Proxy\",\n",
    "        \"permissions\": [\n",
    "            \"proxy\",\n",
    "            \"tabs\",\n",
    "            \"unlimitedStorage\",\n",
    "            \"storage\",\n",
    "            \"<all_urls>\",\n",
    "            \"webRequest\",\n",
    "            \"webRequestBlocking\"\n",
    "        ],\n",
    "        \"background\": {\n",
    "            \"scripts\": [\"background.js\"]\n",
    "        },\n",
    "        \"minimum_chrome_version\":\"22.0.0\"\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    background_js = '''\n",
    "    var config = {{\n",
    "            mode: \"fixed_servers\",\n",
    "            rules: {{\n",
    "              singleProxy: {{\n",
    "                scheme: \"http\",\n",
    "                host: \"{0}\",\n",
    "                port: parseInt({1})\n",
    "              }},\n",
    "              bypassList: [\"foobar.com\"]\n",
    "            }}\n",
    "          }};\n",
    "\n",
    "    chrome.proxy.settings.set({{value: config, scope: \"regular\"}}, function() {{}});\n",
    "\n",
    "    function callbackFn(details) {{\n",
    "        return {{\n",
    "            authCredentials: {{\n",
    "                username: \"{2}\",\n",
    "                password: \"{3}\"\n",
    "            }}\n",
    "        }};\n",
    "    }}\n",
    "\n",
    "    chrome.webRequest.onAuthRequired.addListener(\n",
    "                callbackFn,\n",
    "                {{urls: [\"<all_urls>\"]}},\n",
    "                ['blocking']\n",
    "    );\n",
    "\n",
    "    '''.format(proxy,port,username,password)\n",
    "\n",
    "    plugin_file = os.path.join(directory,'proxy_auth_plugin.zip')\n",
    "\n",
    "    with zipfile.ZipFile(plugin_file, 'w') as zp:\n",
    "        zp.writestr(\"manifest.json\", manifest_json)\n",
    "        zp.writestr(\"background.js\", background_js)\n",
    "\n",
    "    chrome_options.add_extension(plugin_file)\n",
    "\n",
    "    return chrome_options\n",
    "\n",
    "class AnyEC:\n",
    "    \"\"\" Use with WebDriverWait to combine expected_conditions\n",
    "        in an OR.\n",
    "        \n",
    "    Then call it like...\n",
    "\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    # ...\n",
    "    WebDriverWait(driver, 10).until( AnyEC(\n",
    "        EC.presence_of_element_located(\n",
    "             (By.CSS_SELECTOR, \"div.some_result\")),\n",
    "        EC.presence_of_element_located(\n",
    "             (By.CSS_SELECTOR, \"div.no_result\")) ))\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.ecs = args\n",
    "    def __call__(self, driver):\n",
    "        for fn in self.ecs:\n",
    "            try:\n",
    "                if fn(driver): return True\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_sympy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_sympy.py\n",
    "\n",
    "import datetime, os, sys\n",
    "import multiprocessing as mp\n",
    "from sympy import solve\n",
    "\n",
    "class EquationSolver:\n",
    "    \n",
    "    def __init__(self, log_dir=None, debug=False):\n",
    "        self.log_dir = log_dir\n",
    "        self.debug = debug\n",
    "        \n",
    "        if self.log_dir:\n",
    "            self.log_list = []\n",
    "        \n",
    "    def create_log_file(self):\n",
    "        process = mp.current_process()._identity\n",
    "        process_num = str(process[0])\n",
    "        #name = 'log_' + str(datetime.datetime.now().replace(microsecond=0)).replace(':','.') + '_' + process_num + '.txt'\n",
    "        name = 'log_' + process_num + '.txt'\n",
    "        #name = 'log_' + str(datetime.datetime.now().replace(microsecond=0)).replace(':','.') + '.txt'\n",
    "        if not os.path.exists(self.log_dir): os.makedirs(self.log_dir)\n",
    "        self.log_path = os.path.join(self.log_dir, name)\n",
    "                    \n",
    "        if not os.path.exists(self.log_path):\n",
    "            with open(self.log_path, 'w') as f:\n",
    "                f.write('\\n')\n",
    "    \n",
    "    def log(self, message, error=False, neverprint=False):       \n",
    "        self.create_log_file()\n",
    "        \n",
    "        if error:\n",
    "            message = 'ERROR: ' + message\n",
    "        if message != '\\n':\n",
    "            time = datetime.datetime.now().replace(microsecond=0)\n",
    "            message = str(time) + ': ' + message\n",
    "        if self.debug and not neverprint:\n",
    "            sys.stdout.write(message + '\\n')\n",
    "            sys.stdout.flush() #forces output now\n",
    "        try:\n",
    "            with open(self.log_path, 'a') as f:\n",
    "                [f.write(item) for item in self.log_list] #log anything saved in memory that couldn't be written before\n",
    "                f.write(message)\n",
    "                f.write('\\n')\n",
    "            self.log_list = []\n",
    "        except PermissionError: #if someone happened to write to the file at the same time\n",
    "            self.log_list.append(message) #save it to log later\n",
    "            self.log_list.append('\\n')\n",
    "            \n",
    "\n",
    "    def solve_equations(self, selected_tuple, equations=None, params=None):\n",
    "        \"\"\"\n",
    "        Solves a subset of a set of equations. For use with parallel_loop_with_timeout. Combining the two gets\n",
    "        all possible solutions for an overidentified system using parallel processing. Doing this without the\n",
    "        timeout can cause sympy to get stuck on some sets of equations.\n",
    "\n",
    "        Arguments:\n",
    "        selected_tuple: tuple, indices of the equations to select. Typically created by itertools.combinations.\n",
    "        equations: full list of equations to solve. The subset of equations will be pulled from this list.\n",
    "        params: parameters to solve the system in terms of. \n",
    "\n",
    "        Typical usage:\n",
    "        import sympy\n",
    "        sympy.init_session()\n",
    "        eqs = []\n",
    "        eqs.append(Eq(3*x + y, 2))\n",
    "        eqs.append(Eq(2*x + y, 5))\n",
    "        eqs.append(Eq(4*x + 2*y, 6))\n",
    "        params = [x, y]\n",
    "        \n",
    "        log_dir ='C:\\\\Users\\\\derobertisna.UFAD\\\\Dropbox\\\\UF\\\\Nimal\\\\V PIN\\\\Modeling\\\\Testing\\\\Equations'\n",
    "        \n",
    "        combinations = list(itertools.combinations(range(len(equations)), len(params)))\n",
    "        solve = dero.EquationSolver(log_dir=log_dir)\n",
    "        solutions = dero.parallel_loop_with_timeout(solve.solve_equations, combinations, timeout=120,\n",
    "                                                         equations=eqs, params=params)\n",
    "        print([sol for sol in solutions if sol[1] not in ([],'timeout')])\n",
    "\n",
    "        \"\"\"\n",
    "        assert equations != None\n",
    "        assert params != None\n",
    "\n",
    "        solve_eqs = [equations[i] for i in selected_tuple]\n",
    "\n",
    "        if self.log_dir: #if we are logging\n",
    "            self.log('Solving equation set {}:'.format(selected_tuple))\n",
    "            [self.log(str(eq)) for eq in solve_eqs]\n",
    "\n",
    "        result = solve(solve_eqs, params)\n",
    "\n",
    "        if self.log_dir: #if we are logging\n",
    "            self.log('Result for set {}:'.format(selected_tuple))\n",
    "            self.log(str(result))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_multiprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_multiprocessing.py\n",
    "\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import time, timeit, sys\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "from .ext_time import estimate_time\n",
    "\n",
    "def abortable_worker(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    For use with parallel_loop_with_timeout. \n",
    "    \"\"\"\n",
    "    timeout = kwargs.get('timeout', None)\n",
    "    del kwargs['timeout']\n",
    "    p = ThreadPool(1)\n",
    "    res = p.apply_async(func, args=args, kwds=kwargs)\n",
    "    try:\n",
    "        out = res.get(timeout)  # Wait timeout seconds for func to complete.\n",
    "        return out\n",
    "    except multiprocessing.TimeoutError:\n",
    "        print(\"Aborting due to timeout\")\n",
    "        p.terminate()\n",
    "        return 'timeout'\n",
    "\n",
    "def parallel_loop_with_timeout(target, iterlist, timeout=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Parallelizes a loop while imposing a timeout on any individual iteration of the loop. Returns a list\n",
    "    of tuples where the first elements are the index of iterlist and the second elements are the results.\n",
    "    If any iteration times out, will still add the tuple to the list but the second element will be \n",
    "    'timeout'. \n",
    "    \n",
    "    IMPORTANT NOTE:\n",
    "    The target function MUST BE IMPORTED. This will not work if you define the target function in your\n",
    "    current namespace. This is a multiprocessing restriction. The function can be imported from the same\n",
    "    module as this paralellizing function.\n",
    "    \n",
    "    Arguments:\n",
    "    target: function, must be imported. Must accept one regular argument, for which the function will\n",
    "            be evaluated for each element of iterlist. Any additional arguments which do not change with\n",
    "            each iteration must be passed as keyword arguments.\n",
    "    iterlist: list, arguments to pass to target function. Will call the target function one time for\n",
    "              each element of the list.\n",
    "    timeout: int, maximum time to wait for each function call.\n",
    "    **kwargs: include any keyword arguments to pass to target function \n",
    "    \n",
    "    \"\"\"\n",
    "    counter = []\n",
    "    \n",
    "    pool = multiprocessing.Pool()\n",
    "    abortable_func = partial(abortable_worker, target, timeout=timeout)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    num_loops = len(iterlist)\n",
    "    \n",
    "    \n",
    "    results = [(i,\n",
    "              pool.apply_async(abortable_func, args=(arg,), kwds=kwargs, callback=counter.append))\n",
    "              for i, arg in enumerate(iterlist)]\n",
    "    pool.close()\n",
    "    \n",
    "    try: \n",
    "        while len(counter) < num_loops:\n",
    "            estimate_time(num_loops, len(counter), start_time)\n",
    "            time.sleep(0.5)\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        print('Exiting.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    pool.join()\n",
    "    return [(result[0], result[1].get()) for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile core.py\n",
    "\n",
    "import collections\n",
    "import string, pdb\n",
    "from unidecode import unidecode\n",
    "\n",
    "class OrderedSet(collections.MutableSet):\n",
    "\n",
    "    def __init__(self, iterable=None):\n",
    "        self.end = end = [] \n",
    "        end += [None, end, end]         # sentinel node for doubly linked list\n",
    "        self.map = {}                   # key --> [key, prev, next]\n",
    "        if iterable is not None:\n",
    "            self |= iterable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.map)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.map\n",
    "\n",
    "    def add(self, key):\n",
    "        if key not in self.map:\n",
    "            end = self.end\n",
    "            curr = end[1]\n",
    "            curr[2] = end[1] = self.map[key] = [key, curr, end]\n",
    "\n",
    "    def discard(self, key):\n",
    "        if key in self.map:        \n",
    "            key, prev, next = self.map.pop(key)\n",
    "            prev[2] = next\n",
    "            next[1] = prev\n",
    "\n",
    "    def __iter__(self):\n",
    "        end = self.end\n",
    "        curr = end[2]\n",
    "        while curr is not end:\n",
    "            yield curr[0]\n",
    "            curr = curr[2]\n",
    "\n",
    "    def __reversed__(self):\n",
    "        end = self.end\n",
    "        curr = end[1]\n",
    "        while curr is not end:\n",
    "            yield curr[0]\n",
    "            curr = curr[1]\n",
    "\n",
    "    def pop(self, last=True):\n",
    "        if not self:\n",
    "            raise KeyError('set is empty')\n",
    "        key = self.end[1][0] if last else self.end[2][0]\n",
    "        self.discard(key)\n",
    "        return key\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return '%s()' % (self.__class__.__name__,)\n",
    "        return '%s(%r)' % (self.__class__.__name__, list(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, OrderedSet):\n",
    "            return len(self) == len(other) and list(self) == list(other)\n",
    "        return set(self) == set(other)\n",
    "    \n",
    "def return_lines(inpath, outpath, term_list, num_lines=3, encoding='utf-8', debug=False):\n",
    "    '''\n",
    "    Takes a file containing text as input. Searches through file for terms in term_list. When a term\n",
    "    is found, takes that line as well as num_lines surrounding that line, and adds to output file.\n",
    "\n",
    "    Required options:\n",
    "    inpath:    string, full filepath of input file\n",
    "    outpath:   string, full filepath of output file\n",
    "    term_list: list, terms to search for in input file\n",
    "    num_lines: int, number of lines to return around each result. Passing 0 will return only the line of\n",
    "               the found term. Passing 2 would return two lines above, and two below, as well as the original\n",
    "               line, for a total of five lines.\n",
    "    '''\n",
    "    def prepare_for_search(search_list):\n",
    "        '''\n",
    "        Strips out punctuation, extra spacing, converts special characters, and converts to lower space. Can be used\n",
    "        with a string or list.\n",
    "        '''\n",
    "\n",
    "        if isinstance(search_list,str):\n",
    "            searchword_no_punc = \"\".join(char for char in search_list if char not in string.punctuation)\n",
    "            searchword_no_punc = unidecode(searchword_no_punc)\n",
    "            return searchword_no_punc.lower().strip()\n",
    "        elif isinstance(search_list,list):\n",
    "            out_list = []\n",
    "            for searchword in search_list:\n",
    "                searchword_no_punc = \"\".join(char for char in searchword if char not in string.punctuation)\n",
    "                searchword_no_punc = unidecode(searchword_no_punc)\n",
    "                out_list.append(searchword_no_punc.lower().strip())\n",
    "            return out_list\n",
    "        else:\n",
    "            raise ValueError('must provide string or list of strings')\n",
    "    \n",
    "    def file_to_dict(file_path, encoding=encoding):\n",
    "        '''\n",
    "        Creates a dictionary from a file where the keys are line numbers and values are line strings\n",
    "        '''\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            line_dict = {}\n",
    "            for line_num, line in enumerate(f):\n",
    "                line_dict.update({line_num: line.strip()})\n",
    "        return line_dict\n",
    "    \n",
    "    def add_to_output(line_num, outset, last_line, num_lines=num_lines):\n",
    "        outset.add(line_num)\n",
    "        for i in range(num_lines):\n",
    "            up = line_num + i + 1\n",
    "            down = line_num - i - 1\n",
    "            if up <= last_line:\n",
    "                outset.add(up)\n",
    "            if down >= 0:\n",
    "                outset.add(down)\n",
    "        return outset\n",
    "    \n",
    "    if isinstance(term_list,str): term_list = [term_list]\n",
    "    elif not isinstance(term_list,list): raise ValueError('must provide string or list of strings')\n",
    "    \n",
    "    search_list_str = 'Search list: ' + ', '.join(term_list)\n",
    "    search_list = prepare_for_search(term_list) #strips out punctuation, etc.\n",
    "    split_search_list = [term.split() for term in search_list] #divides multi-word terms into individual words\n",
    "    first_word_set = {term_split[0] for term_split in split_search_list} #gets a set of the first words to search for\n",
    "    infile_dict = file_to_dict(inpath)\n",
    "    \n",
    "    #If there are no lines in the file, exit and return zeroes\n",
    "    if infile_dict == {}:\n",
    "        if debug:\n",
    "            print('{} is blank, cannot extract lines.'.format(inpath))\n",
    "        return [0 for item in term_list]\n",
    "    \n",
    "    last_line = max([line for line in infile_dict])\n",
    "    outset = set() #container for line numbers to be outputted\n",
    "    full_match_count_list = [0] * len(term_list) #this is a counter for how many full matches, will be returned\n",
    "\n",
    "    with open(inpath, 'r', encoding=encoding) as f_in:\n",
    "        word_set = first_word_set\n",
    "        match_count_list = [0] * len(term_list) #we need to count matches individually by term\n",
    "        for line_num in infile_dict: \n",
    "            line = prepare_for_search(infile_dict[line_num]) #strip punctuation, etc. from search line\n",
    "            word_list = line.split()\n",
    "            for word in word_list:\n",
    "                #Add 1 to the element of the match count list corresponding to a matched term word (addition part),\n",
    "                #otherwise set match count back to zero (this is the multiplication part)\n",
    "                match_count_list = [match_count * (term_split[match_count] == word) + (term_split[match_count] == word)\n",
    "                                    for match_count, term_split in zip(match_count_list,split_search_list)]\n",
    "                #If we've matched any of the terms completely\n",
    "                if any([len(term_split) == match_count \n",
    "                        for match_count, term_split in zip(match_count_list, split_search_list)]):\n",
    "                    outset = add_to_output(line_num, outset, last_line)\n",
    "                    #Add to full match list\n",
    "                    full_match_count_list = [full_match_count + (len(term_split) == match_count)\n",
    "                                            for match_count, term_split, full_match_count\n",
    "                                            in zip(match_count_list, split_search_list, full_match_count_list)]\n",
    "                    #Set any fully matched back to zero\n",
    "                    match_count_list = [match_count * (1 - (len(term_split) == match_count)) \n",
    "                                        for match_count, term_split in zip(match_count_list, split_search_list)]\n",
    "                else:\n",
    "                    word_set = {term_split[match_count] \n",
    "                                for match_count, term_split in zip(match_count_list, split_search_list)}\n",
    "    \n",
    "    with open(outpath,'w', encoding=encoding) as f_out:\n",
    "        outlist = sorted(list(outset)) #puts lines in proper order\n",
    "        f_out.write(search_list_str)\n",
    "        f_out.write('\\n')\n",
    "        for pos, line_num in enumerate(outlist):\n",
    "            if pos > 0:\n",
    "                if outlist[pos - 1] != line_num - 1: f_out.write('\\n') #split up sections for each match\n",
    "            len_num = len(str(line_num)) #get length of line number, so that we can line up each line\n",
    "            num_space = max(10 - len_num, 0) #set amount of spacing to include to left of line\n",
    "            f_out.write('{}:'.format(line_num) + ' ' * num_space +  '{}'.format(infile_dict[line_num]))\n",
    "            f_out.write('\\n')\n",
    "            \n",
    "    return full_match_count_list\n",
    "\n",
    "def filter_list(inlist, match=None, nomatch=None, remove=True):\n",
    "    '''\n",
    "    Returns outlist, inlist\n",
    "    Takes inlist and finds items which contain match but do not contain nomatch. If remove=True, removes those\n",
    "    items from inlist.\n",
    "    '''\n",
    "    assert not (match == None and nomatch == None)\n",
    "    if isinstance(match, str):\n",
    "        match = [match]\n",
    "    if isinstance(nomatch, str):\n",
    "        nomatch = [nomatch]\n",
    "    if nomatch and match:\n",
    "        q = [(i, n) for n, i in enumerate(inlist) \n",
    "                             if all(map(lambda x: x in i,match)) and all(map(lambda x: x not in i,nomatch))]\n",
    "    elif nomatch:\n",
    "        q = [(i, n) for n, i in enumerate(inlist) if all(map(lambda x: x not in i,nomatch))]\n",
    "    else: #only match\n",
    "        q = [(i, n) for n, i in enumerate(inlist) if all(map(lambda x: x in i,match))]\n",
    "    if q != []:\n",
    "        outlist, outindex = zip(*q) #zip(*) unzips\n",
    "    else:\n",
    "        outlist, outindex = ([],[])\n",
    "    if remove:\n",
    "            inlist = [item for index, item in enumerate(inlist) if index not in outindex]\n",
    "    return outlist, inlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting latex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile latex.py\n",
    "\n",
    "import os, datetime, filecmp, shutil, math\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "from .data import replace_missing_csv\n",
    "\n",
    "\n",
    "def date_time_move_latex(tablename,filepath, folder_name='Tables'):\n",
    "    r\"\"\"\n",
    "    Takes a LaTeX tex and PDF after the PDF's been created by pdflatex and moves it into a table\n",
    "    folder with the date, checking if the table was just previously created. If it's the same as\n",
    "    before, it just deletes the file.\n",
    "\n",
    "    Required arguments:\n",
    "        tablename: operating system name of the table, without extensions\n",
    "        filepath: full filepath of table, without table name. put r before quotes as follows: r'C:\\Users\\Folder'\n",
    "\n",
    "    \"\"\"\n",
    "    def exit_sequence(tablename, filepath):\n",
    "        os.remove(os.path.join(filepath, str(tablename) + '.aux'))\n",
    "        os.remove(os.path.join(filepath, str(tablename) + '.log'))\n",
    "        return\n",
    "\n",
    "    os.chdir(filepath) #sets working directory to current directory of table\n",
    "    table_pdf = tablename + \".pdf\"\n",
    "    table_tex = tablename + \".tex\"\n",
    "    table_xlsx = tablename + \".xlsx\"\n",
    "    inpath_pdf = os.path.join(filepath,table_pdf)\n",
    "    inpath_tex = os.path.join(filepath,table_tex)\n",
    "    inpath_xlsx = os.path.join(filepath,table_xlsx)\n",
    "\n",
    "    tables_path = os.path.join(filepath, folder_name) #set table directory\n",
    "    if not os.path.exists(tables_path): #create a general table directory if it doesn't exist\n",
    "        os.makedirs(tables_path)\n",
    "\n",
    "    current_date = datetime.datetime.today().timetuple()\n",
    "    current_time = datetime.datetime.today().timetuple()\n",
    "    format_time = [str(current_time[0]),str(current_time[1]),str(current_time[2])]\n",
    "    for i in range(3):\n",
    "        if current_time[i] < 10:\n",
    "            format_time[i] = \"0\" + str(current_time[i])\n",
    "    datetime_str = \"{}-{}-{}_\".format(format_time[0],format_time[1],format_time[2])\n",
    "\n",
    "    count = 0 #set up count variable\n",
    "    while True: #continuous loop\n",
    "        count += 1\n",
    "        str_count = \"Num\" + str(count)\n",
    "        name_str = datetime_str + str_count\n",
    "        folder_path = os.path.join(tables_path,name_str)\n",
    "        outpath_tex = os.path.join(folder_path,table_tex)\n",
    "        outpath_pdf = os.path.join(folder_path,table_pdf)\n",
    "        outpath_xlsx = os.path.join(folder_path,table_xlsx)\n",
    "        if os.path.exists(folder_path): #if the folder already exists\n",
    "            if os.path.exists(outpath_tex): #if there is already a tex file with the same name\n",
    "                if filecmp.cmp(outpath_tex,inpath_tex) == True: #if this is the same exact table\n",
    "                    exit_sequence(tablename,filepath)\n",
    "                    os.remove(inpath_pdf)\n",
    "                    os.remove(inpath_tex)\n",
    "                    if os.path.isfile(inpath_xlsx): #if there is an XLSX file, delete it as well\n",
    "                        os.remove(inpath_xlsx)\n",
    "                    return #stop\n",
    "                else: #if there is a tex file with the same name but it's not the same table\n",
    "                    continue #go to next iteration of loop (change output number)\n",
    "            else:\n",
    "                shutil.move(inpath_pdf,outpath_pdf) #moves file\n",
    "                shutil.move(inpath_tex,outpath_tex) #moves file\n",
    "                if os.path.isfile(inpath_xlsx): #if Excel file exists, move it\n",
    "                    shutil.move(inpath_xlsx,outpath_xlsx)\n",
    "                exit_sequence(tablename,filepath)\n",
    "                return\n",
    "        else: #if the folder doesn't exist\n",
    "            os.mkdir(folder_path) #create the folder\n",
    "            shutil.move(inpath_pdf,outpath_pdf) #moves file\n",
    "            shutil.move(inpath_tex,outpath_tex) #moves file\n",
    "            if os.path.isfile(inpath_xlsx): #if Excel file exists, move it\n",
    "                    print(inpath_xlsx)\n",
    "                    print(outpath_xlsx)\n",
    "                    shutil.move(inpath_xlsx,outpath_xlsx)\n",
    "            exit_sequence(tablename,filepath)\n",
    "            return\n",
    "        \n",
    "def csv_to_raw_latex(infile, csvstring=False, missing_rep=\" - \", formatstr='{:.3f}', skipfix=None):\n",
    "    '''\n",
    "    Takes a CSV text file and converts it to a LaTeX formatted list, with each line of the LaTeX\n",
    "    file as an item in the list.\n",
    "    \n",
    "    Required options:\n",
    "        infile: Full file path of CSV (include r before quotes)\n",
    "    \n",
    "    Optional options:\n",
    "        csvstring: True to pass a CSV string to infile rather than load from file\n",
    "        missing_rep: Representation for missing numbers, default \" - \"\n",
    "        formatstr: Python string for number formatting, for example '{:.3f}' with quotes\n",
    "        skipfix: String or list of strings of fixes to skip, options are ['&','%','_']\n",
    "    '''\n",
    "    latex_list = []\n",
    "    if not csvstring:\n",
    "        f = open(infile,'r')\n",
    "    else:\n",
    "        from io import StringIO\n",
    "        f = StringIO(infile)\n",
    "        \n",
    "    if skipfix:\n",
    "        if isinstance(skipfix, str):\n",
    "            skipfix = [skipfix]\n",
    "        assert isinstance(skipfix, list)\n",
    "    \n",
    "    csv_list = f.readlines()\n",
    "    miss_csv_list = replace_missing_csv(csv_list,missing_rep)\n",
    "    latex_list = []\n",
    "    for i, line in enumerate(miss_csv_list):\n",
    "        line_string = ''\n",
    "        for j, item in enumerate(line):\n",
    "            if j is not 0: #don't put an & before the first item in line\n",
    "                line_string += ' & '\n",
    "            #LaTeX character fixes\n",
    "            if skipfix:\n",
    "                if '&' not in skipfix:\n",
    "                    item = item.replace('&', '\\&')\n",
    "                if '%' not in skipfix:\n",
    "                    item = item.replace('%', '\\%')\n",
    "                if '_' not in skipfix:\n",
    "                    item = item.replace('_', '\\_')\n",
    "            else: #make all replacements\n",
    "                item = item.replace('&','\\&')\n",
    "                item = item.replace('%','\\%')\n",
    "                item = item.replace('_','\\_')\n",
    "            if item.find('.') is not -1: #if we are dealing with a number with decimals\n",
    "                try:\n",
    "                    item = formatstr.format(float(item))\n",
    "                except:\n",
    "                    pass\n",
    "            item = item.replace('\\n','')\n",
    "            line_string += item\n",
    "        line_string += \" \\\\\\ \\n\"\n",
    "        if i is 0: #on the first line, remove quotes from names\n",
    "            line_string = line_string.replace('''\"''','') #strip out quotes\n",
    "        latex_list.append(line_string)\n",
    "        \n",
    "    if not csvstring:\n",
    "        f.close()\n",
    "        \n",
    "    return latex_list\n",
    "\n",
    "def df_to_pdf_and_move(dflist, outfolder, outname='table', tabular_string='', string_format='{:.3f}', \n",
    "                       above_text='', below_text='',\n",
    "                     font_size=12, caption='', parse_dates=False, missing_rep=' - ', panel_names=None, colname_flags=None,\n",
    "                      outmethod='pandas'):\n",
    "    '''\n",
    "    Takes a dataframe or list of dataframes as input and outputs to a LaTeX formatted table with multiple panels,\n",
    "    creates a PDF, and moves the LaTeX file and PDF to a dated folder.\n",
    "    \n",
    "    Required options:\n",
    "        dflist:         Dataframe or list of dataframes.\n",
    "        outfolder:      Output folder for LaTeX file and PDF. Inside of this folder, a folder called Tables will be created,\n",
    "                        inside of which the two files will be put inside another folder with the date.\n",
    "        \n",
    "        \n",
    "    Optional options:\n",
    "        outname:        Name of output table, default is table\n",
    "        tabular_string: Can take any string that would normally used in tabular (i.e. rrr for three columns right aligned \n",
    "                        as well as L{<width>), C{<width>}, and R{<width>} (i.e. L{3cm}) for left, center, and right aligned\n",
    "                        fixed width. Additionally . aligns on the decimal. Default is first column left aligned, rest \n",
    "                        center aligned.\n",
    "        string_format:  String or list of format of numbers in the table. Please see Python number formats. {:.3f} is \n",
    "                        three decimals, the default.\n",
    "        font_size:      Font size, default 12\n",
    "        caption:        Title of table\n",
    "        missing_rep:    Representation for missing numbers, default \" - \"\n",
    "        panel_names:    Python list of names of each panel, to go below column names, e.g. ['Table','Other Table']\n",
    "        colname_flags:  Python list of yes or no flags for whether to display column names for each panel. Default is to\n",
    "                        display column names only for the first panel, as usually the panels have the same columns. \n",
    "                        The default input for a three panel table would be ['y','n','n']\n",
    "        outmethod:      String, 'pandas' or 'csv'. If 'pandas', uses pandas' built in df.to_latex() to build latex. If\n",
    "                        'csv', uses df.to_csv() and then dero.raw_csv_to_latex(). The latter case is useful when the table\n",
    "                        itself contains latex expressions.\n",
    "    \n",
    "    '''   \n",
    "    if isinstance(dflist, pd.DataFrame):\n",
    "        dflist = [dflist]\n",
    "    assert isinstance(dflist, list)\n",
    "    if isinstance(string_format, str):\n",
    "        string_format = [string_format] * len(dflist)\n",
    "    assert isinstance(string_format, list)\n",
    "    \n",
    "    def is_number(s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "\n",
    "    outname_tex = str(outname) + \".tex\"\n",
    "    outname_pdf = str(outname) + \".pdf\"\n",
    "    outpath = os.path.join(outfolder, outname_tex)\n",
    "    latex_string_list = [] #set container for final LaTeX table contents\n",
    "    if (colname_flags is None) or (len(colname_flags) is not len(dflist)): #if the user didn't specify whether to use colnames, or they specified an incorrect number of flags\n",
    "        colname_flags = ['y'] #set first colnames to show\n",
    "        for i in range(len(dflist) - 1):\n",
    "            colname_flags.append('n') #set rest of colnames not to show\n",
    "    panel_order = -1\n",
    "    for i, df in enumerate(dflist): #for each csv in the list\n",
    "        df = dflist[i].applymap(lambda x: string_format[i].format(float(x)) if is_number(x) else x)\n",
    "        df = df.fillna(missing_rep)\n",
    "        if outmethod.lower() == 'pandas':\n",
    "            latex_list = [line for line in df.to_latex().split('\\n') if not line.startswith('\\\\')]\n",
    "        elif outmethod.lower() == 'csv':\n",
    "            latex_list = [line for line in csv_to_raw_latex(df.to_csv(), missing_rep=missing_rep,\n",
    "                                                        csvstring=True, skipfix='_') if not line.startswith('\\\\')]\n",
    "        number_of_columns = 1 + latex_list[0].count(' & ') #number of columns is 1 + number of seperators\n",
    "        if panel_names is not None and panel_names[i]:\n",
    "            panel_order += 1 #In combination with next line, sets panel to A, etc.\n",
    "            panel_letter = chr(panel_order + ord('A')) #sets first panel to A, second to B, and so on\n",
    "            #LaTeX formatting code\n",
    "            latex_list.insert(1,r'\\midrule \\\\[-11pt]')\n",
    "            latex_list.insert(2,'\\n')\n",
    "            latex_list.insert(3,r'\\multicolumn{' + str(number_of_columns) + '}{c}{Panel '+ panel_letter + ': ' + panel_names[i] + '} \\\\\\ \\\\\\[-11pt]')\n",
    "            latex_list.insert(4,'\\n')\n",
    "            latex_list.insert(5,r'\\midrule')\n",
    "            latex_list.insert(6,'\\n')\n",
    "        else: #if there is no panel name, just put in a midrule\n",
    "            latex_list.insert(1,r'\\midrule')\n",
    "            latex_list.insert(2,'\\n')\n",
    "        if colname_flags[i].lower() in ('n','no'): #if the flag for colnames is no for this panel\n",
    "            latex_list = latex_list[1:] #chop off colnames\n",
    "        latex_string = \"\\n\".join(latex_list) #convert list to string\n",
    "        latex_string_list.append(latex_string) #add this csv's LaTeX table string to the full list of LaTeX table strings\n",
    "\n",
    "\n",
    "    if tabular_string == \"\": #set default tabular format\n",
    "        tabular_string = 'l' + 'c' * (number_of_columns - 1) #first column left aligned, rest centered\n",
    "    \n",
    "    #Set list of lines to be written to output file at beginning\n",
    "    latex_header_list = [r'\\documentclass[' + str(font_size) + 'pt]{article}',r'\\usepackage{amsmath}',r'\\usepackage{pdflscape}',r'\\usepackage[margin=0.3in]{geometry}',\n",
    "                         r'\\usepackage{dcolumn}',r'\\usepackage{booktabs}',r'\\usepackage{array}', r'\\usepackage{threeparttable}',\n",
    "                         r'\\newcolumntype{L}[1]{>{\\raggedright\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}',\n",
    "                         r'\\newcolumntype{C}[1]{>{\\centering\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}',\n",
    "                         r'\\newcolumntype{R}[1]{>{\\raggedleft\\let\\newline\\\\\\arraybackslash\\hspace{0pt}}m{#1}}',\n",
    "                         r'\\newcolumntype{.}{D{.}{.}{-1}}',r'\\title{\\LaTeX}',r'\\date{}',r'\\author{Nick DeRobertis}',\n",
    "                         r'\\begin{document}',r'\\begin{table}',r'\\centering',r'\\begin{threeparttable}',\n",
    "                         above_text,r'\\caption{' + caption + '}',r'\\begin{tabular}{' + tabular_string + '}',\n",
    "                         r'\\toprule']\n",
    "\n",
    "    #Set list of lines to be written to output file at end\n",
    "    latex_footer_list = [r'\\bottomrule',r'\\end{tabular}',r'\\begin{tablenotes}[para,flushleft]',r'\\item ' + below_text,r'\\end{tablenotes}',r'\\end{threeparttable}',r'\\end{table}',r'\\end{document}']\n",
    "\n",
    "    #Actually write to file\n",
    "    with open(outpath,'w') as f:\n",
    "        for line in latex_header_list: #write each line in the header list, with carriage returns in between\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "        for latex_string in latex_string_list: #write each csv table to file in LaTeX format\n",
    "            f.write(latex_string)\n",
    "        for line in latex_footer_list: #write each line in the footer list, with carriage returns in between\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    os.chdir(outfolder) #changes working filepath\n",
    "    os.system('pdflatex ' + '\"' + outname_tex + '\"') #create PDF\n",
    "    date_time_move_latex(outname,outfolder) #move table into appropriate date/number folder\n",
    "    \n",
    "def latex_equations_to_pdf(latex_list, directory, name='Equations', below_text=None,\n",
    "                           math_size=18, text_size=14, title=None, para_space='1em',\n",
    "                          inline=False):\n",
    "    script_size = math.ceil(math_size * (2/3))\n",
    "    scriptscript_size = math.ceil(math_size * .5)\n",
    "    assert text_size in (8, 9, 10, 11, 12, 14, 17, 20) #latex allowed font sizes\n",
    "    \n",
    "    if inline:\n",
    "        surround_char_beg = '$'\n",
    "        surround_char_end = '$'\n",
    "    else:\n",
    "        surround_char_beg = r'\\begin{dmath}'\n",
    "        surround_char_end = r'\\end{dmath}'\n",
    "\n",
    "    \n",
    "    headers = [r'\\documentclass[{} pt]{{extarticle}}'.format(text_size), \n",
    "               #First size is text size, second is math size, third is script size,\n",
    "               #fourth is scriptscript size\n",
    "               r'\\DeclareMathSizes{{{0}}}{{{1}}}{{{2}}}{{{3}}}'.format(\n",
    "                    text_size, math_size, script_size, scriptscript_size),\n",
    "               r'\\usepackage{amsmath}',\n",
    "               r'\\usepackage{breqn}',\n",
    "               r'\\usepackage[margin=0.3in]{geometry}',\n",
    "              r'\\author{Nick DeRobertis}' ,r'\\begin{document}', r'\\setlength{{\\parskip}}{{{}}}'.format(para_space)]\n",
    "    footers = [r'\\end{document}']\n",
    "    name_tex = name + '.tex'\n",
    "    file_path = os.path.join(directory, name_tex)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('\\n'.join(headers) + '\\n')\n",
    "        [f.write(surround_char_beg + '{}'.format(line) + surround_char_end + '\\n\\n') for line in latex_list]\n",
    "        if below_text:\n",
    "            f.write('\\n' + below_text + '\\n')\n",
    "        f.write('\\n'.join(footers))\n",
    "        \n",
    "    os.chdir(directory)\n",
    "    os.system('pdflatex ' + '\"' + name_tex + '\"') #create pdf\n",
    "    date_time_move_latex(name, directory, 'Equations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_math.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_math.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def transition_matrix(states):\n",
    "    \"\"\"\n",
    "    Creates a numpy array containing the probability of transition from one state to another. Must \n",
    "    pass states as a list of lists, where each inner list is an observation of state changes.\n",
    "    \n",
    "    For example:\n",
    "    states = [\n",
    "    [2,1,3,1,2,3,1],\n",
    "    [1,2,2,2]\n",
    "    ]\n",
    "    \n",
    "    Produces:\n",
    "    array([[ 0.        ,  0.66666667,  0.33333333],\n",
    "           [ 0.25      ,  0.5       ,  0.25      ],\n",
    "           [ 1.        ,  0.        ,  0.        ]])\n",
    "    \"\"\"\n",
    "\n",
    "    max_state = max([max(l) for l in states]) + 1\n",
    "    out = np.zeros((max_state,max_state))\n",
    "    for (x,y), c in Counter(\n",
    "        [transition for state_list in states for transition in \\\n",
    "         zip(state_list, state_list[1:])]).items():\n",
    "        out[x,y] = c\n",
    "        \n",
    "    return out/out.sum(axis=1)[:,None]\n",
    "\n",
    "def _map_to_numbers(string_list):\n",
    "    \"\"\"\n",
    "    Returns a tuple of mapping from numbers to strings, then from strings to numbers\n",
    "    \"\"\"\n",
    "    return {i: item for i, item in enumerate(string_list)}, {item: i for i, item in enumerate(string_list)}\n",
    "    \n",
    "def transition_dataframe(states):\n",
    "    \"\"\"\n",
    "    Creates a pandas dataframe containing the probability of transition from one state to another. Must \n",
    "    pass states as a list of lists, where each inner list is an observation of state changes.\n",
    "    \n",
    "    For example:\n",
    "    states = [\n",
    "        ['b','a','c','a','b','c','a'],\n",
    "        ['a','b','b','b']\n",
    "    ]\n",
    "    \n",
    "    Produces:     a             b            c\n",
    "         a  [ 0.        ,  0.66666667,  0.33333333],\n",
    "         b  [ 0.25      ,  0.5       ,  0.25      ],\n",
    "         c  [ 1.        ,  0.        ,  0.        ]\n",
    "    \"\"\"\n",
    "    unique_states = sorted(set([state for state_list in states for state in state_list]))\n",
    "    #get dicts which map 0, 1, 2, etc. to given states\n",
    "    number_to_string_map, string_to_number_map = _map_to_numbers(unique_states) \n",
    "    #transform strings to mapped numbers\n",
    "    state_numbers = [[string_to_number_map[state] for state in state_list] for state_list in states]\n",
    "    out = pd.DataFrame(transition_matrix(state_numbers))\n",
    "    return out.rename(columns=number_to_string_map, index=number_to_string_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting decorators.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile decorators.py\n",
    "\n",
    "import functools, sys\n",
    "from time import sleep\n",
    "import types\n",
    "\n",
    "def apply_decorator_to_all_functions_in_module(module, decorator):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    import module\n",
    "    import dero\n",
    "    \n",
    "    def my_decorator():\n",
    "        ...\n",
    "    \n",
    "    dero.decorators.apply_decorator_to_all_functions_in_module(module, my_decorator)\n",
    "    \n",
    "    module.whatever_function() #decorator applied\n",
    "    \n",
    "    \"\"\"\n",
    "    for k,v in vars(module).items():\n",
    "        if isinstance(v, types.FunctionType):\n",
    "            setattr(module, k, decorator(v))\n",
    "\n",
    "def simple_decorator(decorator):\n",
    "    '''This decorator can be used to turn simple functions\n",
    "    into well-behaved decorators, so long as the decorators\n",
    "    are fairly simple. If a decorator expects a function and\n",
    "    returns a function (no descriptors), and if it doesn't\n",
    "    modify function attributes or docstring, then it is\n",
    "    eligible to use this. Simply apply @simple_decorator to\n",
    "    your decorator and it will automatically preserve the\n",
    "    docstring and function attributes of functions to which\n",
    "    it is applied.\n",
    "    \n",
    "    Seems to only work for decorators without arguments.'''\n",
    "    def new_decorator(f):\n",
    "        g = decorator(f)\n",
    "        g.__name__ = f.__name__\n",
    "        g.__doc__ = f.__doc__\n",
    "        g.__dict__.update(f.__dict__)\n",
    "        return g\n",
    "    # Now a few lines needed to make simple_decorator itself\n",
    "    # be a well-behaved decorator.\n",
    "    new_decorator.__name__ = decorator.__name__\n",
    "    new_decorator.__doc__ = decorator.__doc__\n",
    "    new_decorator.__dict__.update(decorator.__dict__)\n",
    "    return new_decorator\n",
    "\n",
    "@simple_decorator\n",
    "def dump_args(func):\n",
    "    \"This decorator dumps out the arguments passed to a function before calling it\"\n",
    "    argnames = func.__code__.co_varnames[:func.__code__.co_argcount]\n",
    "    fname = func.__name__\n",
    "\n",
    "    def echo_func(*args,**kwargs):\n",
    "        print(fname, \":\", ', '.join(\n",
    "            '%s=%r' % entry\n",
    "            for entry in list(zip(argnames,args)) + list(kwargs.items())))\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return echo_func\n",
    "\n",
    "def retries(max_tries, delay=1, backoff=2, exceptions=(Exception,), hook=None):\n",
    "    \"\"\"Function decorator implementing retrying logic.\n",
    "\n",
    "    delay: Sleep this many seconds * backoff * try number after failure\n",
    "    backoff: Multiply delay by this factor after each failure\n",
    "    exceptions: A tuple of exception classes; default (Exception,)\n",
    "    hook: A function with the signature myhook(tries_remaining, exception);\n",
    "          default None\n",
    "\n",
    "    The decorator will call the function up to max_tries times if it raises\n",
    "    an exception.\n",
    "\n",
    "    By default it catches instances of the Exception class and subclasses.\n",
    "    This will recover after all but the most fatal errors. You may specify a\n",
    "    custom tuple of exception classes with the 'exceptions' argument; the\n",
    "    function will only be retried if it raises one of the specified\n",
    "    exceptions.\n",
    "\n",
    "    Additionally you may specify a hook function which will be called prior\n",
    "    to retrying with the number of remaining tries and the exception instance;\n",
    "    see given example. This is primarily intended to give the opportunity to\n",
    "    log the failure. Hook is not called after failure if no retries remain.\n",
    "    \"\"\"\n",
    "    def dec(func):\n",
    "        @functools.wraps(func)\n",
    "        def f2(*args, **kwargs):\n",
    "            mydelay = delay\n",
    "            tries = list(range(max_tries))\n",
    "            tries.reverse()\n",
    "            for tries_remaining in tries:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    if tries_remaining > 0:\n",
    "                        if hook is not None:\n",
    "                            hook(tries_remaining, e, mydelay)\n",
    "                        sleep(mydelay)\n",
    "                        mydelay = mydelay * backoff\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    break\n",
    "        return f2\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_time.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_time.py\n",
    "\n",
    "import math, time, datetime, timeit\n",
    "\n",
    "def time_elapsed(seconds):\n",
    "    '''\n",
    "    Takes an amount of time in seconds and converts it into how a human would say it. \n",
    "    \n",
    "    Required Options:\n",
    "    seconds: time in number of seconds (raw number, int or float). \n",
    "    \n",
    "    ''' \n",
    "    if seconds > 60: #if this is greater than a minute\n",
    "        if seconds > 60 * 60: #if this is greater than an hour\n",
    "            if seconds > 60 * 60 * 24: #if this is greater than a day\n",
    "                if seconds > 60 * 60 * 24 * 30: #if this is greater than a month (approx.):\n",
    "                    months = math.trunc(seconds / (60 * 60 * 24 *30))\n",
    "                    seconds -= months * (60 * 60 * 24 * 30)\n",
    "                    days = math.trunc(seconds / (60 * 60 * 24))\n",
    "                    seconds -= days * (60 * 60 * 24)\n",
    "                    hours = math.trunc(seconds / (60 * 60))\n",
    "                    seconds -= hours * (60 * 60)\n",
    "                    minutes = math.trunc(seconds / 60)\n",
    "                    seconds -= minutes * 60\n",
    "                    seconds = math.trunc(seconds)\n",
    "                    time_str = \"{} months, {} days, {} hours, {} minutes, {} seconds.\".format(\n",
    "                        months, days, hours, minutes, seconds)\n",
    "                else:\n",
    "                    days = math.trunc(seconds / (60 * 60 * 24))\n",
    "                    seconds -= days * (60 * 60 * 24)\n",
    "                    hours = math.trunc(seconds / (60 * 60))\n",
    "                    seconds -= hours * (60 * 60)\n",
    "                    minutes = math.trunc(seconds / 60)\n",
    "                    seconds -= minutes * 60\n",
    "                    seconds = math.trunc(seconds)\n",
    "                    time_str = \"{} days, {} hours, {} minutes, {} seconds.\".format(days, hours, minutes, seconds)\n",
    "            else: #if this is greater than an hour but less than a day\n",
    "                hours = math.trunc(seconds / (60 * 60))\n",
    "                seconds -= hours * (60 * 60)\n",
    "                minutes = math.trunc(seconds / 60)\n",
    "                seconds -= minutes * 60\n",
    "                seconds = math.trunc(seconds)\n",
    "                time_str = \"{} hours, {} minutes, {} seconds.\".format(hours, minutes, seconds)\n",
    "        else: #if this is greater than a minute but less than an hour\n",
    "            minutes = math.trunc(seconds / 60)\n",
    "            seconds -= minutes * 60\n",
    "            seconds = math.trunc(seconds)\n",
    "            time_str = \"{} minutes, {} seconds.\".format(minutes, seconds)\n",
    "    else: #if this is less than a minute\n",
    "        seconds = math.trunc(seconds)\n",
    "        time_str = \"{} seconds.\".format(seconds)\n",
    "        \n",
    "    return time_str\n",
    "        \n",
    "def estimate_time(length,i,start_time,output=True):\n",
    "    '''\n",
    "    Returns the estimate of when a looping operation will be finished. \n",
    "    \n",
    "    HOW TO USE:\n",
    "    This function goes at the end of the loop to be timed. Outside of this function at the beginning of the\n",
    "    loop, you must start a timer object as follows:\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    So the entire loop will look like this:\n",
    "    \n",
    "    my_start_time = timeit.default_timer()\n",
    "    for i, item in enumerate(my_list):\n",
    "    \n",
    "        #Do loop stuff here\n",
    "        \n",
    "         estimate_time(len(my_list),i,my_start_time)\n",
    "         \n",
    "    REQUIRED OPTIONS:\n",
    "    length:     total number of iterations for the loop\n",
    "    i:          iterator for the loop\n",
    "    start_time: timer object, to be started outside of this function (SEE ABOVE)\n",
    "    \n",
    "    OPTIONAL OPTIONS:\n",
    "    output: specify other than True to suppress printing estimated time. Use this if you want to just store the time\n",
    "            for some other use or custom output. The syntax then is as follows:\n",
    "    \n",
    "    my_start_time = timeit.default_timer()\n",
    "    for i, item in enumerate(my_list):\n",
    "        \n",
    "        #Do loop stuff here\n",
    "        \n",
    "        my_timer = estimate_time(len(my_list),i,my_start_time, output=False)\n",
    "        print(\"I like my output sentence better! Here's the estimate: {}\".format(my_timer))\n",
    "    \n",
    "    '''\n",
    "    avg_time = (timeit.default_timer() - start_time)/(i + 1)\n",
    "    loops_left = length - (i + 1)\n",
    "    est_time_remaining = avg_time * loops_left\n",
    "    est_finish_time = datetime.datetime.now() + datetime.timedelta(0,est_time_remaining)\n",
    "    \n",
    "    if output == True:\n",
    "        print(\"Estimated finish time: {}. Completed {}/{}, ({:.0%})\".format(est_finish_time, i, length, i/length), end=\"\\r\")\n",
    "    \n",
    "    return est_finish_time\n",
    "\n",
    "def increment_dates(start_date,end_date,frequency='a'):\n",
    "    '''\n",
    "    Returns a list of dates inbetween start and end dates. start_date and end_date should be in one of the following\n",
    "    date formats:\n",
    "        mm/dd/yyyy, mm/yyyy, yyyy\n",
    "    Dates should be frequency should be a single letter, 'a' for annual, 'm' for monthly, 'w' for weekly, and 'd' for daily\n",
    "    '''\n",
    "    #Find number of slashes to determine how to parse date\n",
    "    number_of_slashes = []\n",
    "    number_of_slashes.append(start_date.count('/'))\n",
    "    number_of_slashes.append(end_date.count('/'))\n",
    "    date_formats = ['%Y','%Y'] #set container for date formats\n",
    "    \n",
    "    for i, number in enumerate(number_of_slashes):\n",
    "        if number == 0: #no slashes means interpret as year\n",
    "            pass #already set as year in container\n",
    "        if number == 1: #one slash means interpret as month/year\n",
    "            date_formats[i] = '%m/%Y'\n",
    "        if number == 2: #one slash means interpret as month/year\n",
    "            date_formats[i] = '%m/%d/%Y'\n",
    "    \n",
    "    start = datetime.datetime.strptime(start_date, date_formats[0]).date()\n",
    "    end   = datetime.datetime.strptime(end_date,   date_formats[1]).date()\n",
    "    delta = end - start\n",
    "    \n",
    "    number_of_years = end.year - start.year\n",
    "    number_of_months = end.month - start.month\n",
    "    number_of_days = end.day - start.day\n",
    "    \n",
    "    if frequency == 'd': number_of_periods = delta.days + 1 \n",
    "    if frequency == 'w': number_of_periods = math.ceil((delta.days + 1)/7) \n",
    "    if frequency == 'a': number_of_periods = math.ceil((delta.days + 1)/365)\n",
    "    if frequency == 'm': number_of_periods = math.ceil((delta.days + 1)/(365/12))\n",
    "    \n",
    "    outlist = []\n",
    "    for period in range(number_of_periods):\n",
    "        if frequency == 'd': \n",
    "            new_date = start + datetime.timedelta(days=period)\n",
    "            outlist.append(str(new_date.month) + '/' + str(new_date.day) + '/' + str(new_date.year))\n",
    "        if frequency == 'w': \n",
    "            new_date = start + datetime.timedelta(weeks=period)\n",
    "            outlist.append(str(new_date.month) + '/' + str(new_date.day) + '/' + str(new_date.year))\n",
    "        if frequency == 'a': outlist.append(start.year + period)\n",
    "        if frequency == 'm':\n",
    "            new_period = period - 1\n",
    "            current_month = (start.month + new_period)\n",
    "            current_year = start.year\n",
    "            years_passed = math.floor(current_month/12)\n",
    "            current_year += years_passed\n",
    "            current_month -= 12 * years_passed - 1\n",
    "            if current_month > 12:\n",
    "                pass\n",
    "            outlist.append(str(current_month) + '/' + str(current_year))\n",
    "    \n",
    "    return outlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dero_path = os.path.join(orig_path, 'dero')\n",
    "os.chdir(dero_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.py\n",
    "\n",
    "from sas7bdat import SAS7BDAT\n",
    "import pandas as pd\n",
    "import os, datetime, warnings, sys\n",
    "\n",
    "from .ext_pandas import expand_time, cumulate, convert_sas_date_to_pandas_date, reg_by, factor_reg_by, load_sas, \\\n",
    "                    long_to_wide, year_month_from_date, join_col_strings\n",
    "    \n",
    "from .compustat import convert_gvkey, load_compustat, merge_compustat\n",
    "\n",
    "\n",
    "def replace_missing_csv(csv_list, missing_rep):\n",
    "    '''\n",
    "    Replaces missing items in a CSV with a given missing representation string.\n",
    "    '''\n",
    "    full_list = []\n",
    "    for line in csv_list:\n",
    "        line_list = line.split(',')\n",
    "        new_line_list = []\n",
    "        for item in line_list:\n",
    "            if item == '': #if the item is missing\n",
    "                item = missing_rep\n",
    "            new_line_list.append(item)\n",
    "        full_list.append(new_line_list)\n",
    "    return full_list\n",
    "\n",
    "def merge_dsenames(df, on='TICKER', get='PERMNO', date='Date', \n",
    "                   other_byvars=None, \n",
    "                   crsp_dir=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\CRSP'):\n",
    "    '''\n",
    "    Merges with dsenames file on on variable (TICKER, PERMNO, PERMCO, NCUSIP, CUSIP6), to get get variable (same list).\n",
    "    Must have a Date variable in df.\n",
    "    \n",
    "    Default is to match on TICKER and pull PERMNO.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe containing any of (TICKER, PERMNO, PERMCO, NCUSIP, CUSIP6)\n",
    "    \n",
    "    Optional inputs:\n",
    "    on: str, column name to merge on, one of (TICKER, PERMNO, PERMCO, NCUSIP, CUSIP6)\n",
    "    get: str or list, column or columns to get from dsenames, any of (TICKER, PERMNO, PERMCO, NCUSIP, CUSIP6)\n",
    "         that aren't already in on\n",
    "    date: str, column name of date variable\n",
    "    other_byvars: any other variables signifying groups in the data, prevents from collapsing those groups\n",
    "    '''\n",
    "    #Make get a list\n",
    "    if isinstance(get, str):\n",
    "        get = [get]\n",
    "    assert isinstance(get, list)\n",
    "    \n",
    "    #Make other byvars a list\n",
    "    if not other_byvars:\n",
    "        other_byvars = []\n",
    "    if isinstance(other_byvars, str):\n",
    "        other_byvars = [other_byvars]\n",
    "    assert isinstance(other_byvars, list)\n",
    "    \n",
    "    assert on not in get #can't get what we already have\n",
    "    \n",
    "    #Pull from CRSP dsenames file\n",
    "    file = 'dsenames'\n",
    "    fullpath = os.path.join(crsp_dir, file + '.sas7bdat')\n",
    "    names_df = load_sas(fullpath)\n",
    "    \n",
    "    #Convert NCUSIP to CUSIP6\n",
    "    if on == 'CUSIP6' or 'CUSIP6' in get:\n",
    "        names_df['CUSIP6'] = names_df['NCUSIP'].apply(lambda x: x if pd.isnull(x) else x[:6])\n",
    "    \n",
    "    names_df['start'] = convert_sas_date_to_pandas_date(names_df['NAMEDT'])\n",
    "    names_df['end'] = convert_sas_date_to_pandas_date(names_df['NAMEENDT'])\n",
    "    names_df['end'] = names_df['end'].fillna(datetime.date.today())\n",
    "    \n",
    "    \n",
    "    #Now perform merge\n",
    "    merged = df[[on, date] + other_byvars].merge(names_df[['start','end', on] + get], how='left', on=on)\n",
    "    #Drop out observations not in name date range\n",
    "    valid = (merged[date] >= merged['start']) & (merged[date] <= merged['end'])\n",
    "    #However if there is not a match, doing merged[valid] would drop the observation instead of leaving nan\n",
    "    #Therefore, take merged[valid] and merge back again to original\n",
    "    new_merged = df.merge(merged[valid].drop(['start','end'],axis=1), how='left', on=[on, date] + other_byvars)\n",
    "    new_merged = new_merged.reset_index(drop=True)\n",
    "    \n",
    "    if 'PERMNO' in get:\n",
    "        #Dsenames has no record of which permno is the primary link when a firm has multiple share classes.\n",
    "        #To get this information, we must merge ccmxpf_linktable. We want to keep only primary links.\n",
    "        dups = new_merged[[date, on] + other_byvars].duplicated(keep=False) #series of True or False of whether duplicated row\n",
    "        if dups.any(): #this means we got more than one permno for a single period/firm/byvars\n",
    "            duplicated = new_merged[dups].reset_index() #puts index in a column for later use\n",
    "            not_duplicated = new_merged[~dups]\n",
    "            \n",
    "            #Take duplicated, merge to ccmxpf_linktable to get gvkey, and the ones which do not have gvkeys are\n",
    "            #the non-primary links\n",
    "            with_gvkey = get_gvkey_or_permno(duplicated, date) #default is to get gvkey with permno\n",
    "            removed_duplicates = with_gvkey[~pd.isnull(with_gvkey['GVKEY'])].drop('GVKEY', axis=1)\n",
    "            \n",
    "            #Set index back\n",
    "            removed_duplicates.set_index('index', inplace=True)\n",
    "            \n",
    "            #Now append back together and sort\n",
    "            full = not_duplicated.append(removed_duplicates)\n",
    "            new_merged = full.sort_index()\n",
    "    \n",
    "    return new_merged.reset_index(drop=True)\n",
    "\n",
    "def get_gvkey_or_permno(df, datevar, get='GVKEY', other_byvars=None,\n",
    "                        crsp_dir=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\CRSP'):\n",
    "    \"\"\"\n",
    "    Takes a dataframe containing either GVKEY or PERMNO and merges to the CRSP linktable to get the other one.\n",
    "    \"\"\"    \n",
    "    if get == 'GVKEY':\n",
    "        rename_get = 'gvkey'\n",
    "        l_on = 'PERMNO'\n",
    "        r_on = 'lpermno'\n",
    "    elif get == 'PERMNO':\n",
    "        rename_get = 'lpermno'\n",
    "        l_on = 'GVKEY'\n",
    "        r_on = 'gvkey'\n",
    "    else:\n",
    "        raise ValueError('Need get=\"GVKEY\" or \"PERMNO\"')\n",
    "        \n",
    "    #Make other byvars a list\n",
    "    if not other_byvars:\n",
    "        other_byvars = []\n",
    "    if isinstance(other_byvars, str):\n",
    "        other_byvars = [other_byvars]\n",
    "    assert isinstance(other_byvars, list)\n",
    "    \n",
    "    link_name = 'ccmxpf_linktable.sas7bdat'\n",
    "    link_path = os.path.join(crsp_dir, link_name)\n",
    "    \n",
    "    link = load_sas(link_path)\n",
    "    link['linkdt'] = convert_sas_date_to_pandas_date(link['linkdt'])\n",
    "    link['linkenddt'] = convert_sas_date_to_pandas_date(link['linkenddt'])\n",
    "    #If end date is missing, that means link is still active. Make end date today.\n",
    "    link['linkenddt'] = link['linkenddt'].fillna(datetime.date.today())\n",
    "    \n",
    "    #Remove links with no permno so that they don't match to nans in the input df\n",
    "    link.dropna(subset=['lpermno'], inplace=True)\n",
    "    \n",
    "    merged = df.merge(link[['lpermno','gvkey', 'linkdt', 'linkenddt','linkprim']], how='left', left_on=l_on, right_on=r_on)\n",
    "\n",
    "    valid = (merged[datevar] >= merged.linkdt) & \\\n",
    "            (merged[datevar] <= merged.linkenddt) & \\\n",
    "            (merged.linkprim == 'P')\n",
    "        \n",
    "    merged = merged[valid].drop(['linkdt','linkenddt', 'linkprim', r_on], axis=1).drop_duplicates()\n",
    "    merged.rename(columns={rename_get:get}, inplace=True)\n",
    "    \n",
    "    #Now merge back to the original again to ensure that rows are not deleted\n",
    "    new_merged = df.merge(merged[['PERMNO','GVKEY', datevar] + other_byvars],\n",
    "                          how='left', on=[l_on, datevar] + other_byvars)\n",
    "    \n",
    "    return new_merged\n",
    "\n",
    "\n",
    "class GetCRSP:\n",
    "    \n",
    "    def __init__(self, debug=False, crsp_dir=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\CRSP'):\n",
    "        self.debug = debug\n",
    "        self.crsp_dir = crsp_dir\n",
    "        self.loaded_m = False\n",
    "        self.loaded_d = False\n",
    "        self.crsp_dfs = {} #container for both monthly and daily crsp df\n",
    "        \n",
    "    \n",
    "    def pull_crsp(self, df, coid='PERMNO', freq='m', get=['PRC','SHROUT'], date='Date', other_byvars=None,\n",
    "                 time=None, wide=True,\n",
    "                 abret=False, window=None, cumret=False, includefac=False, includecoef=False, \n",
    "                 drop_first=False):\n",
    "        '''\n",
    "        Pulls prices and returns from CRSP. Currently supports the monthly file merging on PERMNO.\n",
    "\n",
    "        WARNING: Will overwrite variables called \"Year\" and \"Month\" if merging the monthly file\n",
    "\n",
    "        coid = string, company identifier (currently supports 'GVKEY', 'TICKER', 'PERMNO', 'PERMCO', 'NCUSIP')\n",
    "        freq = 'm' for monthly, 'd' for daily\n",
    "        get = 'PRC', 'RET', 'SHROUT', 'VOL', 'CFACPR', 'CFACSHR', or a list combining any of these\n",
    "        date = name of datetime column in quotes\n",
    "        other_byvars = other by vars in dataset besides company identifier and date. Used for long_to_wide\n",
    "        time = list of integers or None. If not None, will pull the variables for a time difference of the time\n",
    "               numbers given, and put them in wide format in the output dataset. For instance time=[-12,0,12] with\n",
    "               freq='m' and get='RET' would pull three returns per observation, one twelve months prior, one contemporaneous,\n",
    "               and one twelve months later, naming them RET-12, RET0, and RET12. If freq='d' in the same example, it would be\n",
    "               twelve days prior, etc.\n",
    "        abret = 1, 3, 4, or False. If 1, 3, or 4 is passed and 'RET' is in get, will calculate abnormal returns according to CAPM,\n",
    "                3 or 4 factor model, respectively. \n",
    "        window = Integer or None. Must provide an integer if abret is not False. This is the number of prior periods to use\n",
    "                 for estimating the loadings in factor models. For instance, if freq='m', window=36 would use the prior\n",
    "                 three years of returns to estimate the loadings.\n",
    "        cumret = 'between', 'zero', 'first', or False. time must not be None and RET in get for this option to matter. \n",
    "                 When pulling returns for multiple periods, gives the option to cumulate returns. If False, will just return \n",
    "                 returns for the individual periods. \n",
    "                 If 'zero', will give returns since the original date. \n",
    "                 If 'between', will give returns since the prior requested time period.\n",
    "                 If 'first', will give returns since the first requested time period.\n",
    "                 For example, if our input data was for date 1/5/2006, and in the CRSP table we had:\n",
    "                     permno  date       RET\n",
    "                     10516   1/5/2006   10%\n",
    "                     10516   1/6/2006   20%\n",
    "                     10516   1/7/2006    5%\n",
    "                     10516   1/8/2006   30%\n",
    "                 Then get_crsp(df, time=[1,3], get='RET', cumret=None) would return:\n",
    "                     permno  date       RET1  RET3\n",
    "                     10516   1/5/2006   20%   30%\n",
    "                 Then get_crsp(df, time=[1,3], get='RET', cumret='between') would return:\n",
    "                     permno  date       RET1  RET3\n",
    "                     10516   1/5/2006   20%   36.5%\n",
    "                 Then get_crsp(df, time=[1,3], get='RET', cumret='zero') would return:\n",
    "                     permno  date       RET1  RET3\n",
    "                     10516   1/5/2006   20%   63.8%\n",
    "                 The output for cumret='first' would be the same as for cumret='zero' because the first period is period zero.\n",
    "                 Had time been =[-1, 1, 3], then returns would be calculated from period -1 to period 1, and period -1 to period 3. \n",
    "        includefac = Boolean, True to include factors in output\n",
    "        includecoef = Boolean, True to include factor coefficients in output\n",
    "        wide = True for output data to be wide form, False for long form. Only applies when time is not None.\n",
    "        drop_first = bool, set to True to drop observations for first time. Can only be used when time != None, and\n",
    "                     when cumret != False. This is a\n",
    "                     convenience function for estimating cumulative return windows. For example, if time = [-1, 1], \n",
    "                     then the typical output would include both cum_RET-1 and cum_RET1. All we actually care about is the\n",
    "                     cumulative return over the window, which is equal to cum_RET1. drop_first=True will drop out \n",
    "                     RET-1 and cum_RET-1 from the output.\n",
    "        debug: bool, set to True to restrict CRSP to only PERMNOs 10516 (gvkey=001722) and 10517 (gvkey=001076)\n",
    "\n",
    "        Typical usage:\n",
    "        Calculating a return window with abnormal returns included:\n",
    "            get_crsp(df, get='RET', freq='d', time=[-1, 1], cumret='between', abret=4, window=250, drop_first=True)\n",
    "        '''  \n",
    "        self.df = df.copy()\n",
    "        self.coid = coid\n",
    "        self.freq = freq\n",
    "        self.get = get\n",
    "        self.date = date\n",
    "        self.other_byvars = other_byvars\n",
    "        self.time = time\n",
    "        self.wide = wide\n",
    "        self.abret = abret\n",
    "        self.window = window\n",
    "        self.cumret = cumret\n",
    "        self.includefac = includefac\n",
    "        self.includecoef = includecoef\n",
    "        self.drop_first = drop_first\n",
    "        \n",
    "        self._log('Initializing pull_crsp function')\n",
    "    \n",
    "        self._clear_old_variables()\n",
    "        self._check_inputs()\n",
    "        self._load_crsp()\n",
    "        \n",
    "        if self.time == None and self.abret == False: \n",
    "            return self._merge_crsp(self.df, self.date) #if we're not doing anything special, just merge CRSP\n",
    "        \n",
    "        if self.time:\n",
    "            self._handle_time()\n",
    "        else:\n",
    "            self._handle_no_time()\n",
    "            \n",
    "        self._create_key()\n",
    "        \n",
    "        if self.abret:\n",
    "            self._handle_abret()\n",
    "        else:\n",
    "            self.long_df = self._merge_crsp(self.long_df, date='Shift Date') #time but not abret        \n",
    "        \n",
    "        if self.cumret:\n",
    "            self._handle_cumret()\n",
    "        \n",
    "        self.long_df.drop('key', axis=1, inplace=True)\n",
    "        \n",
    "        if self.wide:\n",
    "            return self._long_to_wide()\n",
    "        else:\n",
    "            return self._output_long()\n",
    "    \n",
    "    def _clear_old_variables(self):\n",
    "        \"\"\"\n",
    "        If pull_crsp is run multiple times, values of variables would still be set\n",
    "        \"\"\"\n",
    "        try: del self.long_df\n",
    "        except AttributeError: #long_df not set yet\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _output_long(self):\n",
    "        if self.debug:\n",
    "            return self.long_df\n",
    "        for var in ['Shift','Shift Date']:\n",
    "            try:\n",
    "                self.long_df.drop(var, axis=1, inplace=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return self.long_df\n",
    "    \n",
    "    def _long_to_wide(self):\n",
    "        self._log('Reshaping long to wide.')\n",
    "        byvars = [self.coid, self.date]\n",
    "        if self.other_byvars: #add other byvars for correct long_to_wide\n",
    "            byvars += self.other_byvars\n",
    "#         if self.freq == 'm':\n",
    "#             self.long_df.drop(['Year','Month'], axis=1, inplace=True)\n",
    "        try:\n",
    "            self.long_df.drop('Shift Date', axis=1, inplace=True)\n",
    "        except ValueError: #this is the case where time was not shifted\n",
    "            #Therefore need to create a colindex which is just 0 for the current time period\n",
    "            self.long_df['Shift'] = 0\n",
    "        \n",
    "        widedf = long_to_wide(self.long_df.reset_index(drop=True),\n",
    "                              groupvars=byvars, values=self.get, colindex='Shift')\n",
    "        #chop off zeros from contemporaneous\n",
    "        return widedf.rename(columns={name: name[:-1] for name in widedf.columns \\\n",
    "                                      if name.endswith('0') and name[:name.find('0')] in self.get}) \n",
    "    \n",
    "    def _handle_cumret(self):\n",
    "        self._log('Cumret detected.')\n",
    "        cumvars = ['RET']\n",
    "        if self.abret: cumvars += ['ABRET']\n",
    "        self.get += ['cum_' + str(c) for c in cumvars] #get will be used in the end for pivot, need to add pivoting variables\n",
    "        with warnings.catch_warnings(): #cumulate will raise a warning if time is supplied when method is not between\n",
    "            warnings.simplefilter('ignore') #suppress that warning\n",
    "            self._log('Cumulating returns with method {} for time {}.'.format(self.cumret, self.time))\n",
    "            byvars = ['PERMNO', self.date]\n",
    "            if self.other_byvars:\n",
    "                byvars += self.other_byvars\n",
    "                \n",
    "            self.long_df = cumulate(self.long_df, cumvars, periodvar='Shift Date', method=self.cumret,\n",
    "                               byvars=byvars, time=self.time, grossify=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ###########TEMP\n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "            \n",
    "            \n",
    "            \n",
    "            ############\n",
    "            \n",
    "        #Now need to remove unneeded periods\n",
    "        keep_time = self.time\n",
    "        if self.drop_first:\n",
    "            keep_time = self.time[1:]\n",
    "        self.long_df = self.long_df[self.long_df['Shift'].isin(keep_time)]\n",
    "            \n",
    "    def _handle_abret(self):\n",
    "        self._log('Abret detected.')\n",
    "        assert isinstance(self.abret, int) and not isinstance(self.abret, bool) #True evaluates as int\n",
    "        assert isinstance(self.window, int)\n",
    "        \n",
    "        facs = ['mktrf']\n",
    "        if self.abret >= 3:\n",
    "            facs += ['hml','smb']\n",
    "        if self.abret == 4:\n",
    "            facs += ['umd']\n",
    "        if self.abret not in (1,3,4):\n",
    "            raise ValueError('Currently only supports 1, 3, and 4-factor models (mktrf, hml, smb, umd)')\n",
    "        \n",
    "        prior_wind = [i for i in range(0,-self.window - 1,-1)] #e.g. if window = 3, prior_wind = [0, -1, -2, -3]\n",
    "        self._log('Creating abret window periods.')\n",
    "        self.long_df = expand_time(self.long_df, datevar=self.newdate, \n",
    "                                   freq=self.freq, time=prior_wind, \n",
    "                                   newdate='Window Date', shiftvar='Wind')\n",
    "        self._log('Merging with CRSP to get abret window data as well as regular data.')       \n",
    "        self.long_df = self._merge_crsp(df=self.long_df, date='Window Date') \n",
    "        self._log('Getting Fama-French factors, then running regressions.')\n",
    "        self.long_df = get_abret(self.long_df, 'key', fulldatevar='Window Date', freq=self.freq,\n",
    "                                 abret_fac=self.abret, includecoef=self.includecoef,\n",
    "                                 includefac=self.includefac)\n",
    "        \n",
    "        self._log('Dropping unneeded observations (abret window dates).')\n",
    "        if self.time:\n",
    "            #keep only observations for shift dates\n",
    "            self.long_df = self.long_df[self.long_df['Window Date'] == self.long_df['Shift Date']].reset_index(drop=True) \n",
    "        else: #didn't get multiple times per period\n",
    "            #keep only original observations\n",
    "            self.long_df = self.long_df[self.long_df['Window Date'] == self.long_df[self.date]].reset_index(drop=True)\n",
    "        self.get += ['ABRET'] #get will be used in the end for pivot, need to add pivoting variables\n",
    "        \n",
    "        drop = ['Wind', 'Window Date'] #variables to be dropped\n",
    "        coefs = ['coef_' + fac for fac in facs]\n",
    "        if self.includefac: self.get += facs + ['rf']\n",
    "        if self.includecoef: self.get += coefs + ['const']\n",
    "        self.long_df.drop(drop, axis=1, inplace=True)\n",
    "        \n",
    "    \n",
    "    def _create_key(self):\n",
    "        if self.other_byvars:\n",
    "            self.byvars += self.other_byvars\n",
    "        self.long_df['key'] = join_col_strings(self.long_df,self.byvars)\n",
    "    \n",
    "    def _handle_time(self):\n",
    "        self._log('Time detected.')\n",
    "        #Next section only runs if time is not None\n",
    "        #Ensure time is of the right type\n",
    "        if isinstance(self.time, int): self.time = [self.time]\n",
    "        assert (isinstance(self.time, list) and isinstance(self.time[0], int))\n",
    "        intermediate_periods = False\n",
    "        if self.cumret:\n",
    "            self._log('Cumret detected, will generate intermediate periods.')\n",
    "            intermediate_periods = True\n",
    "        self._log('Generating periods {} {}'.format(self.time, '+ itermediate' if intermediate_periods else ''))\n",
    "        self.long_df = expand_time(self.df, intermediate_periods=intermediate_periods, \n",
    "                                   datevar=self.date, freq=self.freq, time=self.time)\n",
    "        self._log('Finished generating periods. Generating key.')\n",
    "        self.byvars = ['PERMNO', self.date, 'Shift Date']\n",
    "        self.newdate = 'Shift Date'\n",
    "        \n",
    "    def _handle_no_time(self):\n",
    "        self.long_df = self.df.copy()\n",
    "        self._log('Generating key.')\n",
    "        self.byvars = ['PERMNO', self.date]\n",
    "        self.newdate = self.date\n",
    "    \n",
    "    def _check_inputs(self):\n",
    "        self._log('Checking inputs.')\n",
    "        \n",
    "#         if self.debug:\n",
    "#             self._log('All inputs: ')\n",
    "#             self._log(str(self.__dict__))\n",
    "\n",
    "        #Check get\n",
    "        if isinstance(self.get, list):\n",
    "            self.get = [item.upper() for item in self.get]\n",
    "#             for item in self.get:\n",
    "#                 assert item in ['PRC','RET','SHROUT','VOL','CFACPR','CFACSHR']\n",
    "        elif isinstance(self.get, str):\n",
    "            self.get = self.get.upper()\n",
    "#             assert self.get in ['PRC','RET','SHROUT','VOL','CFACPR','CFACSHR']\n",
    "            self.get = [self.get]\n",
    "        else:\n",
    "            raise ValueError('''Get should be a list or str containing 'PRC','RET','SHROUT','VOL','CFACPR', or 'CFACSHR'.''')\n",
    "\n",
    "        #Check to make sure inputs make sense\n",
    "        assert not ((self.abret == False) and (self.window != None)) #can't specify window without abret\n",
    "        assert not ((self.abret != False) and (self.window == None)) #must specify window with abret\n",
    "        assert not ((self.abret == False) and (self.includefac == True)) #cannot include factors unless calculating abnormal returns\n",
    "        assert not ((self.abret == False) and (self.includecoef == True)) #cannot include factor coefs unless calculating abnormal returns\n",
    "        assert not ((self.abret != False) and ('RET' not in self.get)) #can't calculate abnormal returns without getting returns\n",
    "        assert not (self.cumret and ('RET' not in self.get)) #can't cumulate returns without getting returns\n",
    "        assert not (self.cumret and (self.time == None)) #can't cumulate over a single period\n",
    "        assert not ((self.drop_first == True) and (self.time == None)) #can't drop first shifted time period if there are none\n",
    "        assert not ((self.drop_first == True) and (len(self.time) == 1)) #can't drop first shifted time period if there's only one\n",
    "        assert not ((self.drop_first == True) and (self.cumret == False)) #no reason to drop first shifted time period if we're not cumulating\n",
    "\n",
    "\n",
    "        #Check to make sure company identifier is valid\n",
    "        assert self.coid in ('GVKEY','TICKER', 'PERMNO', 'PERMCO', 'NCUSIP', 'CUSIP6')\n",
    "        if self.coid != 'PERMNO':\n",
    "            self._log('Company ID is not PERMNO. Getting PERMNO.')\n",
    "            if self.coid == 'GVKEY':\n",
    "                self.df = get_gvkey_or_permno(self.df, self.date, get='PERMNO',\n",
    "                                              other_byvars=self.other_byvars) #grabs permno from ccmxpf_linktable\n",
    "                self._log('Pulled PERMNO from ccmxpf_linktable.')\n",
    "            else: #all others go through dsenames\n",
    "                self.df = merge_dsenames(self.df, on=self.coid, date=self.date,\n",
    "                                        other_byvars=self.other_byvars) #grabs permno\n",
    "                self._log('Pulled PERMNO from dsenames.')\n",
    "\n",
    "        #Check to make sure no columns currently in the dataframe have the same name as columns\n",
    "        #we will be adding\n",
    "        for col in self.get:\n",
    "            if col in self.df.columns:\n",
    "                self.df.rename(columns={col: col + '_old'}, inplace=True)\n",
    "\n",
    "        #Ensure other_byvars is a list\n",
    "        if isinstance(self.other_byvars, str):\n",
    "            self.other_byvars = [self.other_byvars]\n",
    "        assert isinstance(self.other_byvars, (list, type(None)))\n",
    "    \n",
    "    def _load_crsp(self):\n",
    "        if self.freq.lower() == 'm':\n",
    "            if not self.loaded_m:\n",
    "                self.__load_crsp()\n",
    "            self.loaded_m = True\n",
    "        if self.freq.lower() == 'd':\n",
    "            if not self.loaded_d:\n",
    "                self.__load_crsp()\n",
    "            self.loaded_d = True\n",
    "        \n",
    "    def __load_crsp(self):\n",
    "        #Check frequency\n",
    "        if self.freq.lower() == 'm':\n",
    "            filename = 'msf'\n",
    "        elif self.freq.lower() == 'd':\n",
    "            filename = 'dsf'\n",
    "        else: raise ValueError('use m or d for frequency')\n",
    "        if self.debug: filename += '_test' #debug datasets only have permnos 10516, 10517\n",
    "\n",
    "        #Load in CRSP file\n",
    "        self._log('Loading CRSP dataframe...')\n",
    "        filepath = os.path.join(self.crsp_dir, filename + '.sas7bdat')\n",
    "        self.crsp_dfs[self.freq.lower()] = load_sas(filepath)\n",
    "        self._log('Loaded.')\n",
    "\n",
    "        #Change date to datetime format\n",
    "        self._log('Converting SAS date to Pandas format.')\n",
    "        self.crsp_dfs[self.freq.lower()]['DATE'] = convert_sas_date_to_pandas_date(\n",
    "                                                    self.crsp_dfs[self.freq.lower()]['DATE'])\n",
    "        self._log('Converted.')\n",
    "    \n",
    "    def _log(self, message):\n",
    "        if message != '\\n':\n",
    "            time = datetime.datetime.now().replace(microsecond=0)\n",
    "            message = str(time) + ': ' + message\n",
    "        sys.stdout.write(message + '\\n')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    def _merge_crsp(self, df, date):\n",
    "        self._log('Merging CRSP to dataframe.')\n",
    "        df = df.copy()\n",
    "\n",
    "        #If we are using the monthly file, we need to merge on month and year\n",
    "        if self.freq.lower() == 'm':\n",
    "            self.crsp_df = self.crsp_dfs['m']\n",
    "            self.crsp_df = year_month_from_date(self.crsp_df, date='DATE')\n",
    "            orig_df_for_merge = year_month_from_date(df, date=date)\n",
    "\n",
    "            #Now perform merge\n",
    "            merged = orig_df_for_merge.merge(self.crsp_df[['Month','Year','PERMNO'] + self.get],\n",
    "                                   how='left', on=['PERMNO','Month','Year'])\n",
    "            merged.drop(['Month','Year'], axis=1, inplace=True)\n",
    "            \n",
    "        if self.freq.lower() == 'd':\n",
    "            self.crsp_df = self.crsp_dfs['d']\n",
    "            merged = df.merge(self.crsp_df[['DATE','PERMNO'] + self.get],\n",
    "                              how='left', right_on=['PERMNO','DATE'],\n",
    "                              left_on=['PERMNO', date])\n",
    "            merged.drop('DATE', axis=1, inplace=True)\n",
    "            \n",
    "        self._log('Completed merge.')\n",
    "\n",
    "        #Temp\n",
    "        return merged\n",
    "\n",
    "    \n",
    "def get_ff_factors(df, fulldatevar=None, year_month=None, freq='m',\n",
    "                   subset=None, ff_dir=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\FF'):\n",
    "    \"\"\"\n",
    "    Pulls Fama-French factors and merges them to dataset\n",
    "    \n",
    "    df: Input dataframe\n",
    "    fulldatevar: String name of date variable to merge on. Specify this OR year and month variable. Must use this\n",
    "                 and not year_month if pulling daily factors. If merging with monthly factors, will create month\n",
    "                 and year variables in the output dataset. Warning: Will overwrite any variables called Month\n",
    "                 and Year in the input data.\n",
    "    year_month: Two element list of ['yearvar','monthvar']. Specify this OR full date variable.\n",
    "    freq: 'm' for monthly factors, 'd' for daily\n",
    "    subset: str or list, names of ff factors to pull. Can specify any of 'mktrf', 'smb', 'hml', 'umd'\n",
    "    ff_dir: folder containing FF data\n",
    "    \"\"\"\n",
    "   \n",
    "    #Make sure inputs are correct\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    assert freq in ('d','m')\n",
    "    assert isinstance(ff_dir, str)\n",
    "    assert not (fulldatevar == None and year_month == None)\n",
    "    assert not (fulldatevar == None and freq == 'd')\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if not subset:\n",
    "        subset = ['mktrf', 'smb', 'hml', 'umd']\n",
    "    if isinstance(subset, str):\n",
    "        subset = [subset]\n",
    "    assert isinstance(subset, list)\n",
    "    for item in subset:\n",
    "        assert item in ['mktrf', 'smb', 'hml', 'umd']\n",
    "        \n",
    "    subset = subset.copy() #don't modify original beyond converting to list\n",
    "    \n",
    "    if freq == 'm': \n",
    "        ff_name = 'ff_fac_month.sas7bdat'\n",
    "        drop = False\n",
    "        if year_month != None:\n",
    "            left_datevars = year_month\n",
    "            df_for_merge = df\n",
    "        else: #fulldatevar specified\n",
    "            df_for_merge = year_month_from_date(df, date=fulldatevar)\n",
    "            left_datevars = ['Year','Month']\n",
    "            drop = True\n",
    "        right_datevars = ['year','month']\n",
    "    else: \n",
    "        drop = False\n",
    "        df_for_merge = df\n",
    "        ff_name = 'ff_fac_daily.sas7bdat'\n",
    "        left_datevars = fulldatevar\n",
    "        right_datevars = ['date']\n",
    "        \n",
    "    subset += right_datevars + ['rf'] #need to pull date variables and risk free rate as well\n",
    "        \n",
    "    path = os.path.join(ff_dir, ff_name)\n",
    "    ffdf = load_sas(path)\n",
    "    ffdf['date'] = convert_sas_date_to_pandas_date(ffdf['date']) #convert to date object\n",
    "    \n",
    "    merged = df_for_merge.merge(ffdf[subset], how='left', left_on=left_datevars, right_on=right_datevars)\n",
    "    merged.drop(right_datevars, axis=1, inplace=True)\n",
    "    \n",
    "    if drop:\n",
    "        merged.drop(left_datevars, axis=1, inplace=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def get_abret(df, byvars, fulldatevar='Date', year_month=None, freq='m', abret_fac=4, retvar='RET',\n",
    "              includecoef=False, includefac=False):\n",
    "    \"\"\"\n",
    "    Takes a dataframe containing a column of returns, dates, and at least one by variable and calculates\n",
    "    abnormal returns for the model of choice. Returns a dataframe with the abnormal returns merged.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: pandas dataframe\n",
    "    byvars: str or list of strs, column names of columns on which to form by groups\n",
    "    fulldatevar: str, name of column containing date variable. If provided, don't provide year_month.\n",
    "    year_month: list of strs, columns names of year and month variables, e.g. ['Year','Month']. Must\n",
    "                set fulldatevar to None if year_month is provided.\n",
    "    freq: 'm' or 'd', 'm' for monthly returns, 'd' for daily returns\n",
    "    abret_fac: int (1, 3, 4), abnormal return model\n",
    "    retvar: str, name of return variable\n",
    "    includecoef: bool, set to True to get factor loadings\n",
    "    includefac: bool, set to True to get factors and risk free rate\n",
    "    \"\"\"\n",
    "    assert abret_fac in (1, 3, 4)\n",
    "    factors = ['mktrf']\n",
    "    if abret_fac >= 3:\n",
    "        factors += ['smb','hml']\n",
    "    if abret_fac == 4:\n",
    "        factors += ['umd']\n",
    "    \n",
    "    out = get_ff_factors(df, fulldatevar=fulldatevar, freq=freq, subset=factors, year_month=year_month)\n",
    "    out = factor_reg_by(out, byvars, fac=abret_fac, retvar=retvar)\n",
    "    \n",
    "    if not includefac:\n",
    "        out.drop(factors + ['rf'], axis=1, inplace=True)\n",
    "    if not includecoef:\n",
    "        out.drop(['const'] + ['coef_' + fac for fac in factors], axis=1, inplace=True)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def load_and_merge_compustat(df, get=['sale'], freq='a', gvkeyvar='gvkey', datevar='Date', debug=False,\n",
    "                             comp_dir=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\Compustat'):\n",
    "    \"\"\"\n",
    "    Convenience function for both loading and merging compustat to existing dataframe.\n",
    "    \n",
    "    Required inputs:\n",
    "    df: Pandas dataframe containing a date variable and gvkey\n",
    "    \n",
    "    Optional inputs:\n",
    "    get: List of strings, variable names to pull from compustat, not including those needed for\n",
    "         the merge. Do not add the 'q' or 'y' for quarterly variables, this is done automatically.\n",
    "         'y' variables will be converted to quarterly values by looking at changes.\n",
    "    freq: string, 'a' or 'q' for annual or quarterly compustat (funda, fundq)\n",
    "    gvkeyvar: string, name of variable containing gvkeys\n",
    "    datevar: string, name of date variable in df on which to merge. Will pull the newest data\n",
    "             that is before or on this date.\n",
    "    debug: bool, True to restrict to only gvkeys (001076, 001722)\n",
    "    comp_dir: string, directory containing compustat files\n",
    "    \"\"\"\n",
    "    convert_gvkey(df, gvkeyvar)\n",
    "    comp = load_compustat(freq, get=get, debug=debug, comp_dir=comp_dir)\n",
    "    return merge_compustat(df, comp, datevar=datevar).rename(columns={'gvkey':gvkeyvar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting compustat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile compustat.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from numpy import nan, float64, issubdtype, number\n",
    "\n",
    "from .ext_pandas import convert_sas_date_to_pandas_date, load_sas, left_merge_latest\n",
    "\n",
    "def compustat_keep_mask(df):\n",
    "    return (df['indfmt'] == 'INDL') & (df['datafmt'] == 'STD') & \\\n",
    "           (df['popsrc'] == 'D')    & (df['consol'] == 'C')\n",
    "    \n",
    "def add_q_or_y(get, freq, cols):\n",
    "    \"\"\"\n",
    "    Takes a list of get vars and adds q or y to work with quarterly file\n",
    "    \"\"\"\n",
    "    if freq in ('q', 'quarterly'):\n",
    "        return q_or_y(get, cols)\n",
    "    else:\n",
    "        return get\n",
    "    \n",
    "def q_or_y(get, cols):\n",
    "    \"\"\"\n",
    "    Checks compustat cols to see which extension ('q', 'y', none) is appropriate and adds it.\n",
    "    For use with quarterly data.\n",
    "    \"\"\"\n",
    "    out_list = []\n",
    "    for g in get:\n",
    "        if g in ('fqtr','cusip','fyr','tic','conm'):\n",
    "            out_list.append(g)\n",
    "            continue\n",
    "        \n",
    "        q = g + 'q'\n",
    "        y = g + 'y'\n",
    "        if q in cols:\n",
    "            name = q\n",
    "        elif y in cols:\n",
    "            name = y\n",
    "        else:\n",
    "            raise ValueError('variable {} does not have a quarterly counterpart'.format(g))\n",
    "        out_list.append(name)\n",
    "    return out_list\n",
    "\n",
    "def create_q_from_y(df, var_y):\n",
    "    \"\"\"\n",
    "    Single variable conversion\n",
    "    Creates a compustat quarterly \"q\" variable from a compustat quarterly \"y\" variable. \"q\" variables\n",
    "    are for just what happened in the quarter, while \"y\" variables are year to date. \n",
    "    \n",
    "    Note: inplace\n",
    "    \"\"\"\n",
    "    var = var_y[:-1] #gets name of variable without y\n",
    "    df[var_y + '_lag'] = df[var_y].shift(1)\n",
    "    df.loc[(df['fqtr'] > 1) & (df['gvkey'] == df['gvkey_lag']), var + 'q'] = \\\n",
    "                df[var_y] - df[var_y + '_lag']\n",
    "    df.loc[df['fqtr'] == 1, var + 'q'] = df[var_y]\n",
    "    \n",
    "#     var = var_y[:-1] #gets name of variable without y\n",
    "#     df[var_y + '_lag'] = df[var_y].shift(1)\n",
    "#     df.loc[df['fqtr'] > 1, var + 'q'] = df[var_y] - df[var_y + '_lag']\n",
    "#     df.loc[df['fqtr'] == 1, var + 'q'] = df[var_y]\n",
    "    \n",
    "def create_qs_from_ys(df, get, freq):\n",
    "    \"\"\"\n",
    "    Dataframe conversion\n",
    "    Creates compustat quarterly \"q\" variablse from a compustat quarterly \"y\" variables. \"q\" variables\n",
    "    are for just what happened in the quarter, while \"y\" variables are year to date. \n",
    "    \n",
    "    Note: will make edits to prior df even though returns a new df\n",
    "    \"\"\"\n",
    "    if freq in ('q', 'quarterly'):\n",
    "        y_get = [g for g in get if g.endswith('y')]\n",
    "        df['gvkey_lag'] = df['gvkey'].shift(1)\n",
    "        [create_q_from_y(df, g) for g in y_get]\n",
    "        return df.drop(y_get + ['fqtr', 'gvkey_lag'] + [c + '_lag' for c in y_get], axis=1)\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "def check_freq(freq):\n",
    "    freq = freq.lower()\n",
    "    assert freq in ('a','q','annual','quarterly')\n",
    "    return freq\n",
    "\n",
    "def freq_to_name(freq, debug):\n",
    "    if freq in ('a','annual'):\n",
    "        name = 'funda'\n",
    "    elif freq in ('q', 'quarterly'):\n",
    "        name = 'fundq'\n",
    "    if debug:\n",
    "        name += '_test'\n",
    "    name += '.sas7bdat'\n",
    "    return name\n",
    "    \n",
    "def keep_relevant_data_compustat(df, get=['sale'], freq='a'):\n",
    "    get = add_q_or_y(get, freq, df.columns) #adds 'q' or 'y' to getvars if freq='q'\n",
    "    mask = compustat_keep_mask(df)\n",
    "    keepvars = ['gvkey','datadate']\n",
    "    if freq in ('q', 'quarterly'):\n",
    "        keepvars += ['fqtr'] #need for converting 'y' variables to 'q' variables\n",
    "    keepvars += get\n",
    "    comp_y = df.loc[mask, keepvars].drop_duplicates(\n",
    "        subset=['gvkey', 'datadate']).reset_index(drop=True)\n",
    "    #comp_y includes 'y' vars, need to convert to 'q' vars \n",
    "    return create_qs_from_ys(comp_y, get, freq)\n",
    "\n",
    "def convert_date_compustat(df):\n",
    "    df['datadate'] = convert_sas_date_to_pandas_date(df['datadate'])\n",
    "    \n",
    "def load_compustat(freq, get=['sale'], debug=False, comp_dir=r'C:\\Users\\derobertisna.UFAD\\Desktop\\Data\\Compustat'):\n",
    "    freq = check_freq(freq)\n",
    "    name = freq_to_name(freq, debug)\n",
    "    path = os.path.join(comp_dir, name)\n",
    "    comp = load_sas(path, dtype={'gvkey':str})\n",
    "    comp = keep_relevant_data_compustat(comp, get=get, freq=freq)\n",
    "    convert_date_compustat(comp)\n",
    "    return comp\n",
    "\n",
    "def merge_compustat(df, compdf, datevar='Date'):\n",
    "    return left_merge_latest(df, compdf, 'gvkey',\n",
    "                            left_datevar=datevar, right_datevar='datadate')\n",
    "\n",
    "# def merge_compustat(df, compdf, datevar='Date'):\n",
    "#     many = df.merge(compdf, on='gvkey', how='left')\n",
    "#     lt = many.loc[many[datevar] >= many['datadate']] #left with datadates less than date\n",
    "\n",
    "#     #find rows within groups which have the maximum datadate (soonest before given date)\n",
    "#     data_rows = lt.groupby(['gvkey',datevar], as_index=False)['datadate'].max() \\\n",
    "#         .merge(lt, on=['gvkey', datevar, 'datadate'], how='left')\n",
    "    \n",
    "#     return df.merge(data_rows, on=['gvkey',datevar], how='left')\n",
    "\n",
    "def convert_numeric_gvkey_to_string(gvkey):\n",
    "    \"\"\"\n",
    "    Converts a single numeric gvkey to string\n",
    "    \"\"\"\n",
    "    str_gvkey = str(int(gvkey))\n",
    "    num_zeroes = 6 - len(str_gvkey)\n",
    "    return '0' * num_zeroes + str_gvkey\n",
    "\n",
    "def convert_gvkey_col(gvkey):\n",
    "    \"\"\"\n",
    "    Converts a column of numeric gvkeys to string\n",
    "    \"\"\"\n",
    "    if pd.isnull(gvkey): return nan\n",
    "    else:\n",
    "        return convert_numeric_gvkey_to_string(gvkey)\n",
    "    \n",
    "def convert_gvkey(df, gvkeyvar):\n",
    "    \"\"\"\n",
    "    Renames gvkeyvar to 'gvkey' and converts to string if necessary\n",
    "    \n",
    "    Note: this is inplace\n",
    "    \"\"\"\n",
    "    if gvkeyvar != 'gvkey':\n",
    "        df.rename(columns={gvkeyvar: 'gvkey'}, inplace=True)\n",
    "    if issubdtype(df['gvkey'].dtype, number):\n",
    "        df['gvkey'] = df['gvkey'].apply(convert_gvkey_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_logging.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_logging.py\n",
    "\n",
    "import datetime, os, sys\n",
    "import logging, functools\n",
    "import inspect\n",
    "import timeit\n",
    "\n",
    "from .ext_time import time_elapsed\n",
    "from .decorators import apply_decorator_to_all_functions_in_module\n",
    "\n",
    "def apply_logging_to_all_functions_in_module(module):\n",
    "    \"\"\"\n",
    "    To be used after creating a logger with dero.logging.create_logger(), and after importing\n",
    "    a module. On subsequent calls to any functions from that module, they will be logged using\n",
    "    the log_with decorator. \n",
    "    \n",
    "    NOTE: Be careful not to use this on any module containing a function to be called many times.\n",
    "    For such modules, it is better to use the log_with decorator directly excluding those functions.\n",
    "    \n",
    "    Usage:\n",
    "    import module\n",
    "    import dero\n",
    "    \n",
    "    logger = dero.logging.create_logger()\n",
    "\n",
    "    dero.logging.apply_logging_to_all_functions_in_module(module)\n",
    "    \n",
    "    module.whatever_function() #logs correctly\n",
    "    \n",
    "    \"\"\"\n",
    "    name = _get_all_prior_frames()\n",
    "    name += '.' + module.__name__\n",
    "    module.logger = logging.getLogger(name)\n",
    "    module.log = log_with(module.logger)\n",
    "    apply_decorator_to_all_functions_in_module(module, module.log)\n",
    "\n",
    "def create_logger(name='main'):\n",
    "    \"\"\"\n",
    "    Creates a logger in the __main__ namespace. Sets three handlers, two to file and one to stdout.\n",
    "    All output goes to the .debug file, info and higher goes to the .log file, and error and higher\n",
    "    goes to stdout.\n",
    "    \n",
    "    Pass a name to name log files.\n",
    "    \n",
    "    Usage:\n",
    "    Imagine a project with three files, main.py, bar.py, and baz.py. We want to use the \n",
    "    create_logger() function in the main namespace (file being run), and get_logger() in\n",
    "    the imported files.\n",
    "    \n",
    "    Normal logs:\n",
    "    Then log entries may be created with logger.debug(), logger.info(), logger.warning(), logger.error(),\n",
    "    and logger.critical(). \n",
    "    \n",
    "    Exceptions:\n",
    "    Log caught exceptions with logger.exception('Custom message'), this will include the traceback\n",
    "    \n",
    "    Entering and exiting functions:\n",
    "    Use @dero.logging.log_with(logger) decorator, logs when entering and exiting function as well as\n",
    "    passed args and kwargs and return values. Logs enter and exit at the info level and parameters and\n",
    "    return values at the debug level.\n",
    "    \n",
    "    Example usage:\n",
    "    main.py:\n",
    "    import dero\n",
    "    \n",
    "    logger = dero.logging.create_logger()\n",
    "\n",
    "    logger.info('Starting main')\n",
    "    bar.barf()\n",
    "    \n",
    "    bar.py:\n",
    "    import dero\n",
    "    import baz\n",
    "    \n",
    "    logger = dero.logging.get_logger()\n",
    "    \n",
    "    def barf():\n",
    "        logger.info('some info about barf')\n",
    "        baz.baz()\n",
    "        \n",
    "    baz.py:\n",
    "    import dero\n",
    "    \n",
    "    logger = dero.logging.get_logger()\n",
    "    \n",
    "    def baz():\n",
    "        logger.info('some info about baz')\n",
    "        \n",
    "    Running main.py will output:\n",
    "    2016-08-08 15:09:17,109 - __main__ - INFO - Starting main\n",
    "    2016-08-08 15:09:17,111 - __main__.bar - INFO - some info about barf\n",
    "    2016-08-08 15:09:17,111 - __main__.bar.baz - INFO - some info about baz\n",
    "\n",
    "    \"\"\"\n",
    "    #Clear Jupyter notebook logger (this is code that only needs to be run in jupyter notebook)\n",
    "    logger = logging.getLogger()\n",
    "    logger.handlers = []\n",
    "\n",
    "    #Create logger\n",
    "    logger = logging.getLogger('__main__')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    handlers = [] #container for handlers\n",
    "    \n",
    "    #Make log dir\n",
    "    if not os.path.exists('Logs'): os.makedirs('Logs')\n",
    "\n",
    "    #Create debug logfile which logs everything\n",
    "    creation_time = str(datetime.datetime.now().replace(microsecond=0)).replace(':','.')\n",
    "    debug_handler = logging.FileHandler(r'Logs\\{} {}.debug'.format(creation_time, name))\n",
    "    debug_handler.setLevel(logging.DEBUG)\n",
    "    handlers.append(debug_handler)\n",
    "\n",
    "    #Create standard logfile which logs process (info and up)\n",
    "    info_handler = logging.FileHandler(r'Logs\\{} {}.log'.format(creation_time, name))\n",
    "    info_handler.setLevel(logging.INFO)\n",
    "    handlers.append(info_handler)\n",
    "\n",
    "    #Now log errors to standard output\n",
    "    error_handler = logging.StreamHandler(sys.stdout)\n",
    "    error_handler.setLevel(logging.ERROR)\n",
    "    handlers.append(error_handler)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)ls - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    for handler in handlers:\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        \n",
    "    return logger\n",
    "\n",
    "def get_logger():\n",
    "    \"\"\"\n",
    "    To be used in an imported file. See create_logger() for usage.\n",
    "    \"\"\"\n",
    "    name = _get_all_prior_frames()\n",
    "    return logging.getLogger(name)\n",
    "\n",
    "def _get_all_prior_frames():\n",
    "    \"\"\"\n",
    "    Gets the calling stack formatted as a string seperated by periods, e.g.:\n",
    "    __main__.bar.baz\n",
    "    \"\"\"\n",
    "    frame = inspect.currentframe()\n",
    "    out = [] #container for output\n",
    "    while True:\n",
    "        frame = frame.f_back\n",
    "        name = _filter_frame(frame)\n",
    "        if frame is not None:\n",
    "            if name is not False: #if False, is a name we don't need to record, should just continue\n",
    "                out = [name] + out\n",
    "                if name == '__main__': #once we get to __main__, we're done (ignore IPython stuff)\n",
    "                    return '.'.join(out)\n",
    "        else: #if frame is none, we're done (no more frames)\n",
    "            return '.'.join(out)\n",
    "\n",
    "def _filter_frame(frame):\n",
    "    \"\"\"\n",
    "    Checks if this frame is something meaningful and takes the appropriate action\n",
    "    \n",
    "    Returns the name if valid name, returns False if invalid name, returns None if frame is None\n",
    "    \"\"\"\n",
    "    try: name = frame.f_globals['__name__']\n",
    "    except AttributeError: #frame is None\n",
    "        return None\n",
    "    if name in ('importlib._bootstrap','importlib._bootstrap_external', __name__):\n",
    "        return False\n",
    "    return name\n",
    "\n",
    "def get_func_signature(func):\n",
    "    code_list = inspect.getsourcelines(func)[0]\n",
    "    code_str = ' '.join([c.strip() for c in code_list])\n",
    "    return code_str[code_str.find('def') + 4:code_str.find(':')]\n",
    "\n",
    "class log_with(object):\n",
    "    '''Logging decorator that allows you to log with a\n",
    "    specific logger.\n",
    "    \n",
    "    By default, logs entering and exiting function as well as arguments passed at the info level.\n",
    "    \n",
    "    Usage:\n",
    "    import logging\n",
    "    import dero\n",
    "    \n",
    "    logging.basicConfig()\n",
    "    log = logging.getLogger('__name__') #can use custom name but using module name comes with benefits\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    @dero.logging.log_with(log)\n",
    "    def test_func(a, b, c=5):\n",
    "        return a + b\n",
    "    '''\n",
    "    # Customize these messages\n",
    "    ENTRY_MESSAGE = 'Entering {}'\n",
    "    args_message = 'Passed Args: \\n{}, Kwargs: {}'\n",
    "    result_message = '{} Result: \\n{}'\n",
    "    time_message = '{} took {}'\n",
    "    EXIT_MESSAGE = 'Exiting {}'\n",
    "\n",
    "    def __init__(self, logger=None, timer=True):\n",
    "        self.logger = logger\n",
    "        self.timer = timer\n",
    "\n",
    "    def __call__(self, func):\n",
    "        '''Returns a wrapper that wraps func.\n",
    "The wrapper will log the entry and exit points of the function\n",
    "with logging.INFO level.\n",
    "'''\n",
    "        # set logger if it was not set earlier\n",
    "        if not self.logger:\n",
    "            logging.basicConfig()\n",
    "            self.logger = logging.getLogger(func.__module__)\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwds):\n",
    "            if self.timer:\n",
    "                start_time = timeit.default_timer()\n",
    "            \n",
    "            \n",
    "            self.logger.info(self.ENTRY_MESSAGE.format(get_func_signature(func)))  # logging level .info(). Set to .debug() if you want to\n",
    "            self.logger.debug(self.args_message.format(args, kwds))\n",
    "            f_result = func(*args, **kwds)\n",
    "            self.logger.debug(self.result_message.format(func.__name__, f_result))\n",
    "            time_elapsed_str = time_elapsed(timeit.default_timer() - start_time)\n",
    "            self.logger.debug(self.time_message.format(func.__name__, time_elapsed_str))\n",
    "            self.logger.info(self.EXIT_MESSAGE.format(func.__name__))   # logging level .info(). Set to .debug() if you want to\n",
    "            return f_result\n",
    "        return wrapper\n",
    "\n",
    "class Logger:\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.log_list = []\n",
    "        self.create_log_file()\n",
    "\n",
    "    def log(self, message, error=False, neverprint=False):\n",
    "        if error:\n",
    "            message = 'ERROR: ' + message\n",
    "        if message != '\\n':\n",
    "            time = datetime.datetime.now().replace(microsecond=0)\n",
    "            message = str(time) + ': ' + message\n",
    "        if self.debug and not neverprint:\n",
    "            sys.stdout.write(message + '\\n')\n",
    "            sys.stdout.flush() #forces output now\n",
    "        try:\n",
    "            with open(self.log_path, 'a') as f:\n",
    "                [f.write(item) for item in self.log_list] #log anything saved in memory that couldn't be written before\n",
    "                f.write(message)\n",
    "                f.write('\\n')\n",
    "            self.log_list = []\n",
    "        except PermissionError: #if someone happened to write to the file at the same time\n",
    "            self.log_list.append(message) #save it to log later\n",
    "            self.log_list.append('\\n')\n",
    "\n",
    "    def create_log_file(self):\n",
    "        name = 'log_' + str(datetime.datetime.now().replace(microsecond=0)).replace(':','.') + '.txt'\n",
    "        if not os.path.exists(self.log_dir): os.makedirs(self.log_dir)\n",
    "        self.log_path = os.path.join(self.log_dir, name)\n",
    "\n",
    "        if not os.path.exists(self.log_path):\n",
    "            with open(self.log_path, 'w') as f:\n",
    "                f.write('\\n')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Logtimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logtimer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logtimer.py\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dero\n",
    "\n",
    "from .ext_pandas import groupby_merge\n",
    "\n",
    "def load_log_and_produce_timing_dfs(filepath):\n",
    "    \"\"\"\n",
    "    Loads a dero log from filepath and returns a tuple of two pandas dataframes, where the\n",
    "    first is a listing of each function in order and the amount of time it took,\n",
    "    and the second is a summary df which sums and averages times by functions (across\n",
    "    multiple calls)\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        log_list = f.readlines()\n",
    "\n",
    "    df = parse_logs_for_timing(log_list)\n",
    "    summ_df = summary_timing_df(df)\n",
    "    \n",
    "    return df, summ_df\n",
    "\n",
    "def compare_log_timing(filepaths, substrings):\n",
    "    r\"\"\"\n",
    "    Loads multiple logs and produces a summary df with how total and average times per functions on the\n",
    "    different runs. Use substrings to identify which sample is which.\n",
    "    \n",
    "    Required inputs:\n",
    "    filepaths: list of strs, locations of logs\n",
    "    substrings: list of strs, must be same length as filepaths list, identifies each sample\n",
    "    \n",
    "    Usage:\n",
    "    filepaths = [\n",
    "    r'C:\\Users\\derobertisna.UFAD\\Dropbox\\UF\\Investor Attention\\Python\\2016-08-08 23.37.24 ia.debug', #20 subset\n",
    "    r'C:\\Users\\derobertisna.UFAD\\Dropbox\\UF\\Investor Attention\\Python\\2016-08-09 00.02.53 ia.debug' #200 subset\n",
    "    ]\n",
    "    substrings = ['20', '200']\n",
    "\n",
    "    dero.logtimer.compare_log_timing(filepaths, substrings)\n",
    "    \n",
    "    \"\"\"\n",
    "    df_tups = [load_log_and_produce_timing_dfs(fp) for fp in filepaths]\n",
    "    summ_dfs = [tup[1] for tup in df_tups]\n",
    "    \n",
    "    for i, summ_df in enumerate(summ_dfs):\n",
    "        suff = substrings[i] #get suffix\n",
    "        summ_df = summ_df.rename(columns={'time_sum':'time_sum' + suff, 'time_mean':'time_mean' + suff})\n",
    "        if i == 0:\n",
    "            alldf = summ_df.copy()\n",
    "            continue\n",
    "        alldf = alldf.merge(summ_df, on='function', how='outer')\n",
    "    \n",
    "    diff_col = 'diff_' + substrings[0] + '_' + substrings[-1]\n",
    "    alldf[diff_col] = alldf['time_sum' + substrings[-1]] - alldf['time_sum' + substrings[0]]\n",
    "    return alldf.sort_values(diff_col, ascending=False)\n",
    "\n",
    "def _parse_log_entry(l):\n",
    "    \"\"\"\n",
    "    Returns an re match object with the contents:\n",
    "    \n",
    "    First group: timestamp\n",
    "    2: module name\n",
    "    3: logging level\n",
    "    4: message\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'(\\d*-\\d*-\\d* \\d*:\\d*:\\d*,\\d*) - ([\\w.]+) - (\\w*) - (.+)')\n",
    "    return pattern.match(l)\n",
    "\n",
    "def _parse_timing_entry(l):\n",
    "    \"\"\"\n",
    "    Returns a tuple of full function name, time elapsed strings \n",
    "    \"\"\"\n",
    "    match = _parse_log_entry(l)\n",
    "    if not match: return None\n",
    "    timing_pattern = re.compile(r'([\\w.]+) took (.+)[.]')\n",
    "    function_time = timing_pattern.match(match.group(4))\n",
    "    if not function_time: return None\n",
    "    full_function_str = match.group(2) + '.' +  function_time.group(1)\n",
    "    return full_function_str, function_time.group(2)\n",
    "\n",
    "def _parse_logs_for_timing(log_list):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of full function name, time elapsed strings\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x: x, [_parse_timing_entry(l) for l in log_list]))\n",
    "\n",
    "def parse_logs_for_timing(log_list):\n",
    "    \"\"\"\n",
    "    Returns a tuple of two pandas dataframes, where the first is\n",
    "    full function name, time elapsed\n",
    "    and the \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(_parse_logs_for_timing(log_list), columns=['function','time'])\n",
    "    df['time'] = pd.to_timedelta(df['time']).apply(lambda x: x.total_seconds())\n",
    "    return df.sort_values('time', ascending=False).reset_index().rename(columns={'index':'orig order'})\n",
    "\n",
    "def summary_timing_df(parsed_df):\n",
    "    df = groupby_merge(parsed_df, 'function', 'sum', subset='time')\n",
    "    df = groupby_merge(df, 'function', 'mean', subset='time')\n",
    "    return df.drop(['time','orig order'], axis=1).drop_duplicates(\n",
    "        ).sort_values('time_sum', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_matplotlib.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_matplotlib.py\n",
    "\n",
    "def setAxLinesBW(ax):\n",
    "    \"\"\"\n",
    "    Take each Line2D in the axes, ax, and convert the line style to be \n",
    "    suitable for black and white viewing.\n",
    "    \"\"\"\n",
    "    MARKERSIZE = 3\n",
    "\n",
    "    COLORMAP = {\n",
    "        'b': {'marker': None, 'dash': (None,None)},\n",
    "        'g': {'marker': None, 'dash': [5,5]},\n",
    "        'r': {'marker': None, 'dash': [5,3,1,3]},\n",
    "        'c': {'marker': None, 'dash': [1,3]},\n",
    "        'm': {'marker': None, 'dash': [5,2,5,2,5,10]},\n",
    "        'y': {'marker': None, 'dash': [5,3,1,2,1,10]},\n",
    "        'k': {'marker': 'o', 'dash': (None,None)} #[1,2,1,10]}\n",
    "        }\n",
    "\n",
    "\n",
    "    lines_to_adjust = ax.get_lines()\n",
    "    try:\n",
    "        lines_to_adjust += ax.get_legend().get_lines()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    for line in lines_to_adjust:\n",
    "        origColor = line.get_color()\n",
    "        line.set_color('black')\n",
    "        line.set_dashes(COLORMAP[origColor]['dash'])\n",
    "        line.set_marker(COLORMAP[origColor]['marker'])\n",
    "        line.set_markersize(MARKERSIZE)\n",
    "\n",
    "def set_fig_bw(fig):\n",
    "    \"\"\"\n",
    "    Take each axes in the figure, and for each line in the axes, make the\n",
    "    line viewable in black and white.\n",
    "    \"\"\"\n",
    "    for ax in fig.get_axes():\n",
    "        setAxLinesBW(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pdf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pdf.py\n",
    "\n",
    "import os\n",
    "from pdfrw import PdfReader, PdfWriter\n",
    "\n",
    "def strip_pages_pdf(indir, infile, outdir=None, outfile=None, numpages=1, keep=False):\n",
    "    '''\n",
    "    Deletes the first pages from a PDF. Omit outfile name to replace. Default is one page.\n",
    "    If option keep is specified, keeps first pages of PDF, dropping rest.\n",
    "    '''\n",
    "    if outfile is None:\n",
    "        outfile = infile\n",
    "        \n",
    "    if outdir is None:\n",
    "        outdir = indir\n",
    "\n",
    "    output = PdfWriter()\n",
    "    inpath = os.path.join(indir,infile)\n",
    "    outpath = os.path.join(outdir,outfile)\n",
    "    \n",
    "    for i, page in enumerate(PdfReader(inpath).pages):\n",
    "        if not keep:\n",
    "            if i > (numpages - 1):\n",
    "                output.addpage(page)\n",
    "        if keep:\n",
    "            if i <= (numpages - 1):\n",
    "                output.addpage(page)\n",
    "\n",
    "    output.write(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Future (not written to file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "class suppress_output(object):\n",
    "    '''\n",
    "    NOTE: Currently not working, even though no errors\n",
    "    \n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "    \n",
    "    Usage:\n",
    "    with suppress_stdout_stderr():\n",
    "        function_to_be_suppressed()\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        import os\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        import os\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        import os\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup for PyPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\derobertisna.UFAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .pypirc\n"
     ]
    }
   ],
   "source": [
    "%%writefile .pypirc\n",
    "\n",
    "[distutils]\n",
    "index-servers=pypi\n",
    "\n",
    "[pypi]\n",
    "repository = https://pypi.python.org/pypi\n",
    "username = whoopnip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run below to upload to PyPi. Use the first file if it's the first upload, and the second for recurring uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.chdir(orig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_time_upload_str = '''\n",
    "python setup.py sdist bdist_wheel\n",
    "twine register dist\\Dero-{0}*\n",
    "twine upload dist\\Dero-{0}*\n",
    "pause\n",
    "'''.format(version)\n",
    "\n",
    "with open('first_time_upload.bat', 'w') as f:\n",
    "    f.write(first_time_upload_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "upload_str = '''\n",
    "python setup.py sdist bdist_wheel\n",
    "twine upload dist\\Dero-{}*\n",
    "pause\n",
    "'''.format(version)\n",
    "with open('upload.bat', 'w') as f:\n",
    "    f.write(upload_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if upload:\n",
    "    os.system('start cmd /C upload.bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dero_path = os.path.join(orig_path, 'dero')\n",
    "os.chdir(dero_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
